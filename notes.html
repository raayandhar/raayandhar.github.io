<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-03-20 Thu 15:08 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="Raayan Dhar" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org53a9381">1. CS131 Notes</a>
<ul>
<li><a href="#orge6cdff2">1.1. Functional Programming</a>
<ul>
<li><a href="#orgb0e3799">1.1.1. The quiz</a></li>
<li><a href="#orgd6a5901">1.1.2. Approaches to programming languages</a></li>
</ul>
</li>
<li><a href="#orgf9141a6">1.2. OCaml</a>
<ul>
<li><a href="#org3bba864">1.2.1. Values</a></li>
<li><a href="#orgd7e6deb">1.2.2. Conditionals</a></li>
<li><a href="#org0f8239c">1.2.3. Equality</a></li>
<li><a href="#org6092faf">1.2.4. Data Structures</a></li>
<li><a href="#orge987060">1.2.5. Functions</a></li>
<li><a href="#org067611e">1.2.6. Pattern matching</a></li>
<li><a href="#org4180b6f">1.2.7. Recursive Functions</a></li>
<li><a href="#org039cd83">1.2.8. Types</a></li>
</ul>
</li>
<li><a href="#org7fcf8d8">1.3. Grammars</a>
<ul>
<li><a href="#org33a16dd">1.3.1. Syntax vs. Semantics</a></li>
<li><a href="#orga6762dd">1.3.2. Tokens</a></li>
<li><a href="#org3911a88">1.3.3. BNF (Backus-Naur Form)</a></li>
<li><a href="#org0791e83">1.3.4. Extended BNF</a></li>
<li><a href="#org3ea35e1">1.3.5. Debugging Grammars</a></li>
</ul>
</li>
<li><a href="#org17d8fd6">1.4. Parsing</a>
<ul>
<li><a href="#orga854f72">1.4.1. Three issues in parsinng</a></li>
<li><a href="#org331e189">1.4.2. Matchers and Acceptors</a></li>
</ul>
</li>
<li><a href="#org7b9d99e">1.5. Types</a>
<ul>
<li><a href="#orga792abc">1.5.1. Primitive vs. constructed types</a></li>
<li><a href="#org55ed983">1.5.2. <code>float</code></a></li>
<li><a href="#org03b4eec">1.5.3. Type Usage and Properties</a></li>
<li><a href="#org7f7dd12">1.5.4. Type Equivalence</a></li>
<li><a href="#orgc3f1f5d">1.5.5. Abstract vs. Exposed types</a></li>
<li><a href="#org949221d">1.5.6. Subtypes</a></li>
<li><a href="#org8bd5cae">1.5.7. Polymorphism</a></li>
<li><a href="#orgb6562c3">1.5.8. Subtype Invariance</a></li>
<li><a href="#orgda35a6c">1.5.9. Duck Typing</a></li>
<li><a href="#org4d138a3">1.5.10. Static typing (Generics/Java) vs. Dynamic typing (Duck typing/Python)</a></li>
</ul>
</li>
<li><a href="#org51a0080">1.6. Java</a>
<ul>
<li><a href="#org436a155">1.6.1. <code>abstract</code></a></li>
<li><a href="#orgc81968b">1.6.2. <code>interface</code></a></li>
<li><a href="#org8baa489">1.6.3. Java <code>object</code></a></li>
<li><a href="#org91019fc">1.6.4. Java <code>threads</code></a></li>
<li><a href="#org818e317">1.6.5. Race conditions</a></li>
<li><a href="#orgab09131">1.6.6. Semaphores</a></li>
<li><a href="#orgb4a24eb">1.6.7. Exchanges</a></li>
<li><a href="#org7dc3dba">1.6.8. Countdown Latch</a></li>
<li><a href="#org5524a05">1.6.9. Java Memory Model</a></li>
<li><a href="#org69f9c45">1.6.10. Digression on <code>volatile</code></a></li>
<li><a href="#org0b7cd99">1.6.11. Returning to the JMM</a></li>
<li><a href="#org976f5fa">1.6.12. Java Project</a></li>
</ul>
</li>
<li><a href="#org0fe2bbb">1.7. Logic Programming and <code>prolog</code></a>
<ul>
<li><a href="#org9083e76">1.7.1. Quicksort in <code>prolog</code></a></li>
<li><a href="#orgca446e0">1.7.2. <code>prolog</code> syntax</a></li>
<li><a href="#orgbd2af20">1.7.3. Some <code>prolog</code> syntactic sugar</a></li>
<li><a href="#org22f6759">1.7.4. Review: Clauses</a></li>
<li><a href="#org70c5508">1.7.5. Three Kinds of Clauses</a></li>
<li><a href="#org143288d">1.7.6. <code>append</code></a></li>
<li><a href="#org21a6ce5">1.7.7. <code>reverse</code></a></li>
<li><a href="#org10d6106">1.7.8. Simple predicates</a></li>
<li><a href="#orge7c9720">1.7.9. What can go wrong in <code>prolog</code>?</a></li>
<li><a href="#org77d9897">1.7.10. Unification, two-way matching, and infinite (cyclic) terms</a></li>
<li><a href="#org7af13fe">1.7.11. Piano Arithmetic</a></li>
<li><a href="#orgdfef30b">1.7.12. Control mechanisms in <code>prolog</code>: proof trees and backtracking</a></li>
<li><a href="#orge321a82">1.7.13. Control mechanisms in <code>prolog</code>: the cut operator (<code>!</code>)</a></li>
<li><a href="#orgc4cc43e">1.7.14. Philosophy of <code>prolog</code></a></li>
<li><a href="#org0838636">1.7.15. Logic</a></li>
</ul>
</li>
<li><a href="#org18d004e">1.8. Scheme</a>
<ul>
<li><a href="#orgdf7f79e">1.8.1. Basic Scheme syntax</a></li>
<li><a href="#org2799cf1">1.8.2. Scheme vs. other languages</a></li>
<li><a href="#org45143ce">1.8.3. Scheme syntax</a></li>
<li><a href="#org8d4ec6b">1.8.4. Quoting in Scheme</a></li>
<li><a href="#org7dda66b">1.8.5. Special form: <code>lambda</code></a></li>
<li><a href="#org9db7190">1.8.6. Scoping</a></li>
<li><a href="#orge7974de">1.8.7. Bonus Scheme</a></li>
<li><a href="#org8899700">1.8.8. Primitives vs. Library</a></li>
<li><a href="#org66bbf59">1.8.9. Categorization of errors in Scheme</a></li>
<li><a href="#org1647732">1.8.10. Continuations</a></li>
<li><a href="#orgf1da629">1.8.11. Storage Management (from Albert's Notes)</a></li>
</ul>
</li>
<li><a href="#org4ab1754">1.9. Storage Management</a>
<ul>
<li><a href="#org4f22a87">1.9.1. Memory management</a></li>
<li><a href="#org47688fc">1.9.2. Dynamic vs. Static Chain</a></li>
<li><a href="#orgc2ffea4">1.9.3. Heap Management</a></li>
<li><a href="#org3be1509">1.9.4. Garbage collection</a></li>
<li><a href="#orge414761">1.9.5. Names, Identifiers, and Substitution</a></li>
<li><a href="#org61b3b9e">1.9.6. Names and terminology</a></li>
<li><a href="#org340b6f6">1.9.7. Namespaces</a></li>
</ul>
</li>
<li><a href="#org6312f1b">1.10. Information hiding (for modularity)</a>
<ul>
<li><a href="#org1bbe519">1.10.1. <code>C</code>-style information hiding</a></li>
<li><a href="#orgdef348d">1.10.2. <code>Java</code>-style acess modifiers</a></li>
<li><a href="#org9b6bb27">1.10.3. <code>OCaml</code> signatures</a></li>
<li><a href="#org052c4f6">1.10.4. Other langauges (<code>Python</code>, etc)</a></li>
</ul>
</li>
<li><a href="#org143b766">1.11. Errors, faults, and failures</a>
<ul>
<li><a href="#org6f75607">1.11.1. Error checking</a></li>
<li><a href="#org0350678">1.11.2. Error-handling</a></li>
<li><a href="#org564a3b6">1.11.3. Exceptions</a></li>
</ul>
</li>
<li><a href="#orgda256dd">1.12. Parameter Passing, Object-Oriented Programming, Cost Models</a>
<ul>
<li><a href="#org8afde0b">1.12.1. Parameter Passing</a></li>
<li><a href="#orga0e3d20">1.12.2. Advanced Calling Conventions</a></li>
</ul>
</li>
<li><a href="#org55402cf">1.13. Cost Models</a>
<ul>
<li><a href="#org581aea1">1.13.1. Cost models in programming languages</a></li>
</ul>
</li>
<li><a href="#org83b77fd">1.14. Rust</a>
<ul>
<li><a href="#org52327a1">1.14.1. Digression: Static checking of encryption-based program</a></li>
<li><a href="#org2ce4903">1.14.2. Rust: Performance, Ownership and Borrowing</a></li>
<li><a href="#org7cb08b0">1.14.3. Rust failure handling</a></li>
<li><a href="#orgff17c77">1.14.4. Cargo build system</a></li>
</ul>
</li>
<li><a href="#org411430a">1.15. Programming Language Semantics</a>
<ul>
<li><a href="#orgb0809cb">1.15.1. Static semantics</a></li>
<li><a href="#org0295312">1.15.2. Operational semantics</a></li>
<li><a href="#org75a6b10">1.15.3. Axiomatic semantics</a></li>
<li><a href="#orga489a61">1.15.4. Denotational semantics</a></li>
</ul>
</li>
<li><a href="#org6989e5a">1.16. Notes on Python <code>asyncio</code> and <code>Future</code></a></li>
</ul>
</li>
<li><a href="#org38c5a13">2. TODO:</a></li>
</ul>
</div>
</div>
<div id="outline-container-org53a9381" class="outline-2">
<h2 id="org53a9381"><span class="section-number-2">1.</span> CS131 Notes</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orge6cdff2" class="outline-3">
<h3 id="orge6cdff2"><span class="section-number-3">1.1.</span> Functional Programming</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-orgb0e3799" class="outline-4">
<h4 id="orgb0e3799"><span class="section-number-4">1.1.1.</span> The quiz</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
<code>tr -cs A-Za-z '\n' | tr A-Z a-z | sort | uniq -c | sort -nr</code>
</p>

<p>
<b>Why should we care about functional programming?</b>
</p>
<ul class="org-ul">
<li>Clarity: easier to maintain and write. In functional programming, you guarantee <b>referential transparency</b>: reference by identifiers (or names) to values obvious in functional programming! If you write the same expression twice with the same input you get the same value. This is not true for C++, C, non-functional programming languages. Feels a lot like mathematics.</li>
<li>Performance: want our programs to run faster (to be more easily optimizable - compiler should do the heavy lifting)</li>
</ul>
<p>
For example for performance under functional programming assumptions, we could call functions in the following way
</p>
<div class="org-src-container">
<pre class="src src-C">return f(x) + f(x) movl x, %ax
                   call f
                   addl %rax, %rax // With side effects, you cannot make this simplification!
                   ret
</pre>
</div>
<p>
since \(f(x)\) is functional, we can rely on it not affecting program state, allowing us to make this simplification in our "pseudo" assembly.
</p>
</div>
</div>

<div id="outline-container-orgd6a5901" class="outline-4">
<h4 id="orgd6a5901"><span class="section-number-4">1.1.2.</span> Approaches to programming languages</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
<b>Imperative:</b>
</p>
<ul class="org-ul">
<li>The basic unit of an imperative programming language is the <b>command/statement</b>: i.e., <code>S1, S2</code>, etc.</li>
<li>These langauges offer siginficant control over the computer, the "glue" being statements divided by i.e., semicolons like <code>S1; S2; S3</code></li>
<li>It relies on <b>mutable variables</b>: variables with state, modified via assignment.</li>
<li>Note that the sequential, command-based nature of imperative languages makes it inherently difficult to scale program written in them to modern, multi-threaded and distributing computing models.
<ul class="org-ul">
<li>The reason for this is that modifying the program state makes multi-threading environemnts difficult. For example, race condtions modify shared state in an unpredictable order. Execution order is crucial because statements mutate state.</li>
<li>There is also a performance overhead that comes with mutexes, locks, etc to prevent this.</li>
<li>On the other hand, functional programming helps since <i>immutability</i> removes race conditions (no shared mutable state), <i>referential transparency</i> allows parallel execution without worrying about side effects.</li>
</ul></li>
</ul>
<p>
<b>Functional:</b>
</p>
<ul class="org-ul">
<li>The basic unit of a functional programming language is the <b>function</b>: i.e., <code>F1 F2 F3</code>, etc and are based on the idea of <i>mathematical functions</i>.
<ul class="org-ul">
<li>For example <code>A(B(x, y), C(z))</code>. Since we are guaranteed they don't change program state, we can easily parallelize <code>B(x, y)</code> and <code>C(z)</code> "for free".</li>
</ul></li>
<li>Functional languages gives us <b>referential transparency</b>: we always know what an identifier refers to.
<ul class="org-ul">
<li>This comes at the cost of giving up <i>side effects</i>, e.g. assignments. However, without the side effect of I/O, we can't ever get work done.</li>
<li>This means that functional languages need an "escape hatch". OCaml's REPL allow its CLI to communicate with the user; in the real world, functional languages aren't purely functional but generally <i>encourage</i> the user to write in a functional style.</li>
</ul></li>
<li>The "glue" in this case is function application, i.e., <code>F1 (F2 (x), F3 (y, w), w)</code>.</li>
<li>Functional languages also lend themselves to <i>functional forms</i> or <i>higher-order functions</i>: functions that take other functions as arguments, or return other functions as results.
<ul class="org-ul">
<li>In OCaml, functions are first-class values, meaning they can be assigned to variables, passed as arguments, returned as results, or stored in data structures.</li>
<li>We can return a function directly, i.e., <code>let make_adder x = fun y -&gt; x + y;;</code>, and use partial application <code>let add5 = make_adder_5</code> which returns a function.</li>
</ul></li>
</ul>
<p>
<b>Logic:</b>
</p>
<ul class="org-ul">
<li>Logic languages are a subset of functional languages based on predicates, which are simply logical statements. Using logical operations, we can combine predicates to reason about things. The logic model may not be used as much as the functional/imperative models, but the logical way of thinking can be applied to things like database queries and static analysis of other languages.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgf9141a6" class="outline-3">
<h3 id="orgf9141a6"><span class="section-number-3">1.2.</span> OCaml</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Some important features of ML: compile-time type checking, type inference, garbage collection, higher-order functions. OCaml is an object-oriented dialect of ML.
</p>
</div>
<div id="outline-container-org3bba864" class="outline-4">
<h4 id="org3bba864"><span class="section-number-4">1.2.1.</span> Values</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
Everything in OCaml has a value, and every value has a type. OCaml also has type inference, and can typcially infer the type of a value automatically. Below we have <i>bound</i> the value to the identifier ("name") <code>x</code>. <b>Importantly</b>, bindings in OCaml are immutable: the value assigned to some name never changes.
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let x = 37*37;;
# val x : int = 1369 (* We have val (variable), name, : , type, value *)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd7e6deb" class="outline-4">
<h4 id="orgd7e6deb"><span class="section-number-4">1.2.2.</span> Conditionals</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
We can write conditionals like so: <code>if x &lt; 2000 then "a" else "b";;</code>. This is basically a ternary operator, and is an <i>expression</i>, not a statement: it evaluates to a value. In an conditional, you cannot have two different return types.
</p>

<p>
Since they are expressions, rather than <i>statements</i> this makes them fundamentally different from imperative <code>if</code> statements. Since they always produce a value, it does not cause side effects. In imperative languages, an <code>if</code> statement controls the executio flow. It also requires both <code>then</code> and <code>else</code> to esnure all branches return a value.
</p>
</div>
</div>
<div id="outline-container-org0f8239c" class="outline-4">
<h4 id="org0f8239c"><span class="section-number-4">1.2.3.</span> Equality</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
OCaml provides two different equality operators:
</p>
<ul class="org-ul">
<li><b>=</b>: structural equality, which simply compares values, i.e., <code>"hello" = "hello"</code>. Even if they are different objects in memory.
<ul class="org-ul">
<li>For lists and tuples, they are compared element by element. They are equal if 1) they have the same length, and 2) corresponding elements are structurally equal.</li>
</ul></li>
<li><b>==</b>: physical equality, which checks if two expressions point to the same object in memory.  Even if they have same content but are different objects, it will return false. This checks the address, which is fast and efficient, especially compared to something like a list.</li>
</ul>
</div>
</div>
<div id="outline-container-org6092faf" class="outline-4">
<h4 id="org6092faf"><span class="section-number-4">1.2.4.</span> Data Structures</h4>
<div class="outline-text-4" id="text-1-2-4">
<p>
There are two main data structures to consider in OCaml: tuples (heterogeneous) and lists (homogeneous).
</p>

<p>
<b>Tuples:</b>
Tuples are simply fixed length collections of elements of any type. Note that tuples are denoted using the <code>*</code> symbol and that elements are separated using commas. We have for example
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let tj = (1, "a");;
val tj : int * string = (1, "a")

let tj = (1, "a", ("c", 5.3));;
val tj : int * string * (string * float) = (1, "a", ("c", 5.3))
</pre>
</div>
<p>
The empty tuple is of the type <code>unit = ()</code> and are typically associated with side effects and indicate the absence of useful data.
</p>

<p>
<b>Lists:</b>
Lists in OCaml require that every element of a list be the same type. Lists are implemented as <b>immutable singly linked lists</b>, where each list element consists of a value (the head), and a pointer to the next element (the tail). The last element points to an empty list (<code>[]</code>) which acts as the base case for recursion.
</p>

<p>
Take for example the following:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let empty = [];;
val empty : 'a list = []

let notempty = [1; 3];;
val notempty : int list = [1; 3]

empty = notempty;;
- : bool = false
</pre>
</div>
<p>
note that <code>'a</code> denotes a type variable standing for any type. When we compare <code>[]</code> with an <code>int list</code>, it is inferred that <code>[]</code> is an <code>int list</code> in order to do the comparison.
</p>

<p>
There are two important operations on list: prepending and appending. Note that lists <b>cannot be modified</b> afer creation, so operations like <code>@</code> and <code>::</code> return <i>new lists</i> instead of modifying existing ones.
</p>
<ul class="org-ul">
<li><code>::</code> is \(O(1)\) and returns a list with a new head at the front.</li>
<li><code>@</code> is \(O(n)\) and returns a new list with a new tail.</li>
</ul>
<p>
the function <code>head::tail</code> has the following type signature.
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let ocons head tail = head::tail;;
val ocons : 'a -&gt; 'a list -&gt; 'a list = &lt;fun&gt;
</pre>
</div>
</div>
</div>
<div id="outline-container-orge987060" class="outline-4">
<h4 id="orge987060"><span class="section-number-4">1.2.5.</span> Functions</h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
Functions are simply values in OCaml, meaning we define them with <code>let</code>. Importantly, every function in OCaml takes a single argument and returns a single result. To "pass" multiple arguments or return "multiple" results, we are essentially using tuples. From the language point of view, there's always one argument. For the type of a function, only the last "argument" is the actual type.
</p>

<p>
OCaml applies functions using <b>left-associative application</b>, which allows it to write <code>f x y</code> easily. This also means that <code>f a b c</code> is syntactic sugar for <code>(((f a) b) c)</code>, and each function call <i>returns a function</i> unless it reaches full application. Function types are right <b>associative</b>. This means that the function type <code>int -&gt; int -&gt; int</code> is interpreted as <code>int -&gt; (int -&gt; int)</code>. This is related to currying, since this implies that functions in OCaml take one argument at a time and return another function until all arguments are supplied. For example,
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let add x y = x + y;;
val add : int -&gt; int -&gt; int
(* equivalent to *)
val add : int -&gt; (int -&gt; int)
</pre>
</div>
<p>
this means that <code>add</code> is a function that takes an <code>int</code> (first argument) and returns a new function that takes another <code>int</code> returns an <code>int</code>.
</p>

<p>
This is directly related to <b>currying/partial application</b>, which also allows us to guarantee that each function only has one argument. The process of turning a function that takes multiple arguments into a sequence of single-argument functions is known as currying. Namely, <code>let f x1 x2 ... xn = e</code> is syntactic sugar for
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let f =
fun x1 -&gt;
 (fun x2 -&gt;
    (...
        (fun xn -&gt; e)...))
</pre>
</div>
<p>
i.e., they are semantically equivalent.
</p>

<p>
<code>fun</code> is used to create anonymous functions and is inherently curried. <code>fun x y -&gt; x + y;;</code> is equivalent to <code>fun x -&gt; fun y -&gt; x + y;;</code>.
</p>

<p>
<code>function</code> is primarily used for defining functions with pattern matching:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">function
 | 0 -&gt; -3
 | x -&gt; x + 1;;
</pre>
</div>
<p>
which is syntactic sugar for
</p>
<div class="org-src-container">
<pre class="src src-ocaml">fun x -&gt; match x with
 | 0 -&gt; -3
 | x -&gt; x + 1;;
</pre>
</div>
<p>
we can also return functions (in general). Take the following
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let nnn = function
| 0 -&gt; (fun x -&gt; x) (* Identity function if input is 0 *)
| _ -&gt; (fun _ -&gt; 1) (* Function always returning 1 otherwise *)
</pre>
</div>
</div>
</div>
<div id="outline-container-org067611e" class="outline-4">
<h4 id="org067611e"><span class="section-number-4">1.2.6.</span> Pattern matching</h4>
<div class="outline-text-4" id="text-1-2-6">
<p>
Pattern matching is a super powerful way to decompose data structure, destructure values and handle conditional logic. Take the following <code>map</code> function:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let rec map f u =
 match u with
 | [] -&gt; []
 | x :: y -&gt; f x :: map f y;;
val map : ('a -&gt; 'b) -&gt; 'a list -&gt; 'b list = &lt;fun&gt;

map (fun x -&gt; x * x) [1; 2; 3; 4];;
- : int list = [1; 4; 9; 16]
</pre>
</div>
<p>
the <code>match</code> statement allows us to take care of the two cases. To match lists, we use <code>h::t</code>, where the head is just an element (<code>'a</code>) and the tail is a list (<code>'a list</code>). They can also be thought of as conditional statements, as seen below. Note that matches are done top-down, and <code>_</code> is a wildcard that is catch-all: matches with anything and discards the value.
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let g x =
 if x = "foo" then 1
 else if x = "bar" then 2
 else 0;;
val g : string -&gt; int = &lt;fun&gt;

let g' x = match x with
 | "foo" -&gt; 1
 | "bar" -&gt; 2
 | _ -&gt; 0;;
val g' : string -&gt; int = &lt;fun&gt;
</pre>
</div>
<p>
when a pattern match is not exhaustive, OCaml will detect this and warn us. In a general sense, the idea is
</p>
<div class="org-src-container">
<pre class="src src-ML">match E with
| P1 -&gt; E1
| P2 -&gt; E2
| ...
| Pn -&gt; En
</pre>
</div>
<p>
Look at your variable and see if it matches pattern \(P_i\), if so evaluate \(E_i\). A list of basic patterns is below.
</p>
<ul class="org-ul">
<li>Pattern | Value that it matches
<ul class="org-ul">
<li><code>0</code> (any constant) | itself</li>
<li><code>"a"</code> | itself</li>
<li><code>a</code> (any identifier) | any value (binds identifier to that value)</li>
<li><code>_</code> | any value, discarding value (<code>_</code> never bound to anything, "don't care" pattern</li>
<li><code>(P)</code> | whatever P matches</li>
<li><code>P1,P2,...,Pn</code> | any n-tuple, such that its first component matches P1, &#x2026;</li>
<li><code>[P1;P2;...;Pn]</code> | any list of length n, with corresponding submatches (rare)</li>
<li><code>P1::Pn</code> | any list of length &gt; 0, such that P1 matches the first item in the list, P2 matches the rest of the list</li>
</ul></li>
</ul>
<p>
<code>match</code> can be thought of as syntactic sugar in the following way:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let cons (a, b) = a::b
let cons x = match x with | (a, b) -&gt; a::b
let cons = (fun x -&gt; (match x with | (a, b) -&gt; (a::b)))
</pre>
</div>
<p>
The first is syntactic sugar for the second which is syntactic sugar for the third. No fallthrough cases because <code>(a, b)</code> matches any 2-tuple that can be passed to <code>cons</code>. Below we access members of a tuple by pattern matching
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let second p =
 match p with
 | (_, y) -&gt; y;;
val second : 'a * 'b -&gt; 'b = &lt;fun&gt;

second (42, "apple");;
- : string = "apple"
</pre>
</div>
<p>
We can do a similar pattern matching with lists, but we need to be careful to take care of every possible case as lists can have a variable number of elements.
</p>
</div>
</div>
<div id="outline-container-org4180b6f" class="outline-4">
<h4 id="org4180b6f"><span class="section-number-4">1.2.7.</span> Recursive Functions</h4>
<div class="outline-text-4" id="text-1-2-7">
<p>
In OCaml, we declare recursive functions using the <code>rec</code> keyword. When defining recursive functions, use <code>let rec</code>, since the general rule is that you can't have <code>let ID = EXPR</code> where <code>ID</code> is in <code>EXPR</code>. I.e., <code>let n = n + 1</code> is not allowed. For example, the following function removes odd-indexed elements:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let rec eo = fun x -&gt; match x with
                   | [] -&gt; []
                   | h::_::t -&gt; h::(eo t)
                   | [h] -&gt; [h] ;;
</pre>
</div>
<p>
OR, if you wanted to be smarter:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let rec eo = fun x -&gt; match x with
                   | h::_::t -&gt; h::(eo t)
                   | x -&gt; x ;;
</pre>
</div>
<p>
where the last pattern matches both the empty list and single list pattern. Can also use the <code>function</code> keyword for syntactic sugar for <code>fun x = match x with</code>, and we implicitly use <code>x</code> here. 
</p>

<p>
Take for example the following reverse list functions:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let rec reverse = function
    | [] -&gt; []
    | h::t -&gt; (reverse t) @ h;;
(* This is of type 'a list list -&gt; 'a list ! Reverses a list of list of something, and flattens the list.*)
let rec reverse = function
    | [] -&gt; []
    | h::t -&gt; (reverse t) @ [h];;
(* This will reverse the list correctly, but its slow! Lists are linked lists, A@B is O(|A|), A::B is cheap since we just need to make a new link. *)
let rec rev_helper = fun a -&gt; fun l -&gt;
   match l with
     | [] -&gt; a
     | h::t -&gt; rev (h::a) t
(* Now we run rev_helper with a = [], making reverse a curried function.*)
let reverse =
   (let rec rev = fun a -&gt; function
        | [] -&gt; a
        | h::t -&gt; rev (h::a) t
    in rev [])
</pre>
</div>
<p>
suppose we want to write a function where we supply a function (comparator)? Take the \(\min\) operation for example
</p>
<div class="org-src-container">
<pre class="src src-ocaml">let rec gmin lt idval = function
 | [] -&gt; idval
 | h::t -&gt; let tmin = gmin lt idval t
           in if lt h tmin
              then h
              else tmin
(* Use it like so: *)
gmin (&lt;) 99999999999 [5; -9; 27; -100; 12];;
(* returns : int = -100 *)
(* Has type: *)
val gmin : ('a -&gt; 'a -&gt; bool) -&gt; 'a -&gt; 'a list -&gt; 'a = &lt;fun&gt;
</pre>
</div>
</div>
</div>
<div id="outline-container-org039cd83" class="outline-4">
<h4 id="org039cd83"><span class="section-number-4">1.2.8.</span> Types</h4>
<div class="outline-text-4" id="text-1-2-8">
<p>
Declare <b>type alias</b> for a function type with i.e., <code>type myfntype = int -&gt; int</code>.
</p>

<p>
<b>Discriminant Union Type</b> allows a type to hold multiple distinct forms. Consider the following general example
</p>
<div class="org-src-container">
<pre class="src src-ocaml">type shape =
  | Circle of float
  | Rectangle of float * float
  | Square of float;;
</pre>
</div>
<p>
discriminant unions can be thought of a class that has both a type tag and a value. One common example is the <code>option</code> type, which we could implement as
</p>
<div class="org-src-container">
<pre class="src src-ocaml">type 'a option =
  | None
  | Some of 'a
</pre>
</div>
<p>
the <code>option</code> type is useful because it replaces the idea of having null pointers, removing the idea of "dereferencing a null pointer." It essentially indicates if a value does or does not exist, and is useful for pattern matching.
</p>

<p>
We can also compare and contrast the discriminant union type in OCaml vs. the union type in <code>C/C++</code>:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">type mydutype =
   | Foo
   | Bar of int
   | Baz of int * string
(* every mydutype has a tag field, which can't be overwritten. It can be relied on by application *)
(* You can create a value of a discriminant union type by using a constructor named in the type *)
Bar 3
(* You can use pattern matching to examine a value of a discriminant union. *)
match myval with
  | Foo -&gt; 1
  | Bar n -&gt; n + 7
  | Baz (n, _) -&gt; n - 3 ;;
(* Pattern matching guarantees that the way you access is safe *)
</pre>
</div>
<p>
vs. union of C/C++
</p>
<div class="org-src-container">
<pre class="src src-C">union mydutype {
  char dummy; /* nothing here */
  int bar; /* single int */
  struct { int i; char *s; } baz; /* int, string pair */
};

// Unions are "stacked" all addresses are the same.
// On the programmar to make sure that you to not access the wrong member. Error prone!
union mydutype u;
&amp;u == &amp;u.bar
&amp;u == &amp;u.baz.i
...
</pre>
</div>
<p>
importantly, unions in <code>C/C++</code> are dangerous since they are "stacked", and it is on the programmar to only access one field.
</p>

<p>
<b>Generic types</b>
We can define generic type in OCaml, where it is generic/general based on the type. For example,
</p>
<div class="org-src-container">
<pre class="src src-ocaml">type 'a option =
  | None
  | Some of 'a (* type field is Some *)
let x = Some "abc"
let y = None
match x with
  | Some n -&gt; ...
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org7fcf8d8" class="outline-3">
<h3 id="org7fcf8d8"><span class="section-number-3">1.3.</span> Grammars</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Why grammars? We need a specification for the programs we write, especially syntax. We expect things to be consistent and unambigious.
</p>
</div>
<div id="outline-container-org33a16dd" class="outline-4">
<h4 id="org33a16dd"><span class="section-number-4">1.3.1.</span> Syntax vs. Semantics</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
Syntax is about form. Semantics is about meaning. Syntax is the "lesser of the two", an easier problem.
</p>

<p>
Specification is about putting constraints on how a program is supposed to behave, and syntax is just a formal way to describe this.
In english, we can write something syntactically correct, but semantically nonsense ("Colorless green ideas sleep furiously").
You can also get an <b>ambigious</b> sentence (time flies: timing a fly or going fast?). The same idea in programming languages depends on the compiler, and how the compiler parses syntax. And in order to resolve ambiguity, we need an airtight context-free grammar.
</p>

<p>
Successful syntax is easier to use, read, safer for the programmar, has some useful redudancy, and most importantly <b>unambigious</b>. A grammar is simply a syntax specification. We want some redundancy in order for your software to be reliable. Extra constraints in the syntax
help catch mistakes early. For example, explicit type declarations iiin statically typed languages catch type errors at compile time (<code>let x : int = "hello";;</code>). Another example would be
parentheses, to avoid ambiguity, i.e., <code>let y = (3 + 4) * 2;;</code>
</p>
</div>
</div>

<div id="outline-container-orga6762dd" class="outline-4">
<h4 id="orga6762dd"><span class="section-number-4">1.3.2.</span> Tokens</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
Tokens are the lowest level components of most programming languages, but not necessarily things
"we just make up" (e.g. in English, we can make up tokens). A program is a byte sequence that represents a character sequence, and this character sequence represents a token sequence.
Just as not every byte-sequence is not a valid character sequence, not every character sequence represents a valid token sequence.
</p>

<p>
<b>Lexical analysis</b> is the process of taking the bytes of some input program and figuring out where the
boundaries between the words are. This allows the compiler to look at “tokens”, instead of individual
characters. Combining the tokens and any additional information (think “number” and “42”) gives
the set of lexemes.
</p>

<p>
Generally, we use greedy tokenizers, i.e. we continuously find the longest set of characters that
could be a token and choose it.
</p>

<p>
In addition, consider issues of internationalization (support for non-ASCII characters and the
security concerns that could follow) as well as the issue of reserved words: having reserved words
makes it harder to extend a language as new features break old programs.
</p>
</div>
</div>

<div id="outline-container-org3911a88" class="outline-4">
<h4 id="org3911a88"><span class="section-number-4">1.3.3.</span> BNF (Backus-Naur Form)</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
A BNF grammar rule looks like <code>LHS :: = RHS</code>. The left hand side is a non-terminal symbol and the right hand side is any sequence of terminals or non-terminals. For example
</p>
<div class="org-src-container">
<pre class="src src-python">NT = {S} # non-terminals
T = {a, b, c, d} # terminals

S ::= a S
S ::= S b
S ::= c d e
</pre>
</div>
<p>
where we denote non-terminals in capital letters and terminals in lowercase. <code>abdeb</code> is a valid string that would staisfy this grammar rule. If some text is valid under a grammar, it will be possible to build a parse tree such that each of the skeleton nodes (non-terminals) follows a defined grammar rule and the leaves (terminals) represents the text. BNF is a metasyntax notation for context-free grammars.
</p>
</div>
</div>
<div id="outline-container-org0791e83" class="outline-4">
<h4 id="org0791e83"><span class="section-number-4">1.3.4.</span> Extended BNF</h4>
<div class="outline-text-4" id="text-1-3-4">
<p>
Extended BNF extends BNF by adding metanotations which shorthand for BNF, in something similar to a Regex style way. Whatever grammar you can write in BNF you can write in EBNF (and vice versa). For example, to exclude the characters <code>&lt;</code> and <code>&amp;</code> from <code>CharData</code>, we could write
</p>
<pre class="example">
CharData ::= [^&lt;&amp;]*
</pre>
<p>
which is equivalent to
</p>
<pre class="example">
X ::= a
X ::= b
X ::= c
...
CharData ::=
CharData ::= X
</pre>
<p>
Another example below, for the definition of floating-point literals in Python:
</p>
<pre class="example">
floatnumber   ::=  pointfloat | exponentfloat
pointfloat    ::=  [intpart] fraction | intpart "."
exponentfloat ::=  (intpart | pointfloat) exponent
intpart       ::=  digit+
fraction      ::=  "." digit+
exponent      ::=  ("e" | "E") ["+" | "-"] digit+
</pre>
<p>
we extend BNF with optional elements <code>[ ... ]</code>, repetitions <code>{ ... }</code> and grouping <code>( ... )</code>. For example,
</p>
<pre class="example">
&lt;expr&gt; ::= &lt;number&gt; | &lt;expr&gt; ("+" | "*") &lt;expr&gt;
&lt;number&gt; ::= [ "0"-"9" ]+
</pre>
<p>
where <code>[ "0"-"9" ]+</code> means one or more digits, and <code>("+" | "*")</code> means either <code>+</code> or <code>*</code>. Translating EBNF back down to BNF can be difficult in some cases. For example,
</p>
<pre class="example">
&lt;no-fold-literal&gt; ::= "[" *dtext "]" # This is EBNF, the * in front means repeat that 0 or more times.

# To do this in BNF, we have to introduce a new non-terminal
&lt;sdtext&gt; ::= # has to be empty! since we have *0* or more times
&lt;sdtext&gt; ::= dtext sdtext
&lt;no-fold-literal&gt; ::= "[" sdtext "]" # Now they are equivalent.
</pre>
<p>
continuing, we have the following EBNF to BNF translation
</p>
<pre class="example">
# EBNF
atext ::= /ALPHA/DIGIT/ ...
dot-atom-text ::= 1*atext * ("." 1*atext)
# The * on the right means repeat 1 or more times (like +).
# The ( ) grouping means in this context, the * after atext controls the whole group.
# So we would have a dot, followed by 1 or more atext, and we would have on or more of these. For example sub.example.com.

# To extend this to BNF we could have
1atext = /ALPHA/DIGIT/...
1atext = /ALPHA/DIGIT/... 1atext
0atext =
0atext = "." 1atex

dot-atom-text = 1atext 0atext
</pre>
<p>
we can parse these grammars easily with i.e. <code>grep</code> because it is a <b>context-free grammar</b>.
</p>

<p>
Every BNF (and EBNF) grammar is context-free. But you can't turn every EBNF grammar into a regular expression. This is because regular expressions "can't count".
More specifically, they cannot enforce arbitrary nesting or ensure balanced constructs. For example, we could have
</p>
<pre class="example">
&lt;balanced&gt; ::= "(" &lt;balanced&gt; ")" | ε
# valid strings are "", "()", "(())" etc
</pre>
<p>
since regular languages are defined by finite automate, they do not have memory / stack to track dependencies. They cannot remember how many <code>(</code> have been opened or how deep they are inside parantheses.
</p>

<p>
For grammars, we notice that for it to be parsed by a regular expression, there should not be any recursion. Every expansion makes the expression simpler.
</p>

<p>
<b>XML Grammar</b>
</p>

<p>
The XML grammar looks like the following:
</p>
<pre class="example">
CharData ::= [^&lt;&amp;]*
CharRef ::= '&amp;#'([0-9]+ | 'x'[0-9a-fA-F]+)';'
EntityRef ::= '&amp;'Name';'
Reference ::= EntityRef | CharRef
Equality ::= S?'='S?
S ::= (#x20 | #x9 | #xD | #xA)+
AltValue ::= '"'([^&lt;&amp;"] | Reference)*'"' | "'"([^&lt;&amp;'] | Reference)*"'"
NameChar ::= NameStartChar | "-" | "." | [0-9]
NameStartChar ::= ":" | [A-Za-z] | "_"
Name ::= NameStartChar (NameChar)*
Attribute ::= Name Eq AttValue
Stag ::= '&lt;'Name (S Attribute)* S?'&gt;'
Etag ::= '&lt;/'Name S?'&gt;'
EmptyElemTag ::= '&lt;'Name (S Attribute)* S?'/&gt;'
content ::= CharData? ((element | Reference) CharData?)*
element ::= EmptyElemTag | STag content ETag
</pre>
<p>
in the XML grammar, lower-case means a non-terminal that is recursively defined. Uppercase is not recursive.
</p>

<p>
EBNF has an ISO standard, which includes the following
</p>
<pre class="example">
"terminal" # Can write it with single or double quotes
'terminal'
[optional] # Anything in brackets is optional, 0 or 1 times
{repetition} # Anything in braces can appear 0 or more times
(grouping) # Group things together
N*X : repeat N times # Can also write it this way instead of repetition
X-Y : set difference # Instance of X that is not an instance of Y; exception
X,Y : concatenation # Instance of X followed by an instance of Y
X|Y : disjunction (or)
X = Y; # How to write a rule in ISO-EBNF
</pre>
<p>
ISO EBNF syntax is defined in ISO EBNF syntax! See the following
</p>
<pre class="example">
syntax = syntax rule, {syntax rule}; # Can't have an empty syntax rule.
syntax rule = meta id, '=', defns list, ';';
defns list = defn, {'|', defn};
defn = term, {',', term };
term = factor, ['-', exception];
</pre>
<p>
Note that <code>syntax rule</code> describes itself. Similar to how <code>int x = x</code> is not feasible in most programming languages. We could do <code>int* x = &amp;x</code> which is OK because by the time we get to the RHS we have already allocated storage for <code>x</code>, so it's OK to take the address.
</p>

<p>
Back to ISO EBNF in terms of ISO EBNF, why is this OK?
</p>

<p>
The way the authors of ISO EBNF resolve this is to specify ISO EBNF in English, and then put the ISO EBNF in terms of ISO EBNF in an appendix.
The appendix is not the definiton, it's documentation.
</p>
</div>
</div>
<div id="outline-container-org3ea35e1" class="outline-4">
<h4 id="org3ea35e1"><span class="section-number-4">1.3.5.</span> Debugging Grammars</h4>
<div class="outline-text-4" id="text-1-3-5">
<p>
The next question we ask ourselves is what can go wrong?
</p>

<p>
<b>What can go wrong with tokenization?</b>
</p>
<ol class="org-ol">
<li>Confused about characters: i.e., o vs. cyrillic o (i.e., auditing code, international developers)</li>
<li>Keywords in programming languages (reserved words, depends on the programming languages, case matters in some programming languages, in some they don't, etc)</li>
<li>Tokenizers are greedy!
<ul class="org-ul">
<li>Tokenizers look for the longest possible token until it can no longer find it, and then cuts it off and moves to the next at the moment it becomes no longer valid.</li>
<li>e.g. <code>a = b+++++c;</code></li>
</ul></li>
<li>Long tokens, e.g. a character string. Can run into issues like buffer overflow or memory limitations. It could overwrite adjacent memory, or there may be some identifier length limit.
<ul class="org-ul">
<li>Give people ways to write shorter tokens that mean the same thing as a longer token.</li>
<li>In <code>C</code>, can attack this problem with <code>\n</code>
<ul class="org-ul">
<li>This can also run into issues! Consider <code>\</code> elsewhere in your code, you may not be sure what is going on and the compiler may interpret it in a way you are not familiar with.</li>
</ul></li>
</ul></li>
<li>Comments and whitespace.
<ul class="org-ul">
<li>This part of the program is ignored, but you need to recognize it in order to ignore it.</li>
</ul></li>
</ol>

<p>
<b>What can go wrong with (context-free) grammars?</b>
First, we should know the components of a context-free grammar: <code>(set of tokens, set of nonterminals, set of terminals, starting terminal (non-terminal), set of production rules (at least one must have a LHS that is the start symbol))</code>
</p>

<p>
What is a rule?
A rule is a pair: a LHS which is a non-terminal, and a RHS which is a finite sequence of symbols (terminals or nonterminals).
</p>

<p>
Now we move onto examples of grammars that have issues. Consider the following grammar, where <code>S</code> is the start symbol, lowercase letters are terminals and uppercase are nonterminals.
</p>
<pre class="example">
S -&gt; a
S -&gt; b

T -&gt; c
S -&gt; S
</pre>
<p>
Our third and fourth rules are useless. We can never get "c" in a valid sentence and our last rule is simply redundant. Now consider
</p>
<pre class="example">
S -&gt; aTb
S -&gt; Sa
S -&gt; b
</pre>
<p>
in this case, our first rule doesn't make sense: there are no terminal symbols defined in this grammar for <code>T</code>. We also have
</p>
<pre class="example">
S -&gt; Sa
S -&gt; b
</pre>
<p>
this grammar is left-recursive, and these are difficult to parse for top-down parsers. The reason for this is that a recursive descent parser works by calling functions recursively for each non-terminal. The parser calls itself indefinitely. For example, trying to parse <code>1 + 2 + 3</code> using
</p>
<pre class="example">
&lt;expr&gt; ::= &lt;expr&gt; "+" &lt;term&gt; | &lt;term&gt;
</pre>
<p>
will result in the parser calling itself indefinitely on <code>&lt;expr&gt;</code>, never consuming the input. Well-behaved parsers consume tokens as it expands rules.
</p>
<pre class="example">
parse_expr()  → calls parse_expr()  → calls parse_expr()  → infinite loop!
</pre>
<p>
to prevent this, we need to convert our left-recursive grammar to right recursive. From our example earlier, we turn it into right-recursive form in the following way:
</p>
<pre class="example">
&lt;expr&gt; ::= &lt;term&gt; &lt;expr_tail&gt;
&lt;expr_tail&gt; ::= "+" &lt;term&gt; &lt;expr_tail&gt; | ε
</pre>
<p>
this avoids infinite recursion by ensuring <b>each step consumes a token</b> before recursing. From our simpler example from much earlier, we would change it in the following way:
</p>
<pre class="example">
S -&gt; bT
T -&gt; aT
T -&gt; ε
</pre>
<p>
We also have to consider that we need to capture the entirety of our language. For instance: in English, we
need verbs and nouns to agree in plurality. A grammar for English would thus need to capture this
complexity. However, at the same time, we need to consider the level of detail we actually want: for
instance, if we tried to capture the constraints of plurality, gender, and tense, we would have an
incredibly large grammar.
</p>

<p>
Another issue: consider things like <code>typedef</code> in C++: this means that when we parse, we can’t do a
simple tokenization, as the typedef affects whether a token is a variable or a type.
</p>

<p>
In Python, the parser would also need to take note of indentations, adding complication as it can’t
simply do something like looking for braces.
</p>

<p>
<b>What can go wrong, cont.</b>
</p>
<ol class="org-ol">
<li>Useless rules:
<ul class="org-ul">
<li>Unreachable non-terminals: non-terminals that cannot be dervied from the start symbol.
<ul class="org-ul">
<li>e.g. <code>S -&gt; A; T -&gt; B;</code> (if <code>S</code> ius start symbol, <code>T</code> is unreachable)</li>
</ul></li>
<li>Blind alleys: rules that lead to derivations that cannot produce terminal strings.
<ul class="org-ul">
<li>e.g. <code>S -&gt; a; S -&gt; bT; T -&gt; cT;</code> (<code>T</code> can only derive more <code>T</code>'s and <code>c</code>'s; never terminating)</li>
<li>A blind alley refers to a production rule that can never be reached in any valid derivation from the start symbol of a grammar. Beacuse in our example <code>T</code> recursively calls itself, it never reaches a terminal string. And any valid sentence must eventually be made up of terminals, and since it only expands into more <code>T</code> it never produces a valid string and this is unreachable in any valid sentence and serves no purpose in the grammar.</li>
</ul></li>
<li>Redundant rules: rules that are unnecessary as they don't add to the language defined by the grammar
<ul class="org-ul">
<li>e.g. <code>S -&gt; a; S -&gt; bS; S -&gt; ba;</code> (<code>S -&gt; ba</code> is redundant)</li>
</ul></li>
<li>Impact of useless/redundant rules: grammars become more complex and harder to understand, but the language defined remains the same.</li>
</ul></li>
<li>Extra constraints the grammar cannot capture
<ul class="org-ul">
<li>Grammars (BNF/EBNF) are context-free grammars, and CFGs have limitations.</li>
<li>Fundamentally, they cannot capture all syntactic rules of programming languages</li>
<li>e.g. Declaration before Use: CFGs cannot enforce that variables must be declared before being used (semantic rule, not purely syntactic)</li>
<li>e.g. Type Checking: CFGs cannot enforce type correctness (semantic rule)</li>
<li><b>Workaround</b>: programming language standards use "English prose" to specify constraints that CFGs cannot express (semantic rules, context-sensitive rules)</li>
</ul></li>
</ol>
<p>
Example in English: take for example the following grammar:
</p>
<pre class="example">
S -&gt; NP VP . # Sentence can go to a noun phrase followed by a verb phrase followed by a period.
NP -&gt; n # Terminal noun
NP -&gt; adj NP # Terminal adjective followed by a noun phrase
VP -&gt; v # Terminal verb
VP -&gt; VP adv # Verb phrase followed by terminal adverb
</pre>
<p>
in this grammar, the sentences "dogs bark." and "Maxwell meows loudly." are allowed. However, the sentence "Maxwell meow loudly." is valid in the grammar but not in English, our grammar is too generous. We can try to fix this by complicating the grammar:
</p>
<pre class="example">
S -&gt; SNP SVP # Sentence can be a singular noun phrase followed by a singular verb phrase
S -&gt; PNP PVP # or a plural noun phrase followe by a plural verb phrase
SNP -&gt; sn
SNP -&gt; adj SNP
PNP -&gt; pn
PNP -&gt; adj PNP
... would have to do the same for verb phrases
</pre>
<p>
so we can fix this problem, but the fix came at the price of the grammar becoming more complicating. For programming languages, this can be very difficult, so we can introduce "English" (i.e., constraints outside of the grammar)
</p>
<ol class="org-ol">
<li><b>Ambiguity</b>
<ul class="org-ul">
<li>Ambiguous grammar: a grammar that allows more than one parse tree (derivation) for the same input sentence (token sequence)</li>
<li>Undesirable in programming languages: ambiguity leads to uncertainty about program meaning and behavior; compilers might interpret code differently than intended.</li>
<li>Example of ambiguous grammar (arithmetic expression):</li>
</ul></li>
</ol>
<pre class="example">
S ::= id | num | S "+" S | S "*" S | "(" S ")"
Example ambiguous sentence: id "+" num "*" id
Two possible parse trees (different precedence):
(id "+" num) "*" id vs. id "+" (num "*" id)
</pre>


<div id="org5b46b3e" class="figure">
<p><img src="midterm/grammar1.png" alt="grammar1.png" width="300px" />
</p>
</div>


<p>
Suffers from precedence and associativity problems. We can fix this in one way by defining the order of operations, or define grouping for operators of the same precedence (e.g. left-associativity for subtraction: <code>a - b - c</code> is <code>((a - b) - c)</code>). Or, we can complicate the grammar. For the example above, we can do the following:
</p>
<pre class="example">
S ::= T + T
S ::= T
T ::= F * F
T ::= F
F ::= id
F ::= num
F ::= (S)
</pre>
<p>
which gives multiplication higher precedence over multiplication. However, we still have a problem, for example <code>a + b + c</code> will not be able to be parsed because <code>S</code> can only have two subterms. 
</p>


<div id="org5741932" class="figure">
<p><img src="midterm/grammar2.png" alt="grammar2.png" width="300px" />
</p>
</div>

<p>
To fix, we make our grammar recursive (see image above):
</p>
<pre class="example">
S ::= S + T
S ::= T
T ::= T * F
T ::= F
F ::= id
F ::= num
F ::= ( S )
</pre>
<p>
and now we can parse <code>a + b + c</code>. With this fix, we have fixed both precedence and associativity (matters because of overflow, rounding errors, subtraction). If we wanted to e.g. introduce exponentiation, we would do
</p>
<pre class="example">
S ::= S + T
S ::= T
T ::= T * F
T ::= F
F ::= E
E ::= F ^ E  (* Right-associative exponentiation *)
E ::= id
E ::= num
E ::= ( S )
</pre>
<p>
<b>Concrete grammar vs. abstract grammar</b>
</p>

<p>
We could call the grammar where we introduce much more complications as the <i>concrete grammar</i>, and the <i>abstract grammar</i> which does not worry about associativity, precedence, ambiguity, etc. The concrete grammar is complicated, and the abstract grammar is easier to describe, but would not be used for implementation.
</p>

<p>
<b>Dangling-Else Ambiguity</b>
</p>

<p>
When we have nexted <code>if-else</code> statements, there's ambiguity! For example:
</p>
<div class="org-src-container">
<pre class="src src-python">if (condition1):
    if (condition2):
        statement1;
    else:
        statement2; # To which 'if' does this 'else' belong to?
</pre>
</div>
<p>
there are two possible interpretations that the parse tree could take:
</p>
<ol class="org-ol">
<li><code>else</code> is associated with the inner <code>if: if (condition1) { if (condition2) statement1; else statement2; }</code>; correct interpretation in <code>C</code>-like languages</li>
<li><code>else</code> associated with the outer <code>if: if (condition1) { if (condition2) statement1; } else statement2;</code>; incorrect interpretation</li>
<li>We can resolve this by just having "English" for a lexical scoping rule, that is not directly in the grammar: "Dangling <code>else</code> associates with the nearest preceding <code>if</code> that lacks an <code>else</code>"</li>
<li>Or we can have a grammar fix by complicating the grammar: introduce new non-terminals to enforce the correct association in the grammar itself, i.e., distinguish between "matched" and "unmatched" if statements in grammar rules.</li>
</ol>
<p>
Consider the following grammar for statements in <code>C</code>:
</p>
<pre class="example">
stmt:
;
expr;
break;
continue;
return expr_opt;
goto ID;
while (expr) stmt
do stmt while(expr);
for (expr_opt; expr_opt; expr_opt) stmt
switch (expr) stmt
if (expr) *stm* else stmt # we use stm to differentiate.
# to fix, we re-name the top-level stmt: to stm:, and redefine
# stmt:
  stm
  if (expr) stmt
</pre>
</div>
</div>
</div>

<div id="outline-container-org17d8fd6" class="outline-3">
<h3 id="org17d8fd6"><span class="section-number-3">1.4.</span> Parsing</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Parsing is the process of taking a sequence of tokens (program code) and determining its syntactic structure according to a grammar. The goal of parsing is to construct a parse tree (abstract syntax tree - AST) that represents the syntactic structure
of the input code, or to detect syntax errors if the input is not valid according to the grammar.
</p>
</div>

<div id="outline-container-orga854f72" class="outline-4">
<h4 id="orga854f72"><span class="section-number-4">1.4.1.</span> Three issues in parsinng</h4>
<div class="outline-text-4" id="text-1-4-1">
<ol class="org-ol">
<li>Recursion
<ul class="org-ul">
<li>Grammars are typically recursive</li>
<li>Parsers need to handle recursion to parse arbitrarily nested structures</li>
<li>Recursion handling in parses (relatively easy if your tech is recursive): recursive descent parsing is a common technique that directly implmenets grammar rules as recursive functions</li>
</ul></li>
<li>Disjunction (OR)
<ul class="org-ul">
<li>Grammars use alternation (<code>|</code>) to define choices (multiple possible rules for a non-terminal)</li>
<li>Parsers need to handle disjunction by exploring different parsing paths when encountering alternatives in the grammar</li>
<li>Disjunction handling in parsers requires backtracking or alternatives. Parsers may need to try different alternatives and backtrack if a choice leads to a dead and end.
<ul class="org-ul">
<li>Matchers and acceptors are designed to handle disjunction</li>
</ul></li>
</ul></li>
<li>Concatenation (sequence)
<ul class="org-ul">
<li>Grammar rules define sequences of symbols (concatenation -, or implicit juxtaposition: concatenation of symbols without an explicit operators, they just appear next to each other <code>S -&gt; AB=</code>)</li>
<li>Parsers need to process input tokens sequentially according to the order defined in grammar rules.</li>
<li>Concatenation handling in parsers (sequential processing): parsers read input tokens in order, matching them against the expect sequence of symbols in grammar rules.
<ul class="org-ul">
<li>Matchers and acceptors handle concatenation by sequential matching</li>
</ul></li>
</ul></li>
</ol>
<p>
Sequential matching example:
</p>
<pre class="example">
S ::= A B
A ::= "hello"
B ::= "world"
</pre>
<p>
the parsers starts at <code>S</code> and expects <code>A</code> followed by <code>B</code>. It reads <code>"hello"</code> and matches <code>A</code>, it reads <code>"world"</code> and matches <code>B</code>. Since all symbols were matched in the correct order, the input is accepted.
</p>
</div>
</div>
<div id="outline-container-org331e189" class="outline-4">
<h4 id="org331e189"><span class="section-number-4">1.4.2.</span> Matchers and Acceptors</h4>
<div class="outline-text-4" id="text-1-4-2">
<ul class="org-ul">
<li>Matchers are functions that try to match a prefix of the input token sequence against a part of a grammar rule. It returns the matched part and the remaining (unmatched) suffix.</li>
<li>Acceptors are functions that check if the remaining suffix (from a matcher) is "acceptable" according to the rest of the grammar rule.</li>
<li>Matcher matches a part, acceptor validates the rest, combining to parse larger structures.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org7b9d99e" class="outline-3">
<h3 id="org7b9d99e"><span class="section-number-3">1.5.</span> Types</h3>
<div class="outline-text-3" id="text-1-5">
<p>
What is a type?
</p>
<ul class="org-ul">
<li>Definition 1: predefined or user-defined data structure
<ul class="org-ul">
<li>Initial thought: a type is a predefined or user-defined data structure.</li>
<li>Critique:
<ul class="org-ul">
<li>Types are related to data structures but not always the same.</li>
<li>Example: Enum types in C/C++ (e.g., <code>enum colors {A, B, C}</code>) are types but not complex data structures.</li>
<li>Data structures can be complex without necessarily being a single type.</li>
</ul></li>
</ul></li>
<li>Definition 2: a set of things the compiler knows about a value
<ul class="org-ul">
<li>Second attempt: A type is the set of things the compiler knows about a value.</li>
<li>Critique:
<ul class="org-ul">
<li>Too broad and potentially inaccurate.</li>
<li>Example: <code>int n + 1</code>. While the type is int, the compiler might deduce a more precise range than the full int range, especially considering overflow behavior.</li>
<li>Undefined behavior in C/C++ on overflow means the compiler’s knowledge can be limited or lead to unpredictable outcomes.</li>
<li>Python example: In dynamically typed languages like Python, every variable has a type (dynamically checked), but the “compiler” (interpreter) may know very little statically about the value’s specifics.</li>
</ul></li>
</ul></li>
<li>Definition 3 (more accurate perspective): a set of values and associated operations
<ul class="org-ul">
<li>Refined definitiopn: a type is a set of value (and optionally, associated operations)</li>
<li>Explanation:
<ul class="org-ul">
<li>A type defines the possible values a variable or expression can hold.</li>
<li>In object-oriented languages (C++, Python, Java), types often include associated operations (methods)</li>
<li>Example: int in a quasi-C language can be seen as the set of values within a certain range.</li>
<li>Set theory connection: Types can be conceptually viewed as sets of values.</li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="outline-container-orga792abc" class="outline-4">
<h4 id="orga792abc"><span class="section-number-4">1.5.1.</span> Primitive vs. constructed types</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
Primitive types:
</p>
<ul class="org-ul">
<li>Definitions: basic types built into the language.</li>
<li>Examples: int, char, boolean, float, double.</li>
<li>Seemingly “solved,” but implementations vary across languages.</li>

<li>Variations in <code>int</code> type
<ul class="org-ul">
<li>C/C++ <code>int</code>:
<ul class="org-ul">
<li>Minimum requirement: Must store values from at least -32,767 to 32,767.</li>
<li>Implementation-defined size (e.g., 16-bit, 32-bit, 64-bit).</li>
<li>Macros like INT<sub>MIN</sub> and INT<sub>MAX</sub> provide platform-specific limits.</li>
</ul></li>
<li>Mathematical integers (e.g., in some languages like Haskell, Python implicitly for large ints):
<ul class="org-ul">
<li>No fixed size limit (limited only by available memory).</li>
<li>No overflow issues.</li>
</ul></li>
<li>JavaScript <code>int</code>
<ul class="org-ul">
<li>int is a subset of float.</li>
<li>Integers are floating-point numbers without a fractional part.</li>
<li>Maximum integer size is limited by the float representation, but much larger than C/C++ int.</li>
<li>Rounding behavior can occur when integers become very large.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org55ed983" class="outline-4">
<h4 id="org55ed983"><span class="section-number-4">1.5.2.</span> <code>float</code></h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
Most use data type in terms of CPU/GPU cycles!
</p>

<p>
IEEE754 single-precision (32-bit) representation:
</p>
<ul class="org-ul">
<li>32 bits divided into: 1 sign bit, 8 exponent bits, 23 fraction bits</li>
<li>We represent numbers in the form \(+/- 2^{E - 127} \cdot (1.F)_2\)
<ul class="org-ul">
<li>Sign bit: 0 for non-negative, 1 for non-positive (not strictly negative)</li>
<li>Biased exponent: \(E - 127\)</li>
<li>Mantiass/fraction: \(1.F\) where \(F\) is the 23-bit fraction field. The '1' is a hidden bit for normalized numbers.</li>
</ul></li>
<li>Normalization: like scientific notation, ensures a unique representation for most numbers</li>
</ul>

<p>
<b>Special cases:</b>
</p>
<ul class="org-ul">
<li>Case 1: \(E\) is not zeros or all ones (normal case): use \(+/- 2^{E - 127} \cdot (1.F)_2\)</li>
<li>Example: represent <code>5.75</code>:
<ul class="org-ul">
<li>Convert 5 to binary: <code>101</code></li>
<li>Fraction part: <code>0.75</code> in binary is <code>0.11</code></li>
<li>Full binary: <code>101.11</code></li>
<li>Normalize: \(1.0111 \cdot 2^2\)
<ul class="org-ul">
<li><code>01110....0</code> for mantissa, exponent is <code>2 + 127 = 129</code>, 0 for sign bit</li>
</ul></li>
</ul></li>
<li>Example: represent <code>-0.15625</code>:
<ul class="org-ul">
<li>Convert <code>0.15625</code> in binary: <code>0.00101</code></li>
<li>Normalized: \(1.01 \cdot 2^{-3}\)</li>
<li>1 for sign bit, exponent is <code>-3 + 1276 = 124</code>, mantissa is <code>0100....0</code></li>
</ul></li>
<li>Case 2: \(E\) is all zeros</li>
<li>Case 2.1: tiny numbers (denormalized/subnormal)
<ul class="org-ul">
<li>When \(E = 0\) and \(F \neq 0\)</li>
<li>New formula: $+/- 2<sup>-127</sup> &sdot; (0.F)</li>
<li>Not normalized, leading zeros in the fraction</li>
<li>Closer to zero than the smallest normalized number</li>
<li>Can lead to underflow if normalization fails due to exponent limits</li>
<li>Provides gradual underflow rather than abrupt zeroing</li>
</ul></li>
<li>Case 2.2: zero
<ul class="org-ul">
<li>\(E = F = 0\)</li>
<li>There are two representations: <code>+0</code> and <code>-0</code></li>
<li><code>+0</code> and <code>-0</code> compare as numerically equal (<code>x = y</code> even if different sign bits) but representations differ</li>
</ul></li>
<li>Case 3: Infinity and NaN</li>
<li>Case 3.1: Infinity
<ul class="org-ul">
<li>Represented when \(E = 255\) (all ones) and \(F = 0\)</li>
<li>Two infinities: negative and positive</li>
<li>Result of overflow or division by zero (<code>1.0 / 0.0 = +infty</code>)</li>
<li>Up to designer to decide if this should be trapped (program crash) or to return on overflow (GCC <code>-ftrapv</code> flag)</li>
</ul></li>
<li>Case 3.2: Not-a-Number
<ul class="org-ul">
<li>Represented when \(E = 255\) and \(F \neq 0\)</li>
<li>Represents results of undefined operations (e.g. \(\infty - \infty, \sqrt{-1}\))</li>
<li>Many NaN values (\(2^{34}\) possibilities due to different \(F\) values), but generally treated as the same</li>
<li>NaN is not equal to anything, including itself. Can use this to check for NaN: <code>bool isNan(float x) { return x ! x}</code></li>
</ul></li>
</ul>
<p>
Implications of <code>float</code> design:
</p>
<ol class="org-ol">
<li>Complexity: "simple" <code>float</code> type has many nuances and special cases; designing types requires careful consideration of edge cases and error handling</li>
<li>Operations on <code>float</code>: standard operations must be defined for special values i.e., \(-0\), NaN, etc</li>
<li>Trade-off: choose to trap or use exceptional values - when errors occur (overflow, invalid ops), should the program crash (trap/exception) or return a special value (infinity, NaN)? IEEE754 chose to provide exceptional values as default for continuous operation.</li>
</ol>
</div>
</div>

<div id="outline-container-org03b4eec" class="outline-4">
<h4 id="org03b4eec"><span class="section-number-4">1.5.3.</span> Type Usage and Properties</h4>
<div class="outline-text-4" id="text-1-5-3">
<p>
<b>Annotations</b>
</p>
<ul class="org-ul">
<li>Types as comments with teeth: provide information for human readers and compilers</li>
<li>Compiler optimization: type information enables register allocation, efficient code generation</li>
<li>Runtime effects: type annotations can trigger runtime operations (e.g. casting between <code>float</code> and <code>int</code>)</li>
</ul>
<p>
<b>Inference</b>
</p>
<ul class="org-ul">
<li>Inferring types based on context</li>
<li>Example: <code>int i; float f; i * f;</code>. Compiler infers the result type of <code>i * f</code> as <code>float</code> due to mixed-type operation rules (coercion), has to change the assembly/machine code.</li>
</ul>
<p>
<b>Coercion</b>
Coercion occurs occurs when we convert values from one type to another. Ideally, coercion doesn't lose information but this is frequently not the case in real code.
</p>

<p>
We can have issues with this paradigm. For example:
</p>
<pre class="example">
float f = ____;
double d = cos(f);
</pre>
<p>
should we call the <code>float</code> variant of <code>cos</code> and convert the result to a double, or should we convert <code>f</code> to a double and then return the result of using the <code>double</code> variant of <code>cos</code>?
</p>

<p>
Another example of an issue is when we have multiple definitions of an overloaded function: one that takes a <code>double</code> and a <code>float</code> and one that takes a <code>float</code> and a <code>double</code>. if we provide two floats, which function should be called?
<b>Checking</b>
We do type checking to catch errors early and prevent dumb mistakes and serious consequences. There are two main types of checking: static and dynamic checking.
</p>
<ol class="org-ol">
<li>Static checking: performed by the compiler before program execution
<ul class="org-ul">
<li>Advantages:
<ul class="org-ul">
<li>Guarantees no type errors at runtime (for statically checked parts)</li>
<li>Faster execution as runtime type checks are minimized</li>
<li>Improved reliability</li>
</ul></li>
<li>Disadvantages:
<ul class="org-ul">
<li>Less flexible</li>
<li>Can be more strict and reject valid programs (false positives!)</li>
<li>Compiler erros can be complex to understand</li>
</ul></li>
</ul></li>
<li>Dynamic type checking: performed during program execution (runtime)
<ul class="org-ul">
<li>Advantages:
<ul class="org-ul">
<li>More flexible and forgiving</li>
<li>Allows for programs that might be rejected by static type checkers</li>
<li>Easier to get started with (less upfront type declaration)</li>
</ul></li>
<li>Disdavantages:
<ul class="org-ul">
<li>Type erros occur at runtime (can be unexpected)</li>
<li>Slower execution due to runtime type checks</li>
<li>Less relaible (errors found later in development/production)</li>
</ul></li>
</ul></li>
<li>Hybrid approaches
<ul class="org-ul">
<li>Combine static and dynamic checking</li>
<li>e.g. Java is mostly stack, but you can do dynamic casting:
<ul class="org-ul">
<li>take <code>(String) o;</code>: the static type system assumes <code>o</code> could be a <code>String</code>, but dynamic check at runtime to ensure it actually is; throw an exception if not</li>
</ul></li>
</ul></li>
<li>Strong typing
<ul class="org-ul">
<li>Definition: Language where you cannot "cheat" about types. System prevents treating a value of one type as another without explicit conversion (and even then, conversions are controlled)</li>
<li>Strongly typed languages can be static (OCaml) or dynamic (Python)</li>
<li>Weakly typed languages (C, C++) allow type "cheating" (e.g. type pruning via pointers and casts)</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-org7f7dd12" class="outline-4">
<h4 id="org7f7dd12"><span class="section-number-4">1.5.4.</span> Type Equivalence</h4>
<div class="outline-text-4" id="text-1-5-4">
<p>
Given two type definitions, are they considered the same type? There are two main approaches: name equivalence and structural equivalence.
</p>

<p>
Under <b>name equivalence</b>, two types are the same only if they have the same name. Used in C, C++ for <code>struct</code> and <code>class</code> typoes. For example,
</p>
<div class="org-src-container">
<pre class="src src-C">struct S { int val; struct S *next; };
struct T { int val; struct T *next; };
struct S v; struct T w;
v = w; // Type error: S and T are different types despite identical structure.
</pre>
</div>
<p>
Under <b>structural equivalence</b>, two types are the same if they have the same structure or representation, regardless of name. Used in <code>C</code> for <code>typedef</code>, e.g.
</p>
<div class="org-src-container">
<pre class="src src-C">typedef int S;
typedef in T;
S x; T w;
x = w; // Allowed: S and T sare structural equivalent (both are int)
</pre>
</div>
<p>
more complex structural equivalence (e.g. for recursive types) can be difficult to define and implement.
</p>
</div>
</div>

<div id="outline-container-orgc3f1f5d" class="outline-4">
<h4 id="orgc3f1f5d"><span class="section-number-4">1.5.5.</span> Abstract vs. Exposed types</h4>
<div class="outline-text-4" id="text-1-5-5">
<p>
<b>Abstract</b> (name equivalence is the way to go)
</p>
<ul class="org-ul">
<li>Implementation details are hidden from users</li>
<li>Users know the type name and operations but not internal structure</li>
<li>Name equivalence is natural for abstract type (structure is hidden anyway)</li>
</ul>
<p>
<b>Exposed</b> (structural equivalence is the way to go)
</p>
<ul class="org-ul">
<li>Implementation details are visible (structure is known)</li>
<li>Structural equivalence might seem more appropriate as implementation is part of the type definition</li>
<li>Exposed types can offer efficiency but reduce flexibility (tight coupling to implementation)</li>
</ul>
<p>
<code>float</code> is partly exposed: IEE754 standard defines bit layout, but bit order is implementation-defined (partly abstract). The <code>float</code>
on most modern computers is stored in such a way such taht we can avoid catastropic cancellation i.e., <code>a - b =</code> 0 &amp;&amp; a &gt; b= can never be true.
</p>
</div>
</div>

<div id="outline-container-org949221d" class="outline-4">
<h4 id="org949221d"><span class="section-number-4">1.5.6.</span> Subtypes</h4>
<div class="outline-text-4" id="text-1-5-6">
<p>
We have types <code>T</code> and <code>U</code> and want to know if \(T \subseteq_T U\), i.e., if every value of type <code>T</code> is also a value of type <code>U</code> and every operation on <code>U</code> is also an operation on <code>T</code>. For example, we could consider the C++ <code>int</code> type to be a subtype of <code>long long int</code>. The caveat here is that there is a change in representation, meaning we need some machine code to convert from an <code>int</code> to a <code>long long int</code>.
</p>

<p>
The set-based interpretation is that a subtype is a subset of values. The object-oriented interpretation (C++ classes) is a that a subclass is a subtype, and typically has more oeprations than the supertype. Take for example:
</p>
<pre class="example">
type day = {Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday}
type weekday = {Monday, Tuesday, Wednesday, Thursday, Friday}
// weekday is a subtype of day.
</pre>
<p>
Function argument type compatibility: if a function <code>f</code> expects an argument of type <code>day</code>, it is typically safe to pass a value of <code>type</code> weekday.
</p>

<p>
Another classic example of subtyping is inheritance. For instance, if we have some class <code>T</code> that extends a class <code>U</code>, doing something like <code>T a; U b; b = a;</code> requires only a pointer assignment and no extra machine code. Another example of subtyping occurs when the programmar puts some constraint on a parent type. For instance, in Pascal: <code>type alpha = 'a'..'z';</code> or in Lisp <code>(declare (type (and integer (satisfies evenp)) x))</code>.
</p>
</div>
</div>

<div id="outline-container-org8bd5cae" class="outline-4">
<h4 id="org8bd5cae"><span class="section-number-4">1.5.7.</span> Polymorphism</h4>
<div class="outline-text-4" id="text-1-5-7">
<p>
Polymorphism means "many forms", and occurs when a function can accept many forms (types).
</p>

<p>
<b>Ad-hoc polymorphism</b>
"Messy" and "idiosyncratic" polymorphism. Rules for polymorphism are often complex, special-case driven and not always intuitive.
</p>

<p>
There are two types of ad-hoc polymorphism: overloading and coercion.
</p>

<p>
Overloading refers to having multiple functions with the same name, but differentiated by argument types. The compiler chooses the correct function based on argument types in the call context.
</p>

<p>
For compiled languages, this could be an issue: take some polymorphic <code>sin</code> function. In the compiled assembly code, there needs to be separate code to deal with each of these cases. We can solve this ambiguity by mangling the names of each of 
these functions (think <code>sin$f</code> and <code>sin$d</code>) but this mangling must be consistent.
</p>

<p>
Coercion is automatic, implicit type conversions performed by the compiler. The compiler inserts conversions to make operation type-compatible.
</p>

<p>
Earlier we saw <code>float x; int i; x * i;</code>:
</p>
<ul class="org-ul">
<li>There is no direct <code>float * int</code> operation</li>
<li>Compiler coerces <code>i</code> to <code>float</code> (implicitly converting <code>i</code> to a floating point representation)</li>
<li>Then performs <code>float * float</code> operations</li>
</ul>
<p>
There are some potential issues!
</p>
<ul class="org-ul">
<li>Unexpected conversion s can lead to bug</li>
<li>For example: comparing <code>uid_t x</code> (unsigned) with <code>-1</code> (int), the coercion to <code>unsigned</code> results in <code>UINT_MAX</code></li>
</ul>
<p>
Another issue: ambiguity in combined overloading and coercion. Take for example:
</p>
<pre class="example">
overloaded function f:
   int f(float x)
   int f(int y, float z)

call: f(3, 7) // both 3 and 7 are integers.
</pre>
<p>
there are multiple possible coercions:
</p>
<ol class="org-ol">
<li>Coerce 3 to <code>float</code>: call <code>f(3.0f)</code>, matches <code>int f(float x)</code></li>
<li>Coerce 7 to <code>float</code>: call <code>f(3, 7.0f)</code>, matches <code>int f(int y, float z)</code></li>
</ol>
<p>
there is ambiguity: compiler might not know which coercion to apply if both are valid and lead to different function calls. In C++, if ambiguity exists are considering coercions, the compiler typically reports an error. Adding a new overloaded function can break existing code due to newly introduced ambiguity.
</p>

<p>
<b>Parametric polymorphism</b>
A more "organized" and "academic" polymorphism. Designed for constructed (user-defined) types. Types are parametrized (type parameters). Polymorphism is achieved by instantiating type parameters with specific types. Examples: generics in Java, templates in C++, type constructors in OCaml (<code>list&lt;'a&gt;</code>). THe idea that you can
"substitute" a type for the type variable without requiring a runtime action/additional code.
</p>

<p>
Traditional Java (pre-generics):
</p>
<ul class="org-ul">
<li><code>Object</code> is the root type, and everything can be treated as an <code>Object</code></li>
<li>Casting (<code>(String) o</code>) used to downcast from <code>Object</code> to specifric types, with runtime checks.</li>
<li>Example: collection of <code>Object</code>. When retrieving from the collection, we need to cast to a <code>String</code> even if you know it should be strings.</li>
</ul>
<p>
Take for example
</p>
<div class="org-src-container">
<pre class="src src-java">Collection c = ...;
Iterator i = c.iterator();
while (i.hasNext()) {
    if (((String) i.next()).length() == 1) {
        i.remove(); // Cast to string required
    }
}
</pre>
</div>
<p>
the problem is that <code>(String) i.next()</code> is needed, even though the programmer knows its a collection of string. This subverts the type checking, and type safety is no longer guaranteed at compile time as a runtime cast exception is possible.
</p>

<p>
Modern Java (has generics)
</p>
<ul class="org-ul">
<li>Generic types allow parameteerizing types (like type constructors in OCaml)</li>
<li>For example, <code>Collection&lt;String&gt;</code> is a collection specifically on <code>Strings</code></li>
<li>Type safety is enforced at compile time, and we can avoid casts in most cases.</li>
</ul>
<p>
Take for example
</p>
<div class="org-src-container">
<pre class="src src-java">Collection&lt;string&gt; c = ...; // Collection, specifically of Strings
Iterator&lt;String&gt; i = c.iterator();
while (i.hasNext()) {
  if (i.next().length() == 1) {
      i.remove(); // No cast needed, i.next() is a string
  }
}
</pre>
</div>
<p>
the advantages of generics (parametric polymorphism in Java) are
</p>
<ul class="org-ul">
<li>Improved type safety (compile-time checks)</li>
<li>Reduced need for casts</li>
<li>Potentially faster execution (fewer runtime type checks)</li>
</ul>

<p>
<b>Generics vs. Templates</b> (Java vs. C++)
</p>
<ul class="org-ul">
<li>Key differences in implementation and behavior</li>
<li>Generic types (Java, OCaml)
<ul class="org-ul">
<li>Have <i>type erasure</i> (Java at runime). Compile once, run anywhere. This works under the assumption that all types "smell the same": that you can treat all variables the same. Generics are erased to <code>Object</code> meaning you must use wrapper classes, and you cannot check the actual type at runtime.
<ul class="org-ul">
<li>Every generic value is 64-bit</li>
</ul></li>
<li>Compiler checks types and generates a single version of machine code for generic class/function, regardless of type parameters.</li>
<li>Type information like (like <code>&lt;String&gt;</code>) is mostly used for compile time checking and is often ereased or abstracted away at runtime (type erasure)</li>
<li>Cleaner, more type-safe; less flexible for specialization</li>
</ul></li>
<li>Templates (C++)
<ul class="org-ul">
<li>We have code instantiation/specialization</li>
<li>For each different type parameter instantiation (e.g. <code>list&lt;int&gt;</code> and <code>list&lt;float&gt;</code>, the compiler generates separate machine code and performs type checking specifically for that instantiation.</li>
<li>More flexible: templates can be specialized for particular types, allowing for optimized for specific cases.</li>
<li>Can lead to code bloat</li>
<li>Lower-level, more focused on efficiency and control. Potentially more complex type errors during tempalte instantiation.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgb6562c3" class="outline-4">
<h4 id="orgb6562c3"><span class="section-number-4">1.5.8.</span> Subtype Invariance</h4>
<div class="outline-text-4" id="text-1-5-8">
<p>
Consider the following list subtyping issue in Java:
</p>
<div class="org-src-container">
<pre class="src src-java">List&lt;String&gt; ls = ...;
List&lt;Object&gt; lo = ls; // Not allowed in Java - compile-time error!
lo.add(new Thread()); // Adding a Thread to what was originally a list of strings: type violation!
String s = ls.get(); // Attempting to get a String from a list that might now contain a Thread - Runtime ClassCastException
// The core problem is Java's mutability: lists are mutable, there is no .add
</pre>
</div>
<ul class="org-ul">
<li>The problem: contravariant subtype relationship breakdown. Although <code>String</code> is a subtype of <code>Object</code>, <code>List&lt;String&gt;</code> is not a subtype of <code>List&lt;Object&gt;</code>.</li>
<li>The reason: mutability and type safety violation. Allowing <code>List&lt;String&gt;</code> to be treated as <code>List&lt;Object&gt;</code> would break type safety in Java due to list mutability. You could add a non-String object (like <code>Thread</code>) to a list that was supposed to hold only <code>Strings</code>, leading to runtime errors (<code>ClassCastException</code>) when you later try to retrieve and use the elements as <code>Strings</code>.</li>
<li>Subtype invariance of generic types in Java: Java generics are invariant by default. <code>List&lt;String&gt;</code> and <code>List&lt;Object&gt;</code> are considered unrelated types in terms of subtyping, even if <code>String</code> is a subtype of <code>Object</code>.</li>
</ul>
<p>
The standard rule of subtypes: if \(X \subseteq_T Y\), then every value of type \(X\) should be useful in a context that wants type \(Y\). Or conversely, every operation on \(Y\) values should work on \(X\) values. There's a similar problem in C/C++. Consider the following
</p>
<div class="org-src-container">
<pre class="src src-cpp">// Quick definitions
char*; // pointer to a character
char const*; // pointer to a constant character
const char*; // same as above
char* const; // constant pointer to mutable character

// Example
char c = 'A';
char const *p = &amp;c;
printf(*p);
f(&amp;c); // what we pass here is of type char*, so f is allowed can modify c
print(*p); // in this case, the two things will print different things

// Then what we really have is:
char const *p; // "Pointer to character that can't be modified via this pointer."
// As a result: char* \subseteq char const *
// Assigning from a subtype to a supertype, and that's OK
// Every value of type char* can be used in the context of char const *, not necessarily the other way around!
</pre>
</div>

<p>
<b>How to tackle this problem: wildcards</b>
</p>
<div class="org-src-container">
<pre class="src src-java">void printall(Collection &lt;?&gt; c) {
  for (Object i : c)
      System.out.println(i);
}
</pre>
</div>
<p>
We can use an <i>unbounded wildcard</i>: <code>List&lt;?&gt;</code>. The <code>?</code> represnets an unknown type, this list can be of any  object type. This is useful for methods that only read from a list and don't care about the specific type. For example, <code>printList(List&lt;?&gt; l)</code> can print any list of objects (or subtypes), but cannot safely added elements to it (due to type uncertainty).
The type safety tradeoff is that <code>List&lt;?&gt;</code> is more flexible (accepts lists of various types), but less type-safe for operations that modify the list.
</p>

<p>
Now take for example the following scenario:
</p>
<div class="org-src-container">
<pre class="src src-java">public void printShapes(Collection&lt;?&gt; shapes) {
  for (Shape s : shapes) {
      printShape(s); // Assume objects of type shapes have this method
  }
}
// The problem is that by using &lt;?&gt;, any object could fit here, but only shapes have printShape
</pre>
</div>
<p>
to fix this, we introduce a <i>bounded wildcard</i>: use <code>printShapes(Collection&lt;? extends Shape&gt; shapes)</code>, where the unknown type <code>?</code> must be a <i>subtype</i> of <code>Shape</code> (for reading out).
If we instead wanted supertypes to be accepted (but not subtypes), use <code>&lt;? super Shape&gt;</code> (for storing stuff), where we have <code>Shape</code> up through object inclusive, but nothing else is allowed. You can (probably) use both at the same time.
</p>

<p>
<b>Problems with bounded wildcards</b>
</p>

<p>
Consider the following:
</p>
<div class="org-src-container">
<pre class="src src-java">void cvt( ? []ar, Collection&lt;?&gt; cs) {
  for (Object o : ar)
      co.add(o);
}
</pre>
</div>
<p>
this code is too generous: you can copy from an array of strings to a collection of threads! What we need is
some way to indicate that both unknown types <i>are the same</i>. We can accomplish this <i>type variables</i>.
</p>
<div class="org-src-container">
<pre class="src src-java">&lt;T&gt; void cvt( T []ar, Collection&lt;T&gt; cs) {
    for (T o : ar) // hidden iterator is Iterator&lt;T&gt;
        co.add(o);
  }
</pre>
</div>
<p>
this will work, but only if all the types exactly match. However, there is a limitation: we cannot e.g. copy from <code>Square[]</code> to <code>Collection&lt;Shape&gt;</code> because <code>Square</code> and <code>Collection&lt;Shape&gt;</code> are different types,
even though <code>Square</code> is a subtype of <code>Shape</code>.
</p>

<p>
To solve this, we could try to use the upper bound wildcard: <code>&lt;T&gt; void cvt (T [] ar, Collection&lt;? extends T&gt; co) { ... }</code>. Note briefly that wherever we use an unknown type once (like <code>?</code>) we can give it an actual name. <code>?</code> is just syntactic sugar. This is incorrect for this use case (read from array, write to collection).
</p>

<p>
It doesn't allow copying from <code>Square[]</code> to <code>Collection&lt;Shape&gt;</code> because type parameter <code>T</code> must be exactly the same for both array and collection.
</p>

<p>
Instead, we use the lower bounded wildcard (<code>super</code>): <code>&lt;T&gt; void cvt (T [] ar, collection&lt;? super T&gt; co) { ... }</code>. Now,
</p>
<ul class="org-ul">
<li><code>Collection&lt;? super T&gt; co:</code> collection <code>co</code> can be of type <code>T</code> or any supertype of <code>T</code> (e.g. if <code>T</code> is <code>Square</code>, we can have <code>Collection&lt;Square&gt;, Collection&lt;Shape&gt;, Collection&lt;Object&gt;</code>)</li>
<li>Now <code>cvt</code> works for copying from <code>Square[]</code> to <code>Collection&lt;Shape&gt;</code> because <code>Shape</code> is a supertype of <code>Square</code>.</li>
</ul>

<p>
Implementation note:
</p>
<ul class="org-ul">
<li>We can think about Java (and other OOP) languages as having the following implementation:
<ul class="org-ul">
<li>Every object is represented by a pointer to the value of that object. In a strongly typed language, there is a type field there.</li>
<li>In user-defined type languages, that type field would point to another piece of storage, which we could call a type descriptor (what am I subtype of? what's my name? etc).
<ul class="org-ul">
<li>This object won't change as you run, there mostly for type checking</li>
<li>What does the type descriptor look like when you use <code>?</code>, or e.g. <code>List&lt;Integer&gt;</code> or <code>List&lt;String&gt;</code> etc.</li>
<li>In Java, the design decision is to say there is only one type descriptor for <code>List</code>, for efficiency</li>
<li>Instead, it does type erasure: all we know at runtime is effectively <code>List&lt;?&gt;</code>. To do checking, you have to do these checks at runtime.</li>
</ul></li>
</ul></li>
</ul>

<p>
<b>Note on tree-like class hierarchy</b>
</p>

<p>
In Java, the type hierarchy is typically a tree, where:
</p>
<ul class="org-ul">
<li>Every class has a single direct superclass, and all classes ultimately inherit other interfaces.</li>
<li>There is an exceptional case where it can form a DAG: interfaces and multiple inheritance through interfaces.</li>
</ul>
</div>
</div>

<div id="outline-container-orgda35a6c" class="outline-4">
<h4 id="orgda35a6c"><span class="section-number-4">1.5.9.</span> Duck Typing</h4>
<div class="outline-text-4" id="text-1-5-9">
<p>
Motivation: can we do something simpler? Yes: duck typing: "if it quacks like a duck, it's a duck."
</p>

<p>
The type of an object is a bad notion - we shouldn't care about types. Focus on behavior (methods an object supports) rather than type (class hierarchy, type declarations). Type is determined by runtime behavior, not static declarations.
</p>

<p>
Take the following example in python:
</p>
<div class="org-src-container">
<pre class="src src-python">def process_object(obj):
  obj.quack()
  obj.waddle()
  obj.hasRoundedBeak()
</pre>
</div>
<p>
Notice that there are no type declarations and no type annotations for the <code>obj</code> parameter. At runtime, Python will calles these methods on that object to resolve. There's no compile-time type errors if <code>obj</code> doesn't have these methods, but there are runtime
errors (exceptions): if <code>obj</code> does not have these methods when <code>process_object</code> is called, a runtime exceptions occurs: "Duck-typing error" - object doesn't behave like a duck at runtime.
</p>

<p>
<b>Advantages</b>
</p>
<ul class="org-ul">
<li>Simplicity: very simple type system (or lack thereof). No need for complex generic type declarations or subtype rules.</li>
<li>Flexibility: Highly flexible. Functions can work with objects of various types as long as they support the required methods</li>
<li>Code reusability: Encourages code reuse. Functions can be more generic and work with a wider range of objects.</li>
<li>Rapid development: easier to write code quickly without being constrained by strict type systems.</li>
</ul>
<p>
<b>Disadvantages</b>
</p>
<ul class="org-ul">
<li>Runtime errors: Type errors are detected at runtime, not compile time. Errors can be delayed and harder to catch early in development.</li>
<li>Reduced reliability: Less type safety compared to statically typed languages. Potential for unexpected runtime errors due to type mismatches</li>
<li>Debugging challenges: Debugging type-related errors can be more difficult as errors surface only at runtime, potentially far from the source of the type issue.</li>
<li>Performance overhead (Dynamic Dispatch): Method calls might be slightly slower due to dynamic method resolution at runtime (interpreter needs to look up methods at runtime</li>
</ul>
</div>
</div>

<div id="outline-container-org4d138a3" class="outline-4">
<h4 id="org4d138a3"><span class="section-number-4">1.5.10.</span> Static typing (Generics/Java) vs. Dynamic typing (Duck typing/Python)</h4>
<div class="outline-text-4" id="text-1-5-10">
<ul class="org-ul">
<li>Static typing (Java):
<ul class="org-ul">
<li>Pros: Compile-time type safety, early error detection, potentially better performance (less runtime overhead), improved reliability for large, complex systems.</li>
<li>Cons: Less flexible, more verbose code, can be stricter and reject valid programs, steeper learning curve for complex type features (like generics and wildcards).</li>
</ul></li>
<li>Dynamic Typing (Python):
<ul class="org-ul">
<li>Pros: More flexible, concise code, faster development, easier to get started, more forgiving type system</li>
<li>Cons: Runtime type errors, reduced reliability, potential performance overhead, harder to debug type issues in large projects, less suitable for safety-critical applications</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org51a0080" class="outline-3">
<h3 id="org51a0080"><span class="section-number-3">1.6.</span> Java</h3>
<div class="outline-text-3" id="text-1-6">
<p>
<b>At a glance</b>,
</p>

<p>
A first few notes about Java: it's object-oriented programming. Java was meant to be slightly higher level, which we do
for portability, reliability, and performance (small loss compared to C++). A big plus over C++ and C that's easier in Java is concurrency.
</p>

<p>
Java is closely related to the idea of bytecodes: standard bytecode representaton for Java programs that is intended to be portable. Like machine code, except for an abstract machine that doesn't really exist, it's a stack oriented machined that is run by an interpreter + JIT compiler. 
A major difference between Java and C++ is that it has single inheritance, for simplicity and performance (vs. C++ having multiple inheritance), think like a tree. A single class cannot have two superclass, can only have one parent.
</p>

<p>
One special difference in Java vs. C++ are arrays. Arrays in Java are <b>objects</b>, so they live on the heap and are garbage collected, which takes a performance hit but has a reliability advantage, additionally, you can return the array since it lives on the heap.
This, however, gives the opportunity for the Java compiler to optimize it, and it could put it on the stack. Also, the size of an array can be deduced at run-time (<code>a[?]</code>, vs. must be available compile-time for C++). However, the size is fixed once allocated.
</p>
</div>

<div id="outline-container-org436a155" class="outline-4">
<h4 id="org436a155"><span class="section-number-4">1.6.1.</span> <code>abstract</code></h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
A subclass method can "shadow" a parent's (superclass). It's considered to be good practice to have the shadow method to be an "implementation" of the super method.
</p>

<p>
There are some exceptions. For example, the <code>abstract</code> class are classes that come without implementation. For <code>abstract</code> method, this forces the children to implement the marked method. Any class with an abstract method can't be instantiated: we need to initialize a subclass instead.
</p>

<p>
The opposite of <code>abstract</code> is a <code>final</code> class or method, which means it cannot be subclassed or overriden. This helps with inlining, since we can substitute the body of the method directly. Another reason that we have this is <i>trust</i>: to make sure your code is clear, you can use <code>final</code> to ensure bad things can't happen with bad subclass implementations. The other reason to use <code>final</code> is for debugging: you can make guarantees that your method won't be subclassed and won't cause errors that way, and can be helpful for debugging.
</p>
</div>
</div>

<div id="outline-container-orgc81968b" class="outline-4">
<h4 id="orgc81968b"><span class="section-number-4">1.6.2.</span> <code>interface</code></h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
This brings us to a different idea. We have <code>interfaces</code>, which we only define APIs and no implementations. In contrast to classes,
Java allows implementation of multiple interfaces. This allows us to place constraints on objects from an API
perspective instead of an inheritance perspective. An <code>abstract</code> class can be thought of as bundling a concrete class and an interface together.
</p>
<div class="org-src-container">
<pre class="src src-java">interface I { Java
 int m();
 int n(float);
}
class E extends C implements I, J, K { ... }

void p(I x) { Java
 x.run();
}
E o = new E();
p(o);
</pre>
</div>
</div>
</div>

<div id="outline-container-org8baa489" class="outline-4">
<h4 id="org8baa489"><span class="section-number-4">1.6.3.</span> Java <code>object</code></h4>
<div class="outline-text-4" id="text-1-6-3">
<div class="org-src-container">
<pre class="src src-java">class Object {
    // Why did we do this? One reason could be *sentinel values*: special value used in
    // algorithms to mark a boundary or to signal a special condition. It's chosen because it's
    // guaranteed not to occur as a normal element in your data. When you create a new Object(),
    // you get an instance that is unique and does not carry any additional data. Because of its generic
    // nature, you you can be sure it won’t naturally appear in your collection of meaningful data.
    // By inserting this unique, “empty” object into a data structure (like an array), 
    // you can use it as a sentinel—a flag that indicates something special.
    public Object();

    // For any object o, you have an equals method
    // Distinct from == operator! Let designer dictate equality (semantic vs. address)
    // By default goes to ==. Up to the designer!
    public boolean equals(Object obj);

    // Keyword Class is an object that represents compile-time class
    // A capital 'C' Class is an object that stands for lower 'c' class that you wrote
    // in your program, and they live in the object hierarchy!
    // Not an object at runtime! No lying about your type!
    // Good for debugging. Final because efficiency and *trust* (debug, types)
    public final Class&lt;? extends X&gt; getClass();
    // Where X is the type of O (object), i.e., O.getClass()

    // By default hashes the objects internal address, but you can redefine.
    // But! equals and hashCode should be consistent. If equals returns true,
    // then the two objects should have the same object.
    public int hashCode();

    // Get a string representation of the object.
    public string toString();

    // For any object, you can clone it. Except, for some objects,
    // we can throw an exception against cloning.
    // The idea is that we get a copy of the top level of the original object.
    // Figure out how big the object is, copy all the bytes into a new object.
    // Protected means "not intended for general use." The GC can call it... others should probably not.
    protected Object clone() throws CloneNotSupportedException;

    // Throw whatever it likes, the finalize methods is a hook into the GC.
    // Right before the GC claims the object, it calls finalize().
    // Apparently protected and finalize() don't jive well
    protected void finalize() throws Throwable;

    // Soon: wait(), notify(), notifyall()
}
</pre>
</div>
<p>
Note on <code>sentinel</code>:
</p>
<div class="org-src-container">
<pre class="src src-java">Object sentinel = new Object(); // our unique sentinel value
data[data.length - 1] = sentinel; // place it at the end

int i = 0;
while (data[i] != target &amp;&amp; data[i] != sentinel) {
    i++;
}
if (data[i] == target) {
    // found the target
} else {
    // target not found (because we hit the sentinel)
}
</pre>
</div>
</div>
</div>
<div id="outline-container-org91019fc" class="outline-4">
<h4 id="org91019fc"><span class="section-number-4">1.6.4.</span> Java <code>threads</code></h4>
<div class="outline-text-4" id="text-1-6-4">
<p>
Java implements threads using a <code>thread</code> object, and it's under the object hierarchy. Each thread object represents
a distinct thread of execution: has it's own PC/CPU. Even subclasses has the same idea (distinct threads).
</p>

<p>
We could write thread methods like so:
</p>
<div class="org-src-container">
<pre class="src src-java">class T extends Thread {
  void run() {
      doSomething();
  }
}

T t = new T();
t.start();
</pre>
</div>
<p>
note that we override <code>Thread</code>'s run method. This way is too restrictive, because every class that needs to do multithreading
must "match up" in the object hierarchy, i.e., has to be a sublass or superclass. Instead, we use the <code>Runnable</code> interface
</p>
<div class="org-src-container">
<pre class="src src-java">class RT implements Runnable { Java
    void run() {
        doSomething();
    }
}
Thread t = new Thread(new RT());
t.start();
</pre>
</div>
<p>
all you need is a <code>run()</code> method! This is the more common and flexibly way.
</p>

<p>
A thread's life cycle begins when created with <code>new</code>. At this point, the thread exists but hasn't started yet. At this point (NEW),
we can fiddle with the thread, change priority, etc. Once we call <code>.start()</code>, the OS allocates resources (virtual CPU, instruction pointer: PC, stack, etc) and the thread can be run (RUNNABLE, once it's here it can start doing stuff, and it stays RUNNABLE).
</p>

<p>
At this point, the thread can sleep (TIMEDWAITING), wait (WAITING), do I/O (BLOCKED), yield (still, RUNNABLE, this just "yields" to some other thread), or execute code (also RUNNABLE).
Finally, we can exit (<code>return</code> from <code>run()</code> method) and the thread becomes TERMINATED. Note that the thread object still exists: "someone else" may come along and examine the "grave".
</p>
</div>
</div>

<div id="outline-container-org818e317" class="outline-4">
<h4 id="org818e317"><span class="section-number-4">1.6.5.</span> Race conditions</h4>
<div class="outline-text-4" id="text-1-6-5">
<p>
A race condition is when the order of execution changes behavior.
</p>

<p>
The classic example is that thread A writes and thread B reads at the same time (inconsistent state, partial writes).
</p>
<ul class="org-ul">
<li>How can you reproduce a race condition?
In general it's very difficult. You can go into a debugging environment and note the timestamps where everything has occurred and then investigate the log. However, when you turn on logging, you are changing the timing! You slowed the program down, and so may not reproduce the same race condition.</li>
</ul>

<p>
Synchronized methods are one solution. We can mark a method as a <code>synchronized</code>, which causes the Java compiler to insert code to grab/release locks at the beginning/end of function execution.
</p>
<ul class="org-ul">
<li>These locks are <b>spin locks</b> (<code>while</code> loop type lock): unlike traditional locks that might put a thread to sleep if it cannot acquire the lock immediately, a spin lock causes the thread to repeatedly check (or "spin") in a tight loop until the lock becomes available.
<ul class="org-ul">
<li>Because spinning avoids the overhead of system calls for putting threads to sleep and waking them up, spin locks are efficient when the wait time is expected to be very short.</li>
<li>The tradeoff is that spinning consumes CPU cycles. In scenarios where the wait is long, spin locks can lead to inefficient CPU usage, as the spinning thread does no useful work while waiting.</li>
<li>A typical strategy would be that any reads/writes to some object done in a threaded environment are done from synchronzied methods. Many basic classes in Java have synchronized methods (synchronized hash tables). This is fine if accesses are rare or if clients are slow.
<ul class="org-ul">
<li>Note that even if we only do reads, we still need to lock the object in order to prevent writes from occurring during a read.</li>
</ul></li>
<li>A thread that is trying to acquire a spin lock, is under RUNNABLE, and stays RUNNABLE. We want to add supports for some of these other states.</li>
</ul></li>
</ul>

<p>
In Java, every object is associated with a <i>monitor</i>. This monitor acts as a built-in lock mechanism that controls access to the object’s synchronized code blocks and methods.
When you declare a method or a block of code as <code>synchronized</code>, the thread executing that code must acquire the object's monitor. Only one thread can hold that monitor at a time, which prevents other threads from entering any other synchronized block that locks on the same object.
This mechanism is built into the Java language and the <code>java.lang.Object</code> class, meaning every Java object automatically has this synchronization capability without requiring explicit lock objects.
An object's monitor is essentially the internal lock that Java uses to coordinate synchronized access and communication between threads.
</p>

<p>
When a thread calls <code>wait()</code> from within a synchronized context, it temporarily releases the lock it holds. This allows other threads to enter the synchronized block or method and work with the object.
The thread that calls <code>wait()</code> goes into a waiting state until some other thread signals that the condition it was waiting for might now be true. The thread is not actively consuming CPU resources while waiting.
Once notified, the waiting thread must re-acquire the lock before it can resume execution. This means that even after being woken up, it might have to wait if another thread currently holds the lock.
</p>

<p>
<code>o.notify()</code>: This method wakes up a single thread that is waiting on the object’s monitor. The exact choice of which waiting thread to wake up is often left to the operating system’s thread scheduler and may seem random (or sometimes based on how long the threads have been waiting). The key point is that the thread calling <code>notify()</code> is not the one waiting; it’s another thread that signals that it’s done using the object.
</p>

<p>
<code>o.notifyAll()</code>: In contrast, <code>notifyAll()</code> wakes up every thread that is waiting on the object’s monitor. This gives the awakened threads a chance to compete for the lock, which can allow the application to implement its own policy about which thread should proceed. This is more flexible but can be harder to manage because all waiting threads become active, potentially leading to a burst of contention.
</p>

<p>
With <code>notify()</code>, you let the operating system (or rather the underlying thread scheduler) choose which thread to wake up. With <code>notifyAll()</code>, you hand over more control to the application because all waiting threads are awakened and then must re-compete for the lock based on the application’s logic and their own scheduling.
</p>
</div>
</div>

<div id="outline-container-orgab09131" class="outline-4">
<h4 id="orgab09131"><span class="section-number-4">1.6.6.</span> Semaphores</h4>
<div class="outline-text-4" id="text-1-6-6">
<p>
Semaphores are higher-level synchronization primitives built on top of the lower-level wait/notify mechanisms in Java. A semaphore maintains a count (its capacity) that represents how many permits (or resources) are available. A binary semaphore is simply a semaphore with a capacity of 1, while a counting semaphore can have a capacity greater than 1. In Java, we have
</p>
<ul class="org-ul">
<li><code>s.acquire()</code>: which blocks the calling thread until a permit is available. When a permit is acquired, the semaphore’s internal count is decremented.</li>
<li><code>s.tryAcquire()</code>: which attempts to acquire a permit without blocking. It returns a Boolean value indicating whether the acquisition was successful.</li>
<li><code>s.release()</code>: releases a permit, incrementing the semaphore’s count and potentially unblocking a waiting thread.</li>
</ul>
<p>
Semaphores are useful when you want to limit access to a shared resource (for example, a pool of database connections or a fixed number of identical resources). If the resource pool has a capacity of 10, for instance, up to 10 threads can acquire a permit concurrently. When an 11th thread attempts to acquire a permit, it must wait until one of the existing threads calls <code>release()</code>.
</p>
</div>
</div>

<div id="outline-container-orgb4a24eb" class="outline-4">
<h4 id="orgb4a24eb"><span class="section-number-4">1.6.7.</span> Exchanges</h4>
<div class="outline-text-4" id="text-1-6-7">
<p>
An exchanger is a high-level synchronization object that provides a simple rendezvous point where two threads can exchange data with each other.
</p>

<p>
Specifically, it is designed for  pairs of threads. One thread calls the exchange method with its data (for example, calling <code>x.exchange(v)</code>), and it then waits at the rendezvous point until a partner thread also calls the same method with its data (for example, <code>x.exchange(A)</code>).
When both threads have reached the exchange point, the exchanger swaps the objects they provided. The operation is atomic: both threads are blocked until the exchange can occur. This ensures that neither thread proceeds until it has successfully received the object from its partner, making it useful for coordinated data transfers.
This pattern is especially useful when two threads need to collaborate by exchanging results or resources without having to resort to more complex synchronization techniques. It guarantees that both threads know that the exchange has happened before they continue.
</p>

<p>
Take the following example
</p>
<div class="org-src-container">
<pre class="src src-java">T1: a = e.exchange(b);
T2: c = e.exchange(d);
a == d &amp;&amp; c == b
</pre>
</div>
<ul class="org-ul">
<li>Thread 1 calls <code>exchange(b)</code> with its own value <code>b</code>.</li>
<li>Thread 2 CALLS <code>exchange(d)</code> with its own value <code>d</code>.</li>
</ul>
<p>
The exchanger works as a rendezvous point. When one thread reaches the <code>exchange()</code> call (say, T1), it waits
for a partner to arrive. Once thread 2 reaches its <code>exchange()</code> call, the exchanger pairs them together and swaps the values:
</p>
<ul class="org-ul">
<li>T1 receives the value <code>d</code> from T2, so <code>a</code> becomes equal to <code>d</code>.</li>
<li>T2 receives the value <code>b</code> from T1, so <code>c</code> becomes equal to <code>b</code>.</li>
</ul>
<p>
Then, after the exchange the relationships holds that <code>a =</code> d &amp;&amp; c <code>= b</code>. One thread must wait; the other doesn't have to.
</p>
</div>
</div>

<div id="outline-container-org7dc3dba" class="outline-4">
<h4 id="org7dc3dba"><span class="section-number-4">1.6.8.</span> Countdown Latch</h4>
<div class="outline-text-4" id="text-1-6-8">
<p>
A countdown latch (barrier) makes all the threads wait, run a little code, then say "go" and let them continue running.
There is a related notion called a cylic barrier, which is a countdown latch that repeats. Think of it as a race where, after each lap, all horses stop at the barrier. They wait until every horse has completed the lap before starting the next lap together.
</p>

<p>
How can we make Java goes fast? Spin locks are slow. The machine instructions may cause OoO execution. The rule in Java is if a single thread doesn't know different, then the compiler can do it in any order ("as-if rule").
If we do this in a synchronized method, there could be issues! So there are exceptions: lock and unlock prevent re-ordering (generally).
We could use the <code>volatile</code> keyword, which indicates to Java not to optimize access to a variable.
</p>
</div>
</div>

<div id="outline-container-org5524a05" class="outline-4">
<h4 id="org5524a05"><span class="section-number-4">1.6.9.</span> Java Memory Model</h4>
<div class="outline-text-4" id="text-1-6-9">
<p>
<b>Preamble</b>: image a universe with only one spatial dimension (an X-axis) and one time dimension (a t-axis). In this simplified universe, your “location” and the evolution of events over time are graphed much like a Minkowski diagram in special relativity.
In physics, the speed of light (set to 1 in the analogy) is the ultimate limit for the transmission of information. This creates three regions:
</p>
<ul class="org-ul">
<li>Future: The region that can be influenced by an event at (0,0).</li>
<li>Past: The region from which events could have affected the present.</li>
<li>Elsewhere: Regions that lie outside the light cone, where no causal influence is possible.</li>
</ul>
<p>
The analogy maps onto multithreaded programming in that one thread “sends a message” by modifying a shared object. Just as an event in spacetime is limited by the speed of light, changes in shared memory are subject to constraints on visibility and ordering—if two threads try to communicate via a shared variable, they must respect these “causal” rules. Much like in relativity, there is a “region” (or a set of rules) within which messages (i.e., memory updates) can reliably be seen by other threads.
</p>

<p>
In a multithreaded Java program, one thread may modify a shared object while another thread is reading or also writing to it. Without a strict ordering or synchronization, these operations can “step on each other’s toes,” resulting in unpredictable behavior.
Just as you cannot send a message to a region outside your light cone, threads can only reliably “send messages” (i.e., share memory updates) if the operations are properly ordered. Race conditions occur when these messages get garbled because the timing and ordering of operations are not controlled.
</p>

<p>
This in some ways motivates the <b>Java memory model</b>, which was developed to define precisely when and how threads can reliably communicate through shared memory. It specifies rules for the ordering of reads and writes and tells compiler writers what optimizations (like reordering instructions) are allowed.
</p>

<p>
First, compilers and CPUs are allowed to reorder instructions (e.g., caching values in registers) as long as the changes are not observable in a single-threaded context. This is also known as the <b>as-if</b> rule.
</p>

<p>
This will not work in a multithreaded context. In multithreaded programs, one thread’s view might be different from another’s if reordering leads to race conditions.
</p>
</div>
</div>

<div id="outline-container-org69f9c45" class="outline-4">
<h4 id="org69f9c45"><span class="section-number-4">1.6.10.</span> Digression on <code>volatile</code></h4>
<div class="outline-text-4" id="text-1-6-10">
<p>
They keyword <code>volatile</code> in Java tells the compiler and runtime that a variable’s value can change unexpectedly (like a volatile chemical that evaporates). Every access to a volatile variable must reflect the actual value in memory-<i>no caching or reordering of those accesses is allowed</i>.
</p>

<p>
Volatile variables are used for simple message passing between threads. For example, one thread can spin in a loop waiting for a volatile variable to change, ensuring that each check reads the latest value. Take the following
</p>
<div class="org-src-container">
<pre class="src src-java">// Shared variable (if not volatile)
boolean done = false;

// Thread A:
while (!done) {
    // Busy-wait until done becomes true
}
// Continue when done is true

// Thread B:
done = true;
</pre>
</div>
<p>
if <code>done</code> is a normal (non-volatile) variable, the compiler or processor might cache its value in a register. This means that Thread A might read the cached value and never see the change made by Thread B. The loop might never terminate.
By declaring the variable as volatile (<code>volatile boolean done = false;</code>), every read of <code>done</code> must check the actual memory value. Thread A will see the update made by Thread B, and the loop will eventually terminate.
</p>

<p>
There are some key <b>limitations</b>. Volatile prevents certains kinds of reordering, and is not sufficient to implemenent
full locking mechanisms (like a <code>synchronized</code> block) because race conditions can still occur if multiple operations need to be atomic. For example,
a naive spin lock using only a volatile flag can still fail due to timing issues:
</p>
<div class="org-src-container">
<pre class="src src-java">class SpinLock {
    // Using volatile so that changes are visible across threads
    private volatile boolean locked = false;

    public void lock() {
        // Spin until we see that locked is false
        while (locked) {
            // Busy-wait (spin)
        }
        // Once we exit, we assume locked is false, so we set it to true
        locked = true;
    }

    public void unlock() {
        locked = false;
    }
}
</pre>
</div>
<p>
the volatile declaration ensures that each read of <code>locked</code> gets the up-to-date value. However, we can still construct a race condition:
</p>
<ul class="org-ul">
<li>Thread A checks <code>while (locked)</code> and sees <code>false</code>.</li>
<li>Before Thread A sets <code>locked = true;</code>, Thread B also checks <code>while (locked)</code> and sees <code>false</code>.</li>
<li>Both threads then set <code>locked = true</code>, leading to a situation where both believe they hold the lock.</li>
</ul>
<p>
The problem is <i>not visibility</i> (<code>volatile</code> provides that), but <b>atomicity</b>. The check-and-set sequence isn’t atomic. For proper locking, you need hardware-supported atomic operations (or use higher-level constructs like <code>synchronized</code>) that ensure the check and update happen as one indivisible step.
</p>
</div>
</div>

<div id="outline-container-org0b7cd99" class="outline-4">
<h4 id="org0b7cd99"><span class="section-number-4">1.6.11.</span> Returning to the JMM</h4>
<div class="outline-text-4" id="text-1-6-11">
<p>
First, we have to start with definitions. Recall that <b>every Java object has an associated monitor</b> (intrinsic lock, part of its definition like its type).
When you enter a <code>synchronized</code> block or method, the thread must acquire this monitor. These actions are conceptualzed as
</p>
<ul class="org-ul">
<li><b>Enter monitor</b> (Lock): The thread <i>grabs</i> the lock before executing the critical section.</li>
<li><b>Exit monitor</b> (Unlock): The thread releases the lock upon leaving the critical section.</li>
</ul>
<p>
These synchronized blocks ensure that only one thread can execute the protected code at a time, thereby preventing race conditions.
Whereas volatile enforces ordering on individual variable accesses, synchronized blocks guarantee mutual exclusion and a stronger form of visibility. They also involve two primitives (enter and exit monitor) that form a part of the memory model’s rules.
</p>

<p>
At a high level, the Java memory model is trying to describe "causal" relationships between memory operations across threads.
Operations that are “in the future” (or outside the allowed reordering boundaries) cannot affect the present computation reliably.
</p>

<p>
To formalize, what reordering of computation is allowed, the JMM conceptually divides operations into three categories:
</p>
<ul class="org-ul">
<li><b>A</b>: Ordinary (non-volatile) loads and stores. These are the most common and are heavily optimized.
<ul class="org-ul">
<li>Operations within category A can be freely reordered relative to one another if doing so does not change the observable behavior in a single-threaded context.</li>
<li>Simple, common example would be  two non-dependent assignments, where the compiler might swap them or even eliminate redundant loads/stores.</li>
</ul></li>
<li><b>B</b>: Volatile loads and “enter monitor” operations. These operations have stronger ordering constraints because they must fetch the latest value or acquire the lock.
<ul class="org-ul">
<li>A <i>volatile load</i> means reading a volatile variable, which requires getting the up-to-date value from memory.</li>
<li><i>Entering a monitor</i> (or acquiring a lock via a synchronized block) is a synchronization operation that establishes a “happens-before” relationship.</li>
<li>In terms of <b>reordering constraints</b>, these operations create a barrier. When a normal operation (category A) is immediately followed by a category B operation, the compiler is allowed to swap them as long as it doesn’t change what an observer (another thread) can see.</li>
<li>For example, a normal load that precedes an acquire (enter monitor) might be moved after the monitor entry—this effectively “expands” the critical section. However, this movement is allowed only if it doesn’t let an external observer see a result that violates the expected ordering.</li>
</ul></li>
<li><b>C</b>: Volatile stores and “exit monitor” operations. These also enforce ordering, ensuring that changes are visible once the lock is released or the volatile variable is written.
<ul class="org-ul">
<li>A <i>volatile store</i> means writing to a volatile variable and ensuring that the value is immediately visible to all threads.</li>
<li><i>Exiting a monitor</i> (or releasing a lock) is also a synchronization action that “publishes” the changes made in a critical section.</li>
<li>In terms of <b>reordering constraints</b>, if you have a category C operation followed by a normal operation (category A), the normal operation can sometimes be moved to occur before the category C operation. This also has the effect of “expanding” the critical section.</li>
<li>The key is that such reordering must not allow another thread to observe a state that contradicts the intended synchronization order.</li>
</ul></li>
</ul>
<p>
The Java memory model dictates that (as "rules"):
</p>
<ul class="org-ul">
<li><b>Within category A</b>
<ul class="org-ul">
<li><i>Allowed:</i> Normal loads and stores can be reordered if they are not dependent (e.g., a load of X followed by a store to Y can be swapped if no dependency exists).</li>
<li><i>Not Allowed:</i> If there is a data dependency (for instance, reading a variable and then immediately writing a computed value based on it), reordering is disallowed because the outcome could change.</li>
</ul></li>
<li><b>Between category A and B</b>
<ul class="org-ul">
<li><i>Allowed:</i> A normal (category A) operation that precedes a volatile load or an enter monitor (category B) can be swapped so that the category B operation comes first.</li>
<li><i>Effect:</i> This moves the normal operation into the critical section. For example, moving a load after acquiring the lock might be beneficial because it ensures the value read is synchronized with the critical section’s state.</li>
</ul></li>
<li><b>Between category A and C</b>
<ul class="org-ul">
<li><i>Allowed:</i> Similarly, a normal operation (category A) that follows a volatile store or an exit monitor (category C) can be moved to occur before the volatile store or exit monitor.</li>
<li><i>Effect:</i> This expands the critical section in the other direction. In effect, the update to a normal variable is “protected” by the lock even though it appears after the explicit synchronization in the source code.</li>
</ul></li>
<li><b>Between categories B and C (or within B or within C)</b>
<ul class="org-ul">
<li><i>Careful! Not generally allowed:</i> Operations that are already marked as volatile loads/stores or that involve entering/exiting monitors have stricter ordering because they are the points where the happens-before relationships are established.</li>
<li><i>Result:</i> They effectively serve as memory barriers. For instance, a volatile store cannot be delayed or swapped with operations that follow it if that would allow another thread to see stale or inconsistent data.</li>
</ul></li>
</ul>
<p>
Even though the JMM allows aggressive reordering within category A for efficiency, it imposes strict limits at the boundaries (Categories B and C) to ensure that once a thread enters a critical section (or reads a volatile value), it sees a consistent state.
</p>

<p>
We also have a notion of "expanding critical sections". A compiler might move normal loads into a synchronized block (or move stores out of it) if it can do so without affecting the program’s behavior as observed by other threads. This “expansion” of the critical section is allowed by the reordering rules.
</p>

<p>
Importantly, all these reordering permissions <b>are subject to the as-if rule</b>: the observable behavior of the program in a single-threaded context must remain the same. In a multithreaded context, however, the ordering provided by volatile and synchronized constructs (the barriers) is preserved.
</p>

<p>
<b>In short</b>:
</p>

<p>
There are some notable exceptions to this rule. One of which being <code>volatile</code>: every access to a voltile variable has to be done at a machine level
in the same order that it is in your program. Compiler writers like making everything volatile, but programmers hate it because it makes implementation harder/unreliable.
</p>

<p>
How can we do spin-locking reliably at the hardware level? A re-order table. Which has the following categories of operations:
</p>
<ul class="org-ul">
<li>A: normal load + store (most common)
<ul class="org-ul">
<li>Reading into and storing into non-volatile variables</li>
</ul></li>
<li>B: volatile load + enter monitor (entering the critical section)
<ul class="org-ul">
<li>Reading from a volatile variable, monitor means grab a lock</li>
</ul></li>
<li>C: volatile store + exit monitor (exiting the critical section)
<ul class="org-ul">
<li>Writing into a volatile variable, letting going of a lock</li>
</ul></li>
</ul>
<p>
This gives the folloiwng valid optimizations:
</p>
<ul class="org-ul">
<li>Reordering multiple elements of cateogry A</li>
<li>Reordering type A (first op) and B (second op) in any order (it is okay to grow the critical section)</li>
<li>Type C (first op) and type A (second op) in any order.</li>
</ul>
</div>
</div>

<div id="outline-container-org976f5fa" class="outline-4">
<h4 id="org976f5fa"><span class="section-number-4">1.6.12.</span> Java Project</h4>
<div class="outline-text-4" id="text-1-6-12">
<p>
This is just the Java project code
</p>
<div class="org-src-container">
<pre class="src src-java">// State.java
interface State {
    int size();
    long[] current();
    void swap(int i, int j);
}
// NullState.java
// This is a dummy implementation, useful for
// deducing the overhead of the testing framework.
class NullState implements State {
    private long[] value;
    NullState(int length) { value = new long[length]; }
    public int size() { return value.length; }
    public long[] current() { return value; }
    public void swap(int i, int j) { }
}
// SwapTest.java
import java.util.concurrent.ThreadLocalRandom;

class SwapTest implements Runnable {
    private long nTransitions;
    private State state;

    SwapTest(long n, State s) {
        nTransitions = n;
        state = s;
    }

    public void run() {
        var n = state.size();
        if (n &lt;= 1)
            return;
        var rng = ThreadLocalRandom.current();
        var id = Thread.currentThread().threadId();

        for (var i = nTransitions; 0 &lt; i; i--)
            state.swap(rng.nextInt(0, n), rng.nextInt(0, n));
    }
}
// Synchronized state
class SynchronizedState implements State {
    private long[] value;

    SynchronizedState(int length) { value = new long[length]; }

    public int size() { return value.length; }

    public long[] current() { return value; }

    public synchronized void swap(int i, int j) {
        value[i]--;
        value[j]++;
    }
}
// Unsynchronized state
class UnsynchronizedState implements State {
    private long[] value;

    UnsynchronizedState(int length) {
        value = new long[length];
    }

    public int size() {
        return value.length;
    }

    public long[] current() {
        return value;
    }

    public void swap(int i, int j) {
        value[i]--;
        value[j]++;
    }
}
// UnsafeMemory.java
class UnsafeMemory {
    public static void main(String args[]) {
        if (args.length != 5)
            usage(null);
        try {
            boolean virtual;
            if (args[0].equals("Platform"))
                virtual = false;
            else if (args[0].equals("Virtual"))
                virtual = true;
            else
                throw new Exception(args[0]);

            var nValues = (int) argInt(args[1], 0, Integer.MAX_VALUE);

            State s;
            if (args[2].equals("Null"))
                s = new NullState(nValues);
            else if (args[2].equals("Synchronized"))
                s = new SynchronizedState(nValues);
            else if (args[2].equals("Unsynchronized"))
                s = new UnsynchronizedState(nValues);
            else if (args[2].equals("AcmeSafe"))
                s = new AcmeSafeState(nValues);
            else
                throw new Exception(args[2]);

            var nThreads = (int) argInt(args[3], 1, Integer.MAX_VALUE);
            var nTransitions = argInt(args[4], 0, Long.MAX_VALUE);

            dowork(virtual, nThreads, nTransitions, s);
            test(s.current());
            System.exit(0);
        } catch (Exception e) {
            usage(e);
        }
    }

    private static void usage(Exception e) {
        if (e != null)
            System.err.println(e);
        System.err.println("Arguments: [Platform|Virtual] nvalues model"
                           + " nthreads ntransitions\n");
        System.exit(1);
    }

    private static long argInt(String s, long min, long max) {
        var n = Long.parseLong(s);
        if (min &lt;= n &amp;&amp; n &lt;= max)
            return n;
        throw new NumberFormatException(s);
    }

    private static void dowork(boolean virtual, int nThreads,
                               long nTransitions, State s)
      throws InterruptedException {
        var builder = virtual ? Thread.ofVirtual() : Thread.ofPlatform();
        var test = new SwapTest[nThreads];
        var t = new Thread[nThreads];
        for (var i = 0; i &lt; nThreads; i++) {
            var threadTransitions =
                (nTransitions / nThreads
                 + (i &lt; nTransitions % nThreads ? 1 : 0));
            test[i] = new SwapTest(threadTransitions, s);
            t[i] = builder.unstarted(test[i]);
        }
        var realtimeStart = System.nanoTime();
        for (var i = 0; i &lt; nThreads; i++)
            t[i].start();
        for (var i = 0; i &lt; nThreads; i++)
            t[i].join();
        var realtimeEnd = System.nanoTime();
        long realtime = realtimeEnd - realtimeStart;
        double dTransitions = nTransitions;
        System.out.format("Total real time %g s\n",
                          realtime / 1e9);
        System.out.format("Average real swap time %g ns\n",
                          realtime / dTransitions * nThreads);
    }

    private static void test(long[] output) {
        long osum = 0;
        for (var i = 0; i &lt; output.length; i++)
            osum += output[i];
        if (osum != 0)
            error("output sum mismatch", osum, 0);
    }

    private static void error(String s, long i, long j) {
        System.err.format("%s (%d != %d)\n", s, i, j);
        System.exit(1);
    }
}
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org0fe2bbb" class="outline-3">
<h3 id="org0fe2bbb"><span class="section-number-3">1.7.</span> Logic Programming and <code>prolog</code></h3>
<div class="outline-text-3" id="text-1-7">
<p>
<b>At a glance</b>, (TODO)
</p>

<p>
In a <b>logic language</b>, the basic entity is a predicate (like a function which returns true or false. But don't think of it as a function because that is in functional terms).
</p>

<p>
We glue together predicates using <code>&amp;</code> (AND), <code>;</code> (OR), <code>\+</code> (negation as failure, discussed more later). We give up functiuons and side effects. Think declaritvely, you specify what answers you want.
</p>

<p>
We want to split an algorithm into two parts: logic (what you want; spec; correctness) and control (efficiency; implementation).
</p>

<p>
Rules have conditions. Facts don't have conditions and are always true.
</p>
</div>

<div id="outline-container-org9083e76" class="outline-4">
<h4 id="org9083e76"><span class="section-number-4">1.7.1.</span> Quicksort in <code>prolog</code></h4>
<div class="outline-text-4" id="text-1-7-1">
<p>
In <code>prolog</code>, you don't "say" what type of sort (quicksort, heapsort, etc) you're doing. You just indicate to <code>prolog</code> that "I have a sorting problem. You figure it out." This is considered <i>thinking declaratively</i>.
</p>

<p>
Since we have two predicates, we give two arguments to sort:
</p>
<ul class="org-ul">
<li><code>L</code> is the list to be sorted</li>
<li><code>S</code> is the sorted version of <code>L</code></li>
</ul>
<p>
we need to specify what sorting means. <code>S</code> has to have the same elements in <code>L</code>, if <code>L</code> has duplicates, <code>S</code> has to have the same duplicates (mathematically: <code>S</code> is a permutation of <code>L</code>). We can write
</p>
<div class="org-src-container">
<pre class="src src-prolog">sort(L, S) :- perm(L, S), sorted(S)
</pre>
</div>
<p>
we can think of <code>:-</code> as "if" and <code>,</code> as "and". In English, this reads as "if <code>S</code> is a permutation of <code>L</code> and <code>S</code> is sorted, then <code>S</code> is a sorted version of <code>L</code>.
We can also write some "for all <code>L</code> and for all <code>S</code>" but these are implicit to all predicates.
</p>

<p>
Capital letters are logical variables. The <b>scope</b> of a logical variable is just the clause in which it is defined. No nested scope, etc. No need to declare, we are implicitly declaring a variable by writing a capital letter.
</p>

<p>
At this point, we still have two more predicates that we have yet to define (<code>perm</code>, <code>sorted</code>).
</p>

<p>
First we define <code>sorted</code>. We can look at <code>sorted</code> as the following:
</p>
<ul class="org-ul">
<li><b>Fact:</b> the singleton list sorted: <code>sorted([_])</code></li>
<li><b>Fact:</b> empty list is sorted: <code>sorted([])</code></li>
<li><b>Rule</b>: in a sorted list, X &lt;= Y for X before Y: <code>sorted([X, Y]) :- X =&lt; Y</code></li>
<li><b>Rule</b>: if X is less than or equal to Y and Y is less than the head Z then our list is sorted (similar to <code>Y::Z</code> in OCaml): <code>sorted([X, Y|Z]) :- X =&lt; Y, sorted([Y|Z])</code></li>
</ul>

<p>
Next we define <code>perm</code>. We have the following:
</p>
<ul class="org-ul">
<li><code>perm([], [])</code></li>
<li><code>perm([X], [X])</code></li>
<li><code>perm([X, Y], [X, Y])</code></li>
<li><code>perm([X, Y], [Y, X])</code></li>
</ul>
<p>
but we can write a simpler recursive rule to capture all the above base cases (instead of manually enumerating&#x2026;)
</p>
<div class="org-src-container">
<pre class="src src-prolog">perm([X|L], R) :- perm(L, PL), append(P1, P2, PL), append(P1, [X|P2], R)
</pre>
</div>
<p>
<b>Rule</b>: if <code>PL</code> is a permutation of <code>L</code> and appending <code>P1</code> to <code>P2</code> gives you <code>PL</code> and appending <code>P1</code> to <code>[X|L]</code> is equal to <code>R</code>
</p>
<div class="org-src-container">
<pre class="src src-prolog">append([], L, L)
</pre>
</div>
<p>
<b>Fact</b>: appending the empty list to any list <code>L</code> will give you <code>L</code>
</p>
<div class="org-src-container">
<pre class="src src-prolog">append([X|L], M, [X|LM]) :- append(L, M, LM)
</pre>
</div>
<p>
one list is a permutation of another if we can rearrange the suffix of the first list <code>L</code> into two parts <code>PL1</code> and <code>PL2</code>, and <code>PL1, X, PL2</code> appended to each other gets us the resulting list.
</p>

<p>
<b>By the way, this implementation is \(O(n!)\)</b>. The logic is optimized for DFS, and also depends how you write the logic. Who the $*@&amp; is using this?
</p>

<p>
If we wanted to write a faster <code>sort</code>, we could do
</p>
<div class="org-src-container">
<pre class="src src-prolog">% Base case: an empty list is already sorted.
fast_sort([], []).

% Recursive case: sort the tail, then insert the head into the sorted tail.
fast_sort([H|T], Sorted) :-
    fast_sort(T, SortedTail),
    insert(H, SortedTail, Sorted).

% insert(+Elem, +SortedList, -NewSortedList)
% Inserts Elem into the correct position in SortedList.
insert(X, [], [X]).
insert(X, [Y|T], [X,Y|T]) :- 
    X =&lt; Y, !.
insert(X, [Y|T], [Y|Rest]) :-
    insert(X, T, Rest).
</pre>
</div>
</div>
</div>
<div id="outline-container-orgca446e0" class="outline-4">
<h4 id="orgca446e0"><span class="section-number-4">1.7.2.</span> <code>prolog</code> syntax</h4>
<div class="outline-text-4" id="text-1-7-2">
<p>
Prolog is built out of terms, which are one of the following:
</p>
<pre class="example">
atom := [a-z][a-zA-Z0-9_]*
variable := [A-Z_][a-zA-Z0-9_]*
structure := atom(T1, ... Tn) where n &gt; 0 (arity)
</pre>
<p>
Atoms are individual values that only have the properties that you assign to then, They are simply names, atoms can never be equal to numbers. You can write atoms in quotes and not have naming restrictions.
A logical variable will stand for any value that you want it to stand for, except every occurrence of the same logical variable in a clause has to be the same thing. Logical variables are bound to terms on success, and unbound on failure (will happen to all occurrences of the logical variable).
A structure <code>f(T1,...,Tn)</code>, for \(n > 0\) and \(f\) an atom, <code>T1-Tn</code> are terms: a simple data structure in memory (just a vector of atom then arguments).
</p>
</div>
</div>

<div id="outline-container-orgbd2af20" class="outline-4">
<h4 id="orgbd2af20"><span class="section-number-4">1.7.3.</span> Some <code>prolog</code> syntactic sugar</h4>
<div class="outline-text-4" id="text-1-7-3">
<p>
Syntactic sugar means that while Prolog allows you to write things in a very natural, human‐friendly way, these forms are formally equivalent to more “raw” or explicit term representations. You could write them out in full if you wanted to; the meaning wouldn’t change.
</p>

<p>
<b>Infix operators</b>: Prolog supports built-in operators (like <code>+</code>, <code>-</code>,===, <code>:-</code>, etc.) with precedences similar to those in languages like C++ or Java. For example, when you write <code>X = 2 + 2.</code>, Prolog interpretes this as a term that is equivalent to <code>=(X, +(2, 2)).</code>.
</p>
<ul class="org-ul">
<li>In this form the operator <code>=</code> is just a functor that tries to make its two arguments equal (via unification), and <code>+</code> is a functor that constructs a term representing "2 plus 2".</li>
<li>Internally, when you call the equals predicate (<code>=</code>), Prolog attempts to unify its two arguments. In our example, it doesn’t actually perform arithmetic addition; it simply constructs the term <code>+(2,2)</code> and binds <code>X</code> to that structure. Later, when Prolog prints <code>X</code>, it pretty-prints it as <code>2+2</code>, which is a neat representation of the underlying data structure.</li>
<li>Prolog does not automatically evaluate arithmetic expressions. If you want to perform arithmetic, you must explicitly ask it to evaluate (for example, using the <code>is/2</code> predicate). Thus, <code>X</code> is <code>2 + 2</code> would compute the sum and bind <code>X</code> to 4, while <code>X = 2 + 2</code> just constructs the term.</li>
</ul>
<p>
<b>Data structures and terms</b>
</p>
<ul class="org-ul">
<li>Every Prolog expression is a term. Operators and infix notations are just ways to build these terms. For instance, the expression <code>2+2</code> is internally the compound term <code>+(2,2)</code>, which is exactly what you’d write if you were manually constructing the term.</li>
</ul>
<p>
<b>List notation</b>
</p>
<ul class="org-ul">
<li>The square bracket notation <code>[a, b, c]</code> is shorthand for explicit construction using the <code>.</code> operator. Internally, this list is represented as <code>.(a, .(b, .(c, []))).</code></li>
</ul>
<p>
<b>Rules as terms</b>
</p>
<ul class="org-ul">
<li>When you write a rule in Prolog, such as <code>head :- body.</code>, this is not some separate language construct but is itself a term. The binary operator <code>:-</code> (which has low precedence) connects the head of the rule with its body. In other words, a rule is just a compound term that represents both a fact and a rule simultaneously.</li>
</ul>
</div>
</div>

<div id="outline-container-org22f6759" class="outline-4">
<h4 id="org22f6759"><span class="section-number-4">1.7.4.</span> Review: Clauses</h4>
<div class="outline-text-4" id="text-1-7-4">
<p>
<b>Clauses:</b> logic statements about the world that are universally quantified. For example: \(\forall x \forall y \forall z (BC(z, F(y, x)) \rightarrow(x, F(y, x)))\). In English, this reads "For every x, every y and every z, if BC holds for z and F applied to y and x, then AB holds for x and F applied to y and x."
In <code>prolog</code> we would express this as <code>ab(X, Y) :- bc(Z, F(Y, X)).</code>, which means "For all X and Y, AB(X, Y) is true if there exists some Z such that BC(Z, F(Y, X)) is true."
</p>

<p>
Sometimes, we might write down two clauses that are very similar, but one is "more specific" (has extra conditions)
than the other. The more general caluse applies in a wider range of situations because it has fewer constraints.
</p>

<p>
<b>Substitution test:</b> to decide if one clause is more general than another, you check whether you can take the more general clause and,
by consistently subtituting its variables with specific terms, get the more specific clause.
</p>
<ul class="org-ul">
<li>If yes, then the general clause covers all cases of the specific clause (and maybe more)</li>
<li>If no, then the specific clause cannot be derived from the general one via substitution</li>
</ul>
<p>
For example:
</p>
<ul class="org-ul">
<li><code>sibling(X, Y) :- parent(P, X), parent(P, Y), X \ = Y.</code> says "X and Y are siblings if they share a parent P and are not the same person." (general clause)</li>
<li><code>sibling(john, mary) :- parent(P, john), parent(P, mary), john \ = mary.</code>, if we substitute <code>X = john</code> and <code>Y = mary</code> in the general clause, you obtain the specific clause.</li>
</ul>
<p>
substitution is a set of assignments to logical variables. <code>prolog</code> is based on the idea of coming up with substitutions. If you ask <code>prolog</code> a question,
we get some series of substitutions.
</p>
<ul class="org-ul">
<li>In a substitution, the LHS must be a single variable. e.g. <code>{F(y) = B}</code> is not valid.</li>
</ul>
</div>
</div>

<div id="outline-container-org70c5508" class="outline-4">
<h4 id="org70c5508"><span class="section-number-4">1.7.5.</span> Three Kinds of Clauses</h4>
<div class="outline-text-4" id="text-1-7-5">
<ol class="org-ol">
<li><b>Facts:</b> statements that are unconditionally true. Like raw data or assertions. (<code>prereq(cs31, cs131)</code>)
<ul class="org-ul">
<li>Facts can also include variables, making them more general. We also have a "don't care" variable: <code>prereq(intro101, _)</code></li>
<li><b>Ground term:</b> a fact that does not involve any logical (unbound variables).e.g. <code>prereq(cs31, cs131)</code></li>
</ul></li>
<li><b>Rules:</b> rules are clauses that have conditions - written with a <code>:-</code> (reads as "if") and define relationships beased on other clauses
<ul class="org-ul">
<li><b>Transitive closure:</b> we can combine facts and rules to define more complex predicates. e.g.
<ul class="org-ul">
<li><code>prt(A, B) :- prereq(A, B)</code> as our base case</li>
<li><code>prt(A, Z) :- prereq(A, B), prt(B, Z)</code> as our recursive case</li>
</ul></li>
</ul></li>
<li><b>Queries:</b> questions you ask <code>prolog</code> to find out what substitutions (answers) make the statement true.
<ul class="org-ul">
<li>When you pose a query, <code>prolog</code> uses a depth first search to look for proofs based on the facts and rules.</li>
<li>You can view a query in a logical sense as saying, "it is not the case that there is no solution." Formally, a query
is equivalent to asserting that if there were no valid subsitutions making the query true, then then false would follow. Prolog works by trying to disprove this "negative" until it finds a counter example (a valid solution). e.g. it using a form of <b>proof by contradiction</b>.
<ul class="org-ul">
<li>You can type <code>;</code> to keep getting more "counterexamples".</li>
</ul></li>
<li>Prolog scans its list of facts and rules from the top, and picks the first clauses that matches. Use the match to set up further subgoals.</li>
</ul></li>
</ol>
<p>
<b>Subgoals and proof tree</b>
</p>
<ul class="org-ul">
<li>When a clause has multiple conditions (subgoals) connected by an "and" (<code>,</code>), Prolog tries to satisfy them one after another from left to right
<ul class="org-ul">
<li>If the first subgoal fails, Prolog will not even attempt the later ones in that branch</li>
</ul></li>
<li>For a proof tree, we have that
<ul class="org-ul">
<li>For an <b>and</b> node, all the children (subgoals) must be proven for the node to succeed.</li>
<li>For an <b>or</b> node, only one branch (one way of proving the goal) needs to succeed. Prolog backtracks if needed.</li>
</ul></li>
<li>Prolog explores the proof space in a DFS manner:
<ul class="org-ul">
<li>Dives down one branch (resolving subgoals in l-r order) before trying alternative branches</li>
<li>If a branch fails at any point, it backtracks to the last decision point (an OR node) to try a different alternative</li>
</ul></li>
</ul>
<p>
<b>Backtracking</b>
Take for example the following:
</p>
<div class="org-src-container">
<pre class="src src-prolog">% A fact: X is a member if it is the head of the list.
member(X, [X|_]).

% A rule: X is a member if it is in the tail of the list.
member(X, [_|Tail]) :- member(X, Tail).
</pre>
</div>
<ul class="org-ul">
<li>You can query a specific element: <code>?- member(a, [a, b, c]).</code>  which matches the fact <code>X = a</code> and returns true immediately.</li>
<li>You can query for all possible elements: <code>?- member(X, [a, b, c]).</code> which first matches <code>X = a</code> from the fact. On a semicolon, it backtracks and would next find <code>X = b</code>.</li>
<li>You can do a proof by contradiction in queries i.e. a query that logically asserts "there is no solution" by negating the possibility of any valid substitution. For example,
<ul class="org-ul">
<li><code>?- \+ member(q, [a,b,c]).</code>, this query asks "is it false that <code>q</code> is a member of <code>[a,b,c]</code>?". Prolog attempts to prove this. Since no substitution mkakes this true, the negation succeeds.</li>
</ul></li>
</ul>
<p>
Prolog does not compute all answers before returning the first one. Instead, it returns the first answer it finds immediately, and then backtracks to the next valid answer on <code>;</code>. Technically, this means that if the logic allows for an infinite number of answers, Prolog will keep generating answers as long as you keep request more semicolons.
</p>

<p>
<b>Simple vs. complex substitutions</b>
</p>
<ul class="org-ul">
<li>(Simple substitutions) When prolog finds a straightforward match (binding a variable to a ground term), it often doesn't bother displaying the substitution explicitly because it's trivial, i.e., <code>?- member(a, [a,b,c]).</code></li>
<li>(Complex substitutions) When the match involves more elaborate structures (such as lists built with "cons"), prolog must print out these substitutions to show how the variables are instantiated. i.e. when the result for query is a compound data structure, prolog displays the entire structure.</li>
<li>When you ask a query that logically asserts "member is always false", <code>prolog</code> will generate substitions showing a list structure that contradicts your assumption.
<ul class="org-ul">
<li>e.g. <code>?- member(Q, R).</code>, here <code>prolog</code> might first bind <code>Q</code> to a variable, and then construct a list where <code>Q</code> is the head. This gives you a concrete counterexample.</li>
</ul></li>
</ul>
<p>
<b>Finite vs. infinite answers</b>
</p>
<ul class="org-ul">
<li>In some cases, there are infinitely many ways to construct a list that satisfies the query.
<ul class="org-ul">
<li>e.g. <code>?- member(Q, R).</code>, <code>prolog</code> can generate an infinite number of answers because R can be any list containing Q in various positions. Practically, you will run out of memory since we will backtrack indefinitely.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org143288d" class="outline-4">
<h4 id="org143288d"><span class="section-number-4">1.7.6.</span> <code>append</code></h4>
<div class="outline-text-4" id="text-1-7-6">
<p>
The predicate <code>append/3</code> is commonly defined with two clauses.
</p>
<ol class="org-ol">
<li>Base case: <code>append([], L, L).</code></li>
<li>Recursive case: <code>append([X|L1], L2, [X|L3]) :- append(L1, L2, L3).</code></li>
</ol>
<p>
For example, on querying <code>?- append([a], [b,c], R).</code> prolog will
</p>
<ol class="org-ol">
<li>Match the head of the first list (<code>a</code>)</li>
<li>Create a fresh copy of the variable for the tail</li>
<li>Solve the subgoal <code>append(L1, [b, c], L3)</code>, eventually matching the base case</li>
<li>Return <code>R = [a, b, c]</code></li>
</ol>
<p>
<b>Scope</b>
</p>
<ul class="org-ul">
<li>Every time a clause is used, <code>porlog</code> treats the variable in that clause as locally scoped.
<ul class="org-ul">
<li>In the recursive call, the variables get "renamed", ensures that matching in one clause does not interfere with another.</li>
</ul></li>
</ul>
<p>
<b>Reversing</b> <code>append</code>
You can also use append to split a list into two sublists, e.g. <code>?- append(R, S, [a,b,c]).</code>. However, there are possibly infinite answers.
</p>
</div>
</div>

<div id="outline-container-org21a6ce5" class="outline-4">
<h4 id="org21a6ce5"><span class="section-number-4">1.7.7.</span> <code>reverse</code></h4>
<div class="outline-text-4" id="text-1-7-7">
<p>
We want to define a predicate such that <code>reverse(List, ReversedList)</code> is true if the second argument is the first list in reverse order.
</p>

<p>
As a base case, the simplest fact is that the empty list is the reverse of itself: <code>reverse([], []).</code>
</p>

<p>
One idea is to handel non-empty lists by saying that if a list starts with a head <code>X</code> and a tail <code>L</code>, then its reverse is the reverse of <code>L</code> with <code>X</code> appended to the end:
<code>reverse([X|L], R) :- reverse(L, RevL), append(RevL, [X], R).</code>. The predicate recursively computes <code>reverse(L, RevL)</code>, and it then appends <code>[X]</code> to the end of <code>RevL</code> by using the <code>append/3</code> predicate. However, this is inefficient, as each recursive call uses <code>append/3</code>, which itself takes time proportional to the length of the first argument, which is \(O(N^2)\).
</p>

<p>
<b>Improving performance</b>
</p>

<p>
In order to improve performance, we can avoid repeated appending by defining a helper predicate (<code>rev/3</code>) that carries an extra argument (accumulator) to build up the reversed list as we traverse the original list.
</p>

<p>
The helper predicate <code>rev(L, A, R)</code> is intended to be true when reversing list <code>L</code> and then appending accumulator <code>A</code> produces <code>R</code>. Then,
</p>
<div class="org-src-container">
<pre class="src src-prolog">rev([], Acc, Acc).
rev([X|L], Acc, R) :- rev(L, [X|Acc], R).
reverse(L, R) :- rev(L, [], R).
</pre>
</div>
<p>
this gives us an \(O(N)\) algorithm. 
</p>

<p>
<code>reverse</code> is often used as a benchmark in prolog systems to measure how many logical inferences (sometimes called "lips") per second can be made.
</p>
</div>
</div>

<div id="outline-container-org10d6106" class="outline-4">
<h4 id="org10d6106"><span class="section-number-4">1.7.8.</span> Simple predicates</h4>
<div class="outline-text-4" id="text-1-7-8">
<p>
The simplest predicates are
</p>
<ul class="org-ul">
<li><code>fail/0</code>, which has no clause defined for <code>fail.</code>. Querying <code>?- fail.</code> immediately returns <code>no</code>.</li>
<li><code>true/0</code>, which is defined as a single fact: <code>true</code>. Querying immediately returns <code>yes</code>.</li>
</ul>
<p>
Infinite loops:
</p>
<ul class="org-ul">
<li><code>loop/0</code> which is simply <code>loop :- loop</code>, which basically has prolog infinitely setting up a subgoal <code>loop</code> to prove, which sets up a subgoal <code>loop</code>&#x2026; etc.</li>
<li><code>repeat/0</code> is a slightly more comlicated. We have
<ul class="org-ul">
<li><code>repeat.</code></li>
<li><code>repeat :- repeat.</code></li>
<li>When we query this, it will prove <code>repeat</code> immediately via the fact. But typing a semicolon makes prolog backtrack and repreat infinitely.</li>
<li>We can use <code>repeat</code> to print and debug: <code>?- repeat, write('Ouch!'), fail.</code> will create an infinite loop of printing.</li>
</ul></li>
</ul>
<p>
We can also write predicates poorly. Take for example <code>repeats :- repeats, repeat.</code>
</p>
<ul class="org-ul">
<li>The rule is self-referential without any grounding fact, which leads to an infinite loop.</li>
<li>Always ensure that recursive predicates have a well-defined base case or are written in a way that allows the search to eventually terminate.</li>
</ul>
</div>
</div>

<div id="outline-container-orge7c9720" class="outline-4">
<h4 id="orge7c9720"><span class="section-number-4">1.7.9.</span> What can go wrong in <code>prolog</code>?</h4>
<div class="outline-text-4" id="text-1-7-9">
<ul class="org-ul">
<li>In <code>prolog</code>, you may write "wrong code" in the sense that the logic you express does not match your intent.
<ul class="org-ul">
<li>For example, a predicate intended to decide a relationship may have its rules ordered improperly or be missing a necessary base case.</li>
</ul></li>
</ul>
<p>
<b>Debugging in prolog</b>
</p>

<p>
Prolog provides a few built-in facilities for debugging:
</p>
<ul class="org-ul">
<li>Use <code>trace/0</code>: invoking <code>trace</code> puts "hooks" into the interpreter</li>
<li>Uses the 4-port debugging model: every goal (or subgoal) in a proof has four "ports":
<ol class="org-ol">
<li>Call: when the goal is first invoked</li>
<li>Exit: when the goal succeeds</li>
<li>Fail: when the goal fails</li>
<li>Redo: when the goal is re-invoked (backtracking) to seek another solution</li>
</ol></li>
</ul>
<p>
Take for example the following model:
</p>
<pre class="example">
  [Call Q]
     │
(Goal processing)
     │
 ┌─────────┐
 │  Q Succeeds  ├─► [Exit Q]
 └─────────┘
     │
(If further solutions are requested)
     │
 ┌─────────┐
 │  Q Redo  ├─► [Redo Q] (then eventually Exit again, or)
 └─────────┘
     │
(If no alternatives remain)
     │
 [Fail Q]

</pre>
<ul class="org-ul">
<li>When you execute a query, Prolog will show (if tracing is turned on) each time it enters or leaves a goal.</li>
<li>You can also "instrument" your code with print statements, e.g. you can write a predicate that, for every success of a goal, prints the bound values and then calls <code>fail</code> to force backtracking. This is often more efficient thatn using the built-in debugger.</li>
</ul>
<p>
This complicated model exists because in <code>prolog</code>, things can succeed more than once (vs. just returning in other languages). For built-in predicates and their pitfalls, reference the "Simple predicates" section
</p>
</div>
</div>

<div id="outline-container-org77d9897" class="outline-4">
<h4 id="org77d9897"><span class="section-number-4">1.7.10.</span> Unification, two-way matching, and infinite (cyclic) terms</h4>
<div class="outline-text-4" id="text-1-7-10">
<p>
<b>Unification</b> is the process of making two terms identical by finding a substitution (i.e. binding variables to values), that, when applied, makes the terms the same. In <code>prolog</code>, unification is used
to match a goal against the head of a clause. Every time you execute a gaol, <code>prolog</code> unifies the goal with a clause head to see if the clause applies.
</p>

<p>
In OCaml or other functional languages, we have one-way pattern matching: we have some fixed data structure and we match it against patterns that may include variables. The data is "given" and cannot contain variables. In Prolog, we have two-way pattern matching:
</p>
<ul class="org-ul">
<li>Both the goal and the clause head may contain logical variables</li>
<li>Variables can be instantiated during unification in either direction</li>
<li><b>Advantage</b>: more powerful; allows for bidirectional data flow</li>
<li><b>Disadvantage</b>: can lead to unintended consequences</li>
</ul>

<p>
Take the following examples:
</p>
<ul class="org-ul">
<li>Basic two-way matching: <code>?- X = f(Y).</code>.
<ul class="org-ul">
<li>Here the left side is the variable <code>X</code> and the right side is the compount term <code>f(Y)</code>, where <code>Y</code> is unbound.</li>
<li>Unification succeeds by binding <code>X</code> to <code>f(Y)</code>.</li>
<li>Even though <code>Y</code> remains unbound, the binding is two-way: Prolog now "knows" that <code>X</code> and <code>f(Y)</code> are the same term.</li>
<li>Unification succeeds, result: <code>X = f(Y).</code></li>
</ul></li>
<li>Matching two compound terms: <code>?- f(X, a) = f(b, Y).</code>.
<ul class="org-ul">
<li>Both terms have the same functor <code>f/2</code>.</li>
<li>Prolog compares the corresponding arguments:
<ul class="org-ul">
<li>First arguments: <code>X</code> and <code>b</code> -&gt; binds <code>X = b</code>.</li>
<li>Second arguments: <code>a</code> and <code>Y</code> -&gt; binds <code>Y = a</code>.</li>
</ul></li>
<li>Unification succeeds, result: <code>X = b, Y = a.</code>.</li>
</ul></li>
<li>More complex unification: <code>?- f(g(X), Y) = f(Z, g(a)).</code>
<ul class="org-ul">
<li>Both terms start with the functor <code>f/2</code>, so Prolog attempts to unify the arguments:
<ul class="org-ul">
<li>For the first argument: <code>g(X)</code> must equal <code>Z</code>. So, <code>Z</code> is bound to <code>g(X)</code>.</li>
<li>For the second argument: <code>Y</code> must equal <code>g(a)</code>. So, <code>Y</code> is bound to <code>g(a)</code>.</li>
</ul></li>
<li>Unification succeeds: <code>Z = g(X), Y = g(a).</code>.</li>
</ul></li>
</ul>

<p>
Now we consider <b>cyclic terms</b> and discuss the "Occurs" check. Consider the fact <code>p(X, X).</code>.
</p>
<ul class="org-ul">
<li>Querying <code>?- p(a, Y).</code> successfuly unifies <code>X</code> with <code>a</code> and <code>Y</code> with <code>a</code>.</li>
<li>However, trying a more complicated unification like <code>?- p(Z, f(Z)).</code> will have <code>prolog</code> attempting to set <code>Z = f(Z)</code>.</li>
</ul>
<p>
In the second (bad) case, internally, <code>prolog</code> binds <code>Z</code> to therm <code>f(Z)</code>, creating a cycle (term that refers to itself). This is cheap to do (just setting a pointer), but when you try to print or process
this structure, you may run into an infinite loop. These are <b>cyclic</b> terms that can lead to unintended infinite loops.
</p>

<p>
How can we fix this? For efficiency, standard unification in <code>prolog</code> does not perform the <b>"occurs check"</b> (i.e. it does not verify that a variable does not occur within the term it is being bound to). 
</p>
<ul class="org-ul">
<li>We can use <code>unify_with_occurs_check/2</code>: a built-in predicate that performs unification but fails if the unification would create an infinite cyclic term.
<ul class="org-ul">
<li>We can use it as <code>?- unify_with_occurs_check(Z, f(Z)). false.</code>. However, the <b>tradeoff</b> is that it is slower since it must traverse the entire datastructure in order to check for cycles.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org7af13fe" class="outline-4">
<h4 id="org7af13fe"><span class="section-number-4">1.7.11.</span> Piano Arithmetic</h4>
<div class="outline-text-4" id="text-1-7-11">
<p>
The representation of numbers in <code>prolog</code> is interesting.
</p>
<ul class="org-ul">
<li><b>Atoms and successor function</b>: some <code>prolog</code> systems may not have built-in arithmetic. So, we represent numbers using <code>Z</code> to represent 0 and the functor <code>s/1</code> to represent the successor (i.e. plus one). So the number 2 is represented as <code>(s(s(Z)).</code></li>
<li><b>Defining addition</b>:
<ul class="org-ul">
<li>Base case: adding 0 to any number yields that number <code>plus(Z, X, X).</code></li>
<li>Recursive case: adding the successor of <code>X</code> to <code>Y</code> results in the successor of (<code>X</code> plus <code>Y</code>). Similar to list append logic <code>plus(s(X), Y, s(Z)) :- plus(X, Y, Z).</code></li>
<li>To compute <code>2 + 2</code>, we would have <code>?- plus(s(s(Z)), s(s(Z)), R).</code>.</li>
</ul></li>
<li><b>Defining subtraction and comparison</b>:
<ul class="org-ul">
<li>We can define substraction similarly by "inverting" addition.</li>
<li>First we define a less-than predicate, with the rule that 0 is less than any non-zero number: <code>lt(Z, s(_)).</code>.</li>
<li>The recursive rule: a number is less than the successor of another number if it is less than that number. <code>lt(s(X), s(Y)) :- lt(X, Y).</code></li>
<li>Pitfall: unification might yield cyclic terms (i.e. trying to deduce a number less than itself may force a term like <code>Z = s(Z)</code>.</li>
</ul></li>
</ul>
<p>
To avoid cyclic terms in arithmetic, we could use the <code>unify_with_occurs_check/2</code>, at the tradeoff of efficiency.
</p>
<pre class="example">
lt(X, Y) :-
    unify_with_occurs_check(X, Y),  % Ensures X is not "inside" Y
    % ... further conditions ...
    true.
</pre>
</div>
</div>

<div id="outline-container-orgdfef30b" class="outline-4">
<h4 id="orgdfef30b"><span class="section-number-4">1.7.12.</span> Control mechanisms in <code>prolog</code>: proof trees and backtracking</h4>
<div class="outline-text-4" id="text-1-7-12">
<p>
The default control flow in <code>prolog</code> involves the <b>proof tree</b> and <b>backtracking</b>:
</p>
<ul class="org-ul">
<li>Prolog's execution can be thought of as exploring a <b>proof tree</b>.</li>
<li>The nodes in the proof tree are:
<ul class="org-ul">
<li><b>Or-nodes</b>: represents alternative ways (clauses) to prove a goal. These represent a choice between different clauses that might prove a goal. For example, if a predicate is defined by two clauses, the proof tree will branch (an “or” branch) showing both possibilities.</li>
<li><b>And-nodes</b>: represent conjunctions where all subgoals must succeed. These represent the situation where multiple subgoals (conjuncts) must all succeed for the parent goal to succeed. In a clause like: <code>goal :- subgoal1, subgoal2.</code>, both <code>subgoal1</code> and <code>subgoal2</code> must be proven, this is an "and" relationship.</li>
</ul></li>
</ul>
<p>
The default strategy from <code>prolog</code> uses depth-frist, left-to-right search. It explores one branch fully, backtracking only when necessary.
</p>

<p>
Recall that in HW2, we had to manually define an "acceptor" function to handle multiple solutions. Here, <code>prolog</code> automatically backtracks.
</p>
<ul class="org-ul">
<li>When a goal fails, Prolog returns to the previous choice point (an or-node) and tries a different alternative.</li>
<li>This backtracking continues until a solution is found or no more alternatives remain.</li>
</ul>

<p>
We can visualize a proof tree. Consider a simple <code>prolog</code> program:
</p>
<div class="org-src-container">
<pre class="src src-prolog">% Facts for colors of fruits.
fruit(apple).
fruit(banana).

% Rule: A fruit is tasty if it is an apple.
tasty(Fruit) :- fruit(Fruit), Fruit = apple.

% Alternative rule: A fruit is tasty if it is a banana.
tasty(Fruit) :- fruit(Fruit), Fruit = banana.
</pre>
</div>
<p>
Querying <code>?- tasty(X).</code> would essentially have <code>prolog</code> DFS L-R searching the following simplified diagram
</p>
<pre class="example">
                    [tasty(X)]
                         │
            ┌────────────┴────────────┐   &lt;--- OR-node (two alternative clauses for tasty/1)
            │                         │
  Clause 1: tasty(Fruit) :-     Clause 2: tasty(Fruit) :-
      fruit(Fruit),                fruit(Fruit),
      Fruit = apple.               Fruit = banana.
            │                         │
       (And-node: Both           (And-node: Both
        subgoals must            subgoals must
        succeed)                 succeed)
            │                         │
     ┌──────┴──────┐            ┌─────┴─────┐
     │             │            │           │
[fruit(Fruit)]  [Fruit = apple]  [fruit(Fruit)] [Fruit = banana]
     │             │            │           │
  Succeeds:       Binds:       Succeeds:   Binds:
  Fruit = apple   Fruit = apple Fruit = banana Fruit = banana
     │             │            │           │
     └─────────────┴────────────┘           └─────────────┴─────────────┘
              Each clause yields a solution:
                  X = apple   OR   X = banana
</pre>
<p>
there are two solutions: <code>X = apple</code> and <code>X = banana</code>. Consider a predicate defined with multiple subgoals
</p>
<div class="org-src-container">
<pre class="src src-prolog">% Facts:
parent(john, mary).
parent(mary, susan).

% Rule for grandparent:
grandparent(X, Z) :- parent(X, Y), parent(Y, Z).
</pre>
</div>
<p>
On the query <code>?- grandparent(john, Who).</code>, we have the following proof tree:
</p>
<pre class="example">
       [grandparent(john, Who)]
                   │
        (Rule: grandparent(X,Z) :- parent(X,Y), parent(Y,Z))
                   │
        ┌──────────┴──────────┐   &lt;--- AND-node (both subgoals must succeed)
        │                     │
[parent(john, Y)]       [parent(Y, Who)]
        │                     │
  Fact: Y = mary        Fact: Who = susan
        │                     │
        └──────────┬──────────┘
                   │
     Conclusion: grandparent(john, susan) holds

</pre>
</div>
</div>
<div id="outline-container-orge321a82" class="outline-4">
<h4 id="orge321a82"><span class="section-number-4">1.7.13.</span> Control mechanisms in <code>prolog</code>: the cut operator (<code>!</code>)</h4>
<div class="outline-text-4" id="text-1-7-13">
<p>
The cut operator <code>!</code> is a control primitive that tells the <code>prolog</code> interpreter to "commit" to the choices made up to that point and not consider any altnerative solutions for the current predicate. How?
</p>
<ul class="org-ul">
<li>When <code>prolog</code> encounters a cut, it succeeds immediately.</li>
<li>If later backtracking reaches the cut, the interpreter does not try any alternatives that were available before the cut - it "cuts them off."</li>
</ul>
<p>
Vaguely, in a proof tree we might have
</p>
<pre class="example">
  (Goal P)
    /    \
 ...      Alternatives
  |          │
 [!]  &lt;--- Cut: No alternative exploration here
  |
Continue with rest of P
</pre>
<p>
Suppose we have a predicate that generates candidates using <code>member/2</code> and then performs an expensive test <code>R/3</code> only if the candidate is valid. Without cut, if <code>R</code> fails, <code>prolog</code> backtracks and may re-try the same candidate multiple times:
</p>
<div class="org-src-container">
<pre class="src src-prolog">% Without cut: Multiple alternatives might be explored.
process(X, Z) :-
    member(X, List),
    expensive_test(X, Z).
</pre>
</div>
<p>
to prevent multiple re-evaluations of <code>expensive_tgest/2</code> for the same candidates, you can write a variant (commonly known as a <i>member check</i>):
</p>
<div class="org-src-container">
<pre class="src src-prolog">% Member check using cut:
member_check(X, [X|_]) :- !.
member_check(X, [_|Tail]) :-
    member_check(X, Tail).
</pre>
</div>
<p>
here, the cut <code>!</code> ensures that once a match is found, <code>prolog</code> does not try alternative clauses for <code>member_check/2</code> for that candidate.
</p>

<p>
<b>Scope</b>:
</p>
<ul class="org-ul">
<li>The cut only affects alternatives in the <i>current</i> predicate. It does not remove choice points outside its predicate.</li>
<li>Placing a cut too early or too late can affect whether some solutions are even considered.</li>
</ul>

<p>
Now we can move onto the <code>once/1</code> predicate: the purpose of <code>once/1</code> is that it serves as a meta predicate (<code>once(P)</code>) that succeeds if <code>P</code> can be prove at least once, but prevents any further backtracking into <code>P</code>. At a high level,
</p>
<ol class="org-ol">
<li><code>once(P)</code> will try to prove <code>P</code>.</li>
<li>If <code>P</code> succeeds, <code>once(P)</code> succeeds.</li>
<li>If you attempt to backtrack (by i.e. typing a semicolon), no alternative solutions for <code>P</code> will be considered.</li>
</ol>
<div class="org-src-container">
<pre class="src src-prolog">% Suppose you have a predicate that may generate many solutions:
candidate(X) :- member(X, [a,b,a,c]).

% To ensure only one solution is considered:
process_once :-
    once(candidate(X)),
    write(X).
</pre>
</div>
<p>
when we call <code>process_once</code>, it prints only the first solution for <code>candidate(X)</code> even if further alternatives exist.
</p>

<p>
Next we can discuss the backslash-plus operator (<code>+</code>) in <code>prolog</code>. It is a unary operator that implements negation as failure. Its definition is essentially two-clause (meta-predicate) behavior:
</p>
<ol class="org-ol">
<li>First clause: cut-fail branch
<ul class="org-ul">
<li>If the goal <code>P</code> succeeds, then the definition immediately performs a cut and fails.</li>
<li>This branch "rejects" any situation where <code>P</code> can be prove.</li>
</ul></li>
<li>Second clause:
<ul class="org-ul">
<li>If the goal <code>P</code> fails, then <code>+ P</code> succeeds; if <code>prolog</code> cannot prove <code>P</code>, it concludes that <code>P</code> is not provable.</li>
</ul></li>
</ol>
<p>
Informally,
</p>
<div class="org-src-container">
<pre class="src src-prolog">% Negation as failure definition (informal)
\+ P :- P, !, fail.
\+ _.
</pre>
</div>
<ul class="org-ul">
<li>The first clause says: "Try to prove P. If P succeeds, execute the cut (!) to prevent backtracking, then force failure with fail."</li>
<li>The second clause is reached only if the first clause fails (i.e. <code>P</code> was not provable). In that case, <code>+P</code> succeeds.</li>
</ul>
<p>
Consider the following examples. First, consider negating a simple equality. Take the query <code>?- \+ (X = 9).</code>. Step-by-step, we have
</p>
<ol class="org-ol">
<li>An attempt to prove the inner goal: <code>prolog</code> first tries to prove the goal <code>(X = 9)</code>.
<ul class="org-ul">
<li>Since <code>X</code> is unbound, <code>X = 9</code> succeeds by binding <code>X</code> to <code>9</code>.</li>
</ul></li>
<li>First clause activation
<ul class="org-ul">
<li>Because the inner goal succeeded, the first clause of <code>+</code> is used.</li>
<li>The cut <code>!</code> is exexucted, which prevents any backtracking to try alternative proofs for <code>(X = 9)</code>.</li>
<li>Immediately after the cut, <code>fail</code> is excuted.</li>
<li>Result: query fails (returns <code>no.</code>).</li>
</ul></li>
</ol>
<p>
<b>Importantly</b>, the operator <code>+</code> does not "return" a new binding for <code>X</code>. Instead, it tests whether the given predicate can be proven. In this case, it can so the negation fails.
</p>

<p>
Now consider negating a non-provable goal. Take the query <code>?- \+ (member(9, [1,2,3])).</code>. Step-by-step we have
</p>
<ol class="org-ol">
<li>An attempt to prove the goal <code>member(9, [1,2,3])</code>
<ul class="org-ul">
<li>Since <code>9</code> is not in the list, the goal fails.</li>
</ul></li>
<li>Second clause activation
<ul class="org-ul">
<li>Since the inner goal failed, <code>prolog</code> does not invoke the fist clause.</li>
<li>Instead, it falls through to the second clause, which succeeds unconditionally</li>
<li>Then, this query <code>?- \+ (member(9, [1,2,3])).</code> succeeds.</li>
</ul></li>
</ol>
<p>
Below is a diagram that visuales the evaluation of <code>\+ P</code>:
</p>
<pre class="example">
    Start: Evaluate "\+ P"
               │
       ----------------
       |              |
Try to prove P       (P fails?)
       │                   │
  [P succeeds]         [P fails]
       │                   │
Execute P, then            Directly
  cut (!) and fail         succeed
       │                   │
  (No alternative)         \+ P succeeds
       │
  \+ P fails
</pre>
</div>
</div>

<div id="outline-container-orgc4cc43e" class="outline-4">
<h4 id="orgc4cc43e"><span class="section-number-4">1.7.14.</span> Philosophy of <code>prolog</code></h4>
<div class="outline-text-4" id="text-1-7-14">
<p>
We are concerned with <b>provability vs. truth</b> now.
</p>
<ol class="org-ol">
<li>Probability:
<ul class="org-ul">
<li>In Prolog, <code>+ P</code> succeeds if <code>P</code> is not provable from the given facts and rules. This is the essence of <b>negation as failure</b>.</li>
<li>In Prolog, “+ P” means “P is not provable from the given facts and rules.”</li>
</ul></li>
<li>Truth under the <b>Closed World Assumption</b> (CWA):
<ul class="org-ul">
<li><b>CWA:</b> assumes that the database contains all true facts.</li>
<li>Under CWA, if <code>P</code> cannot be proven, it is assumed to be false.</li>
<li>However, in the "real world" (or in incomplete databases), failing to prove <code>P</code> does not necessarily mean <code>P</code> is false.</li>
</ul></li>
</ol>
<p>
In practice, consider the following example:
</p>
<ul class="org-ul">
<li>In a course prerequisite database, if the fact “Cs131 is a prerequisite for Dance201” is not present, <code>+ P</code> will succeed.</li>
<li>But strictly speaking, the absence of a fact does not guarantee that the relationship is false—it just means it wasn’t recorded.</li>
<li>Many programming environments (e.g., in imperative languages) operate under a closed world assumption.</li>
</ul>
<p>
There are some <b>limitations</b> of negation as failure. Namely,
</p>
<ul class="org-ul">
<li><b>Non-commutativity</b>: Because of the cut and the order of evaluation, the order of conjuncts matters. For example:
<ul class="org-ul">
<li>If you ask <code>\+ (X = 6), X = 5</code>, Prolog binds <code>X = 5</code> first, then tries to prove <code>X = 6</code> (which fails), so the negation succeeds.</li>
<li>If you reverse the order, you might end up with a different (and unintended) behavior.</li>
</ul></li>
<li><b>Counterintuitive behavior</b>: The technique works well when the domain is well understood (closed world), but it can be unsound or counterintuitive in cases where alternative bindings exist. For instance, if you try to use it to prove “X is not 9” by writing <code>\+ (X = 9)</code>:
<ul class="org-ul">
<li>Prolog will fail because it can bind X to 9, even though there might be other possible values for X.</li>
<li>The logic here does not “search for” a value that makes the goal false—it simply checks if the goal is provable.</li>
</ul></li>
</ul>
<p>
<b>Incompleteness</b>:
Professor Piano’s dream (circa 1900) was to have a complete and consistent theory for integer arithmetic. However, Gödel’s incompleteness theorems later showed that for sufficiently complex theories, there are true statements that cannot be proven. For <b>prolog</b>, the use of + (negation as failure) mirrors this in that it bases “falsehood” on the inability to prove a statement, rather than on an absolute notion of falsehood.
</p>

<p>
Take for example our query <code>\+ (X = 9).</code>. Although there exist values for X (like 10) that would make the statement “X is not 9” true, Prolog simply tests if it can prove <code>X = 9</code>. Because Prolog unifies X with 9 immediately, the negation fails—even though we might think “X could be something else.” There's some more trouble we can run into. Suppose we have two goals:
</p>
<ol class="org-ol">
<li>Goal A: bind <code>X</code> to <code>5</code> (<code>X = 5</code>)</li>
<li>Goal B: prove that <code>X</code> equals <code>6</code> (<code>X = 6</code>).</li>
</ol>
<p>
We then apply the negation operator <code>+</code> (negation as failure) to the second goal. There are two orders to consider:
</p>
<ol class="org-ol">
<li><code>?- (X = 5), \+ (X = 6).</code>
<ul class="org-ul">
<li>First goal: <code>X = 5</code>; Prolog binds X to 5.</li>
<li>Second goal: <code>X = 6</code>. Prolog tests if <code>X = 6</code> can be proven. With <code>X</code> already bound to 5, the unification <code>5 = 6</code> fails. Since <code>X = 6</code> fails, the negation <code>+</code> succeeds.</li>
<li>Result: overall query succeeds, <code>X</code> remains bound to 5.</li>
</ul></li>
<li><code>?- \+ (X = 6), (X = 5).</code>
<ul class="org-ul">
<li>First goal: <code>\+ (X = 6)</code>. Prolog attempts to prove <code>X = 6</code>. Because <code>X</code> is not yet bound <code>X = 6</code> succeeds by binding <code>X</code> to 6. The <code>+</code> operator then immediately executes its cut and <code>fail</code> (first clause of the <code>+</code> definition): <code>\+ P :- P, !, fail.</code>
<ul class="org-ul">
<li>Because <code>X = 6</code> succeeded, the cut is executed and then <code>fail</code> forces the whole negation to fail.</li>
</ul></li>
<li>Since the first goal fails, <code>prolog</code> does not even proceed to the second goal <code>(X = 5)</code>.</li>
<li>Result: the overall query <b>fails</b> and no solution is returned.</li>
</ul></li>
</ol>
<p>
In <code>prolog</code>, the order in which you write goals is critical when using negation as failure.
</p>
<ul class="org-ul">
<li>In order 1, because the binding occurs first (<code>X = 5</code>), the subsequent test (<code>X = 6</code>) fails as expected.</li>
<li>In order 2, because the negation is applied before any binding, the unification <code>(X = 6)</code> succeeds (binding X to 6) and triggers the cut-fail sequence.</li>
</ul>
<p>
This behavior demonstrates that when cuts and negation are involved, the conjunction operator <code>,</code> is <b>not commutative</b>. This departure from commutativity is one of the reasons why logicians become nervous when cuts are introduced. The order-sensitive behavior can lead to results that are “illogical” or counterintuitive when compared to classical logic, where conjunction is commutative.
</p>
</div>
</div>

<div id="outline-container-org0838636" class="outline-4">
<h4 id="org0838636"><span class="section-number-4">1.7.15.</span> Logic</h4>
<div class="outline-text-4" id="text-1-7-15">
<p>
Why should we care about logic? We’ve seen that using cuts and negation as failure in Prolog can cause non-commutative behavior. e.g. the order of goals. Understanding why these anomalies occur requires some background in logic. Prolog is built upon well-established logical systems that originate in philosophy. Today, we review the simplest system—propositional logic—and then see why it isn’t enough to capture everyday reasoning, which leads us to first‑order logic and finally to Horn clauses.
</p>

<p>
<b>Proposition</b>: simple statements about the world. We abbreviate them with simple letters (P, Q, R, &#x2026;).
</p>

<p>
<b>Basic Connectives</b>:
</p>
<ul class="org-ul">
<li>Negation (¬): The negation of P (written as ¬P) is true if P is false and vice versa.</li>
<li>Conjunction (∧): P ∧ Q is true only if both P and Q are true.</li>
<li>Disjunction (∨): P ∨ Q is true if at least one of P or Q is true.</li>
<li>Implication (→): P → Q (read “P implies Q”) is false only when P is true and Q is false.
<ul class="org-ul">
<li>Truth table:
<ul class="org-ul">
<li>If P is false, P → Q is always true.</li>
<li>If P is true and Q is true, then P → Q is true.</li>
<li>If P is true and Q is false, then P → Q is false.</li>
</ul></li>
</ul></li>
<li>Biconditional (↔): P ↔ Q means “P if and only if Q” (they have the same truth value).</li>
</ul>
<p>
There are some limitations of propositional logic.
</p>
<ol class="org-ol">
<li>Expressiveness - Propositional logic treats propositions as atomic entities. It cannot express internal structure (e.g., “Socrates is a man” and “All men are mortal”).</li>
<li>Syllogisms: e.g. "All men are mortal", "Socrates is a man", "Therefore Socrates is mortal." In propositional logic, you’d have to treat these as whole propositions (say, P, Q, R), but then the logical form isn’t a tautology. It is merely an implication that might not hold for arbitrary truth assignments.</li>
</ol>
<p>
<b>First-Order Logic</b> (Predicate Calculates) adds
</p>
<ol class="org-ol">
<li>Predicates with arguments: Instead of atomic propositions, you have predicates such as man(x) and mortal(x).</li>
<li>Quantifiers: universal quantifier (for all) and existential quantifier (there exists)</li>
</ol>
<p>
Tautalogy is a formula that is true regardless of the interpretation of its component terms, with only the logical constants have a fixed meaning.
</p>

<p>
<b>Horn Clauses</b> is a type of clause with at most one positive (unnegated) literal.
</p>
<ol class="org-ol">
<li>Fact: A Horn clause with one positive literal and no antecdents (<code>man(socrates).</code>)</li>
<li>Rule: A Horn clause with one positive literal (the head) and a body (a conjunction of literals). (<code>mortal(X) :- man(X).</code>)</li>
<li>Query: Also expressed as Horn clauses.</li>
</ol>
<p>
Prolog works on Horn clauses. There are some reasons:
</p>
<ol class="org-ol">
<li>Efficiency
<ul class="org-ul">
<li>Prolog's inference mechanism (DFS, backward chaining) works efficiently on Horn clauses</li>
<li>They restrict the form of logical expressions so that the problem of logical inference becomes tractable in practice</li>
</ul></li>
<li>Limitation: less-expressive than full first-order logic</li>
</ol>
<p>
We can convert general first-order logic to Horn clauses. Take for example:
</p>
<ul class="org-ul">
<li>“If you are in Santa Monica and you have a license, then you are either a dog, a cat, or a pig.”</li>
</ul>
<p>
This clause has a disjunction in the consequent. In practice, we would split this into multiple Horn clauses:
</p>
<pre class="example">
licensed_in_sm(X) -&gt; dog(X).
licensed_in_sm(X) -&gt; cat(X).
licensed_in_sm(X) -&gt; pig(X).
</pre>
<p>
Prolog then works efficiently because each cluase has at most one positive literal.
</p>
</div>
</div>
</div>

<div id="outline-container-org18d004e" class="outline-3">
<h3 id="org18d004e"><span class="section-number-3">1.8.</span> Scheme</h3>
<div class="outline-text-3" id="text-1-8">
<p>
<b>At a glance</b>, (TODO)
</p>


<p>
First, why scheme? The idea of Scheme is to try and strip away as much as possible from the core of the language. But, more specifically:
</p>
<ul class="org-ul">
<li><b>Simplicity of syntax</b>: Scheme has a very simple and uniform syntax compared to languages like ML or C++. Every expression is written as a list, which makes parsing and manipulation straightforward.</li>
<li><b>Programs as data</b>: In Scheme, code is data. Every Scheme program is represented using the same data structures (lists, atoms, pairs) that you use for ordinary data. This allows you to write programs that generate, transform, or evaluate other programs on the fly.</li>
<li><b>Continuations</b>: (later) Advanced control structure in Scheme. They provide a powerful way to manipulate control flow and are often considered one of the nicest low-level control primitives available. Like <code>goto</code> in <code>C++</code> or <code>!</code> in <code>prolog</code>.</li>
<li>Scheme doesn’t have exception handling or even operations like addition in its core.</li>
<li>Objects in Scheme are dynamically allocated and never explicitly freed (garbage collection).</li>
<li>Types are latent, not manifest, meaning that you look at an object at runtime to figure out what type it is and that it’s not obvious what the type of an object is from the code.</li>
<li>Scheme has static scoping, meaning it has to maintain both a dynamic and static chain. Note that this is unlike Lisp, which uses dynamic scoping. Scheme is call by value and objects in Scheme include the usual (numbers/data) as well as procedures, including continuations.</li>
<li>Scheme syntax is very simple and includes a straightforward representation of a program as data.</li>
<li>Finally, tail recursion optimization is required, not optional.</li>
</ul>
</div>
<div id="outline-container-orgdf7f79e" class="outline-4">
<h4 id="orgdf7f79e"><span class="section-number-4">1.8.1.</span> Basic Scheme syntax</h4>
<div class="outline-text-4" id="text-1-8-1">
<ul class="org-ul">
<li><b>Uniform syntax</b>: a function call in Schem is written with an opening parenthesis follow by the function and its arguments, then a closing parenthesis. e.g. <code>(+ 2 2)</code>.</li>
<li><b>Nested calls</b>: scheme allows you nest expressions. <code>(* (+ 1 2) 3)</code> evaluates to <code>9</code>.</li>
<li><b>Function as data</b>: Scheme also allows function calls where the result of one expression is treated as a function (like partial). For example, <code>((f 3) 4)</code>.</li>
</ul>
<p>
<i>Data Structures</i>
</p>

<p>
The <code>cons</code> cell: the fundamental data structure, constructed using <code>cons</code> function, which creates a pair from two values. e.g. <code>(cons 10 20)</code> creates a pair.
</p>

<p>
In Scheme, a list is either the empty list <code>()</code> or a cons cell whose second element is a list. For example, to build the list <code>(10, 20, 30)</code>, we would do <code>(cons 10 (cons 20 (cons 30 '())))</code> where <code>'()</code> is the empty list.
As a diagram:
</p>
<pre class="example">
cons cell
┌───────┐
│ 10    │──► (cons cell)
└───────┘       ┌───────┐
                │ 20    │──► (cons cell)
                └───────┘       ┌───────┐
                                │ 30    │──► ()
                                └───────┘
</pre>
<p>
An improper list occurs when the final "cdr" (second part) of a cons cell is not the empty list. e.g. <code>(cons 3 (cons 4 5))</code>, and as a diagram,
</p>
<pre class="example">
cons cell
┌───────┐
│ 3     │──► (cons cell)
└───────┘       ┌───────┐
                │ 4     │──► 5   (not a list)
                └───────┘
</pre>
<p>
<b>Quoting</b> in Scheme prevents evaluation. When you quote an expression, you tell Scheme to treat it as literal data rather than trying to evaluate it as a function call. Simply do <code>'F</code>, which produces the symbol <code>F</code> without trying to evaluate it. Or <code>'(1 2 3)</code> returns the list <code>(1 2 3)</code> as data, rather than trying to call the function <code>1</code> on <code>2</code> and <code>3</code>. It is syntactic sugar for <code>quote</code>.
</p>

<p>
Because Scheme code is represented as lists, you can quote an entire program. For example, <code>'(define (square x) (* x x))</code> is treated as a list rather than being executed. Scheme will print it as <code>(define (square x) (* x x))</code>.
</p>
</div>
</div>

<div id="outline-container-org2799cf1" class="outline-4">
<h4 id="org2799cf1"><span class="section-number-4">1.8.2.</span> Scheme vs. other languages</h4>
<div class="outline-text-4" id="text-1-8-2">
<ul class="org-ul">
<li><b>Simple syntax</b>: Scheme’s syntax is minimalistic; every expression is either an atom or a list. This simplicity allows programs to be easily manipulated and represented as data. (very unlike C++, whose simplest program representation is probably a string)</li>
<li>Objects are allocated dynamically, and <i>never freed</i>, we use a garbage collector.</li>
<li><b>Dynamic typing</b>: Unlike ML or OCaml, Scheme does not enforce static type checking. Instead, type checking occurs at runtime. This flexibility means you can mix different types in a list if you’re careful.</li>
<li><b>Interactivity</b>: Scheme’s simple syntax and representation make it very suitable for interactive programming and rapid prototyping. You can easily write, modify, and evaluate code on the fly.</li>
<li><b>Static scoping</b>: all the identifiers in the language that are declared are scoped at compile time, in the sense that you can tell where they are being used used by checking them statically at compile time. The compiler knows how to scope the variable.
<ul class="org-ul">
<li>Like basically every language we have covered: ML, C++, Java, Python</li>
<li>I.e., a variable's scope is determined by its position in the source code. The meaning of a variable is fixed by the block in which it is defined.</li>
</ul></li>
</ul>

<p>
What about <b>dynamic scoping</b>?
</p>

<p>
We have dynamic scoping in languages like Lisp and <code>sh</code>
</p>
<ul class="org-ul">
<li>In Lisp, you have to look at the caller's context to figure out what a nonlocal variable means</li>
</ul>
<div class="org-src-container">
<pre class="src src-elisp">(define (f x)
 (f x y)) ; 'y' is not locally defined in f
(let ((y 12))
 (f 13))  ; here, when f is called, y is bound to 12
(let ((y 19))
 (f 3))    ; here, when f is called, y is bound to 19
</pre>
</div>
<ul class="org-ul">
<li>Shell enviroment variables are passed dynamically and use dynamic scoping</li>
</ul>
<p>
The downside of dynamic scoping is that it's hard to optimize at compile time, as machine code and has more run-time cost. More importantly, it's harder to debug and has less software reliability.
</p>

<p>
This begs the question, why use dynamic scoping at all? For example, if the shell is not dynamically scoped and environment variables are not passed dynamically, what changes? In terms of general advantages, we get
</p>
<ul class="org-ul">
<li><b>Implicit context sharing</b>: Variables are resolved based on the call stack at runtime, which means that functions can automatically access variables defined in their caller's context. This eliminates the need to pass every variable explicitly as a parameter, simplifying code in certain cases.</li>
<li><b>Flexibility</b>: Dynamic scoping allows programs to adapt behavior based on the current runtime context. This is particularly useful in situations where the behavior of a function might need to change according to the environment in which it is called.</li>
</ul>
<p>
Specifically for shell scripts, there are two main advantages
</p>
<ol class="org-ol">
<li><b>Implicit inheritance</b>: Functions and subprocesses automatically see any modifications to environment variables made in the caller’s context. This can simplify scripting since you don’t need to explicitly pass variables as parameters between functions or commands.</li>
<li><b>Flexibility in configuration</b>: Dynamic scoping allows for a more flexible and adaptive behavior. For instance, a script can set an environment variable and have that setting influence all subsequent operations, without having to re-declare or pass it around. This can be especially handy in interactive shells or scripts where the context may change over time.</li>
</ol>
<p>
If the shell were not dynamically scoped, there are some advantages and disadvantages. Specific to environments, we have
</p>
<ul class="org-ul">
<li><b>Explicit passing of variables</b>: Without dynamic scoping, each function or command would operate within a fixed, lexically determined environment. This means that any changes to an environment variable in a caller would not automatically be visible to the callee. Instead, you’d have to pass these variables explicitly. While this can make dependencies clearer and improve predictability, it also introduces more boilerplate code and is against what <code>sh</code> is about.</li>
<li><b>Potential for reduced side effects</b>: Static scoping confines variables to well-defined blocks of code. This isolation could improve reliability by preventing unintended side effects. For example, a change to an environment variable in one part of the script wouldn’t unexpectedly alter the behavior of code in another part. However, the trade-off is a loss of the convenience that comes with the implicit sharing of state.</li>
<li><b>Optimizations, debugging</b>: With static scoping, compilers or interpreters might have an easier time optimizing code since the variable bindings are fixed at compile time. Conversely, dynamic scoping requires additional runtime checks and may introduce subtle bugs if variables are unintentionally shadowed or overwritten, complicating debugging efforts.</li>
</ul>
<p>
Returning to the main topic:
</p>
<ul class="org-ul">
<li>Scheme uses <i>call-by-value</i>: when you call a function, there is a simple calling mechanism that copies the value (like ML, C++, Java, Python)
<ul class="org-ul">
<li>This can also be thought of as having all all arguments evaluated before a function is called.</li>
</ul></li>
<li>Objects include the usual (full-fledged, unbounded integers, strings, etc), and <i>procedures</i> are considered to objects
<ul class="org-ul">
<li>Procedures (functions) are treated like lists by the compiler just like everything else</li>
<li>This also includes continuations</li>
</ul></li>
<li>High-level arithmetic - scientific programming should be correct, we get unbounded integers, floats, easy addition, etc.</li>
<li>TRO: tail recursion optimization. We discuss this further now.</li>
</ul>
<p>
<b>TRO</b>
</p>

<p>
First, a tail call is a function call made as the last operation in a function. Tail calls allow tail recursion optimization (TRO), meaning that recursive calls do not grow the call stack, it will shrink it / stay constant.
</p>

<p>
A naive, typical way to do a recursive factorial could look like
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (factorial n)
  (if (zero? n)
      1
      (* n (factorial (- n 1)))))
</pre>
</div>
<p>
Functions calls here are done recursively via a stack where you place the more recent function calls on top. 
</p>
<ul class="org-ul">
<li>In our case, the last call would be the <code>*</code> function, and when <code>*</code> returns then <code>factorial</code> returns.</li>
</ul>
<p>
However, scheme requires that the last call must be a tail function. In our case, if we wanted to use <code>*</code>, the last function call on the stack
(on top of <code>main</code>) must <code>*</code>.
</p>
<ul class="org-ul">
<li>Everything that happens in the factorial call must be done by the <code>*</code> function.</li>
</ul>
<p>
We want to require TRO because we don't want to overflow the stack with our <code>factorial</code> calls. In order to improve the
code above, we want the function to <i>call itself</i> at the end so that it pops itself off and then also pushes itself back on,
so the stack will not grow (stay constant).
</p>

<p>
We want to multiply first and then call <code>factorial</code>, rather than call <code>factorial</code> first then multiply. Instead, we want
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (factorial n)
  (fact n 1))

(define (fact n a)
  (if (zero? n)
      a
      (fact (- n 1)
            (* a n))))
</pre>
</div>
<p>
Now, following the program, the main function will call <code>factorial</code>, <code>factorial</code> will call <code>fact</code>, <code>fact</code> will call <code>zero?</code>, <code>zero</code> will return
then <code>fact</code> will call <code>fact</code> with a smaller argument.
</p>
<ul class="org-ul">
<li><code>fact</code> will not blow up the stack, in fact Scheme will turn this code into a for loop automatically!</li>
</ul>

<p>
This type of programming, akin to functional programming where we use recursion to essentially do the work of what would typically be a <code>for</code> loop is so common,
Scheme has a simpler way of achieving this, via a <b>named 'let'</b>.
</p>

<p>
A named 'let' is different from an ordinary <code>let</code>, which looks something like
</p>
<div class="org-src-container">
<pre class="src src-scheme">(let ((x (+ 9 3 -7)))
  (* x (+ x 3)))
; something like
; {
 ; int x = 9 + 3 + -7;
 ; return x * (x + 3);
; }
</pre>
</div>
<p>
instead, we have
</p>
<div class="org-src-container">
<pre class="src src-scheme">; named let:
(define (factorial n)
  ; Define an internal/inline function ONLY in the scope of factorial
  ; n' starts as n, a starts as 1
  (let fact ((n' n) (a 1))
    (if (zero? n')
        a
        (fact (- n' 1) (* n' a)))))
; Kinda like a for loop but we use let
; But generally people will just write (n' n) as (n n)
; This is loops but in a functional sense with no side effects
</pre>
</div>
<p>
<code>let</code> binds local variables and defines a local recursive function <code>fact</code> without needing separate <code>define</code>.
</p>
</div>
</div>

<div id="outline-container-org45143ce" class="outline-4">
<h4 id="org45143ce"><span class="section-number-4">1.8.3.</span> Scheme syntax</h4>
<div class="outline-text-4" id="text-1-8-3">
<p>
Identifiers in scheme are almost anything: <code>a-zA-Z0-9+-.?*&lt;&gt;:%_...</code> etc.
</p>
<ul class="org-ul">
<li>This means that something like <code>*</code> is just an identifier in Scheme. For example,</li>
</ul>
<div class="org-src-container">
<pre class="src src-scheme">(let ((* 5))
 (- * 3))
; This is valid code
</pre>
</div>
<p>
Identifiers cannot start with <code>0-9+-.</code>. Except, <code>+</code>, <code>-</code>, <code>...</code>, <code>-&gt;</code> with some stuff after, i.e. <code>-&gt;xyz</code> is an identifier.
</p>

<p>
We comment with <code>;</code>. Lists look like <code>(l i s t s)</code>. Booleans have <code>#t</code> and <code>#f</code>. Interestingly, <code>#f</code> is the only false value in Scheme.
So, something like <code>(if 0 5 10)</code> in Scheme will return 10 because 0 is true. Vectors look like <code>#(a v e c t o r)</code>. Internally, they are represented as
arrays under the hood, which are unlike lists (linked lists).
</p>

<p>
Strings look like <code>"Strings"</code>, characters are #\c, this is the lower case character.
</p>

<p>
Interestingly, numbers are represented a couple of ways in Scheme. We can write things like
<code>12, -19</code>, or we could do <code>12e-9</code> (scientific notation), or also write <code>2/3</code>, which is an
<i>exact</i> representation of <code>2/3</code>. If you multiply this with <code>3/2</code> it will return 1. It's represented by an array
with 2 and 3, which essentially represents the fraction.
</p>
</div>
</div>

<div id="outline-container-org8d4ec6b" class="outline-4">
<h4 id="org8d4ec6b"><span class="section-number-4">1.8.4.</span> Quoting in Scheme</h4>
<div class="outline-text-4" id="text-1-8-4">
<p>
If you put an apostrophy in front of any expression, Scheme handles it differently. If we
have <code>(quote E)</code>, we take <code>E</code> as a piece of data and not evaluate the expression. For example
for a list <code>(a b c)</code> it returns the linked list of <code>a b c</code> and will not evaluate the expression
<code>a</code> with arguments <code>b c</code>.
</p>
<ul class="org-ul">
<li>We can also write <code>(a b .c)</code> is <code>(quote (a b .c))</code>, which creates a list of <code>[a | -] --&gt; [b | c]</code>, which is the same as calling <code>(cons 'a (cons 'b 'c))</code>.
<ul class="org-ul">
<li>Essentially, <code>.</code> creates a pair in Scheme (similar to a tuple). So calling <code>(cons 3 4)</code> returns <code>'(3 . 4)</code> (space is necessary)</li>
</ul></li>
</ul>

<p>
<b>Digression on special forms</b>
</p>

<p>
The quote is considered a special form. A special form is something that <i>looks like</i> a function call, but it's not. For example, some special forms are <code>define</code> and <code>if</code>. We're
not calling <i>functions</i>, just calling a built-in feature. <code>not</code> is a function, and the idea is that special forms cannot be defined as other things in the language. For example, <code>not</code> could be defined as
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (not x)
  (if x #f #t))
</pre>
</div>
<p>
one may think that we could re-write <code>if</code> like
</p>
<pre class="example">
(define (if-test cond then-part else-part)
  (if cond then-part else-part))
</pre>
<p>
and then called it as  <code>(if (zero? n) 1 (/ 1 n))</code>, this not will work since Scheme does <b>call by value</b>. Scheme would value both <code>1</code> and <code>(/1 n)</code> <i>before</i> <code>if-test</code> is applied.
If <code>n</code> is zero, we would actually get a division by zero. If <code>if</code> were not a special form, you would need to delay the branches. One approach would be
to pass the branches as zero-argument procedures (thinks) so thaty they are not evaluated until explicitly called. For example,
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (if-func cond then-thunk else-thunk)
  (if cond (then-thunk) (else-thunk)))
; call it like
(if-func (zero? n)
         (lambda () 1)
         (lambda () (/ 1 n)))
</pre>
</div>
</div>
</div>

<div id="outline-container-org7dda66b" class="outline-4">
<h4 id="org7dda66b"><span class="section-number-4">1.8.5.</span> Special form: <code>lambda</code></h4>
<div class="outline-text-4" id="text-1-8-5">
<p>
The most special special form in Scheme is the <code>lambda</code> expression. We could write something like <code>(lambda (x) (+ x 1))</code> which just takes the argument and adds one.
We can also nest lambdas in lambdas. For example,
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define f (lambda (x)
 (lambda (y)
 (+ x y))))
; scope of x here extends to the bottom call
; you can curry functions here just like you can in ml
; fun x -&gt; fun y -&gt; x + y
(define g (f 3))
(g 7)
; same as let f = fun x -&gt; fun y -&gt; x + y, let g = f 3, g 7
</pre>
</div>
<p>
Scheme has a notion of variadic functions, you can write something like <code>(define printf (lambda (format . args) (........)))</code>, this is a special form for
a function that takes one <i>or more</i> arguments. We could call the function like <code>(printf "%d hello %r" 19 "abc")</code>, when this function is called,
<code>args</code> is bound to a list of the caller's trailing arguments. It would look something like <code>[lambda | -]-&gt;[[format | args] | -]-&gt; ........</code>, and we take it apart
with <code>car</code> and <code>cdr</code>.
</p>

<p>
Lambda is perhaps the lowest level of function definitions in Scheme, it's hardwired into the compiler. You
cannot define lambda in terms of anything else. However, there's common syntactic sugar for defining functions.
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (f x y ) (+ x (* y 3)))
; same as
(define f (lambda x y) (+ x (* y 3)))
(define (list x x))
; same as
(define list (lambda x x))
; this is also built into the compiler
</pre>
</div>

<p>
Calling something like <code>(list a b (+ c 5))</code> will evaluate the three expressions, i.e. returns a 3 item list
with 3 values that return from evaluating <code>a</code>, <code>b</code> and <code>(+ c 5)</code>. If you quote this, it will just return the data structure. 
</p>

<p>
<b>Back to quoting</b>
</p>

<p>
We have quasiquoting / dequoting, with <code>`</code> (backtick). It allows you to "unquote" parts of the expression (using a comma <code>,</code>) so those parts
are evaluated instead of taken literally. Suppose somewhere in your code you have <code>(define b 37)</code>. Now <code>b</code> is bound to the value <code>37</code>. 
When we dequote the list <code>`(a ,b (+ c 5))</code>, it will look something like <code>(a 37 (+ c 5))</code>. In this way,
quasiquoting is like a template: we fill it what we have later. Unlike quoting, we can nest quasiquotes. For example, take
<code>`(a ,(append `(3 x) l) (+ c 5))</code>. Since <code>a</code> is not unquoted, it remains <code>a</code>. The second element is <code>,(append (3 x) l)</code>, which
means the expression should be evaluated. So <code>`(3 x)</code> expands to <code>(list 3 x)</code>. Then, <code>(append '(3 x) l)</code> is evaluated. Suppose <code>l = '(y z)</code>, then our final
expanded list is <code>(a (3 x y z) (+ c 5))</code>.
</p>
</div>
</div>

<div id="outline-container-org9db7190" class="outline-4">
<h4 id="org9db7190"><span class="section-number-4">1.8.6.</span> Scoping</h4>
<div class="outline-text-4" id="text-1-8-6">
<p>
If in a <code>let</code>, you declare several variable, such as the following (assume <code>a</code> and <code>x</code> are defined outside)
</p>
<div class="org-src-container">
<pre class="src src-scheme">  (let ((x (+ a 3))
      (y (* x2)))
    + (* x x) (- y x)) ; body of let
; x's scope extends all the way to the bottom, but does not extend to the scope of y
</pre>
</div>
<p>
In Scheme's standard <code>let</code>, all the binding expressions are evaluated simultaneously in the environment outside
the <code>let</code>. This means that each binding's initializer is computed without knowing about any of the other variables defined in the <i>same</i> <code>let</code>. More specifically, step-by-step:
</p>
<ul class="org-ul">
<li>When you enter the <code>let</code>, Scheme creates a new environment for the variables that will be bound (in this case, <code>x</code> and <code>y</code>).</li>
<li>However, the expressions used to compute the values for <code>x</code> and <code>y</code> are evaluated in the <b>environment outside</b> the <code>let</code>. This means that any bindings introduced in this same <code>let</code> are not visible during these evaluations.</li>
</ul>
<p>
In our example:
</p>
<ul class="org-ul">
<li>The expression for <code>x</code> is evaluated: <code>(+ a 3)</code>. At this point, <code>a</code> is looked up in the outer environment, but the new binding for <code>x</code> is not available (it hasn’t been established yet).</li>
<li>The expression for <code>y</code> is evaluated: <code>(* x 2)</code>. Here, you might expect that the new binding for <code>x</code> would be used. However, because all bindings are set up simultaneously, the <code>x</code> used in <code>(* x 2)</code> is the <code>x</code> from the outer environment (if any), not the one being defined in the <code>let</code>.</li>
<li>After both expressions are evaluated, the resulting values are then bound to <code>x</code> and <code>y</code> respectively in the new environment.</li>
<li>Now, within the body of the let, when you refer to <code>x</code> or <code>y</code>, Scheme will use these newly established bindings.</li>
</ul>
<p>
If you wanted sequential dependency, just <code>let*</code>. This is part of Scheme's philsophy that "you cannot define something in terms of itself." 
Our function could also be written like
</p>
<div class="org-src-container">
<pre class="src src-scheme">((lambda (x y) (+ (* x x) ( - y x)))
 (+ a 3) ; call
 (* x 2)) ; call
; inside the scheme compiler it will compile the first code into the
second
; notice that since we convert let to lambda we can see how x extends its
; scope to the body of the funciton + (* x x) ......, but does not extend to
; the + a3 and * x2 because its not in scope for lambda
</pre>
</div>

<p>
Some more special functions. We have <code>(and E1 ... En)</code>, which evaluates each expression <code>E1 -&gt; En</code> left to right,
and if any of them return <code>false</code> immediately returns <code>false</code>. However, it differs from <code>&amp;&amp;</code> in <code>C++</code> because if all of them
return true, it will return <code>En</code>. Similarily, we have <code>or</code>, which for the first value it succeeds on, it just returns. For example, we
could have <code>(or (getenv "PATH") "/bin:/usr/bin")</code>, gives you simple backtracking.
</p>

<p>
<b>More on tail recursion.</b> 
</p>

<p>
If we have a funtion like the following,
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (f n)
  (if (zero? n)
      (f x)  ; tail call
      37))

(lambda (...) 
  ... 
  (f ...))  ; the last f is in the tail call context
            ; because you are committed to returning what f returns

(if E1 
    E2  ; tail call context
    E3) ; tail call context

(and E1 ... En) ; En is in the tail call context
</pre>
</div>
<p>
we have tail call contexts, where as long as the last function call is the last thing called, it's a tail call. Thinking
in terms of what we <i>committing</i> to calling.
</p>

<p>
<b>Macros</b>.
</p>

<p>
You can define your own special forms if you want. You can use the special form <code>define-syntax</code>, which lets you define your own special forms. Similar to macros in
<code>C/C++</code>. In fact, it is not uncommon that <code>and</code> and <code>or</code> are not defined and instead written using <code>define syntax</code>. We could write
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define-syntax and
  (syntax-rules ()
    ((and) #t) ; When there are no arguments, return true.
    ((and x) x) ; When there's a single argument, simply return x.
    ((and x y ...)
     (if x (and y ...) #f))) ; For more than one argument, test x; if true, recursively process the rest.
; Writing
(and (&lt; x 1) (&lt; y 3) (p x))
; Expands to
(if (&lt; x 1)
    (if (&lt; y 3) (p x) #f)
    #f)
</pre>
</div>
<p>
We also can define <code>or</code>, but care must be taken to avoid evaluating an expression more than once.
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define or
  (syntax-rules ()
    ((or) #f) ; No arguments yields false.
    ((or x) x) ; A single argument returns x.
    ((or x y ...)
     (if x x (or y ...))))) ; Otherwise, if x is true, return x; else, check the rest.
; Writing
(or (getenv "PATH") "/bin:/usr/bin")
; Expands to
(if (getenv "PATH")
    (getenv "PATH")
    "/bin:/usr/bin")
</pre>
</div>
<p>
you can think of it as, what should the compiler substitute in as text? Also, in order to avoid multiple
evaluations, we can write
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define or
  (syntax-rules ()
    ((or) #f)
    ((or x) x)
    ((or x y ...)
     (let ((xc x))
       (if xc xc (or y ...))))))
; Recall that if works like
(if test consequent alternative)
</pre>
</div>
<p>
to avoid multiple evaluations, the macro use a <code>let</code> to bind the value of the first expression to a temporary variable <code>xc</code>. 
What happens when we calling the <code>or</code> function with a predefined variable called <code>xc</code>? In our case
</p>
<div class="org-src-container">
<pre class="src src-scheme">(let ((xc 93)) 
  (or (getenv "0") xc))

; This will transform into:

(let ((xc (getenv "0")))
  (if xc xc xc))

; The compiler will actually differentiate between the 
; two xc variables and will label them differently,
; resulting in something like this:

(let ((xc1 (getenv "0")))
  (if xc1 xc1 xc2))
</pre>
</div>
</div>
</div>

<div id="outline-container-orge7974de" class="outline-4">
<h4 id="orge7974de"><span class="section-number-4">1.8.7.</span> Bonus Scheme</h4>
<div class="outline-text-4" id="text-1-8-7">
<p>
<b>Tail-recursive Fib</b>
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (fib n)
  (define (fib-helper a b n)
    (if (= n 0)
        a
        (fib-helper b (+ a b) (- n 1))))
  (fib-helper 0 1 n))
</pre>
</div>
<ul class="org-ul">
<li><code>fib-helper</code> carries accumulators (<code>a</code> and <code>b</code>) representing successive Fibonacci numbers.</li>
<li>When <code>n</code> reaches 0, <code>a</code> is the result.</li>
<li>Runs in linear time and uses tail recursion.</li>
</ul>
<p>
We now turn to some list processing code. 
</p>

<p>
<b>Sum of a list</b>:
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (sum lst)
  (if (null? lst)
      0
      (+ (car lst) (sum (cdr lst)))))
</pre>
</div>
<ul class="org-ul">
<li>If the list empty (<code>null? lst</code>) return 0.</li>
<li>Otherwise, add the first element (<code>car lst</code>) to the sum of the rest (<code>cdr lst</code>)</li>
</ul>
<p>
Another useful function is <code>map</code>:
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (square x) (* x x))

(define (map-square lst)
  (map square lst))

(map-square '(1 2 3 4 5)) ; Returns (1 4 9 16 25)
</pre>
</div>
<ul class="org-ul">
<li><code>square</code> simply squares the argument.</li>
<li><code>map</code> applies <code>square</code> to each element of the list.</li>
</ul>
<p>
Also useful is <code>filter</code>:
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (even? x) (= (modulo x 2) 0))

(define (filter-even lst)
  (filter even? lst))

(filter-even '(1 2 3 4 5 6)) ; Returns (2 4 6)
</pre>
</div>
<ul class="org-ul">
<li><code>even?</code> checks whether a number is even</li>
<li><code>filter</code> returns a list of elements that satisfy the predicate <code>even?</code></li>
</ul>
<p>
Also neat are <code>lambdas</code>. A few examples - first, defining a lambda
</p>
<div class="org-src-container">
<pre class="src src-scheme">(lambda (x) (+ x 1))
</pre>
</div>
<p>
Using lambda in higher-order functions
</p>
<div class="org-src-container">
<pre class="src src-scheme">(map (lambda (x) (* x x)) '(1 2 3 4 5)) ; Returns (1 4 9 16 25)
</pre>
</div>
<p>
Assigning a lambda to a variable
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define add-one (lambda (x) (+ x 1)))
(add-one 5) ; Returns 6
</pre>
</div>
<p>
Using lambda in filtering
</p>
<div class="org-src-container">
<pre class="src src-scheme">(filter (lambda (x) (&gt; x 3)) '(1 2 3 4 5)) ; Returns (4 5)
</pre>
</div>
<p>
Using lambda in folding (reduction)
</p>
<div class="org-src-container">
<pre class="src src-scheme">(foldl (lambda (x y) (+ x y)) 0 '(1 2 3 4 5)) ; Returns 15
</pre>
</div>
<p>
<b>Hygienic macros</b> ensure that variables defined within a macro do not accidentally capture or clash with variables in the surrounding code. They preserve lexical scope and prevent unintended side effects.
</p>
<ul class="org-ul">
<li>Example: swapping two values</li>
</ul>
<div class="org-src-container">
<pre class="src src-scheme">(define-syntax swap!
  (syntax-rules ()
    ((swap! x y)
     (let ((temp x))
       (set! x y)
       (set! y temp)))))
</pre>
</div>
<ul class="org-ul">
<li><code>swap!</code> macro takes two variables and swaps their values</li>
<li>The macro is hygienic: teh temporary variable <code>temp</code> is local to the macro and cannot interfere with any variable outside</li>
</ul>
<p>
Another example is a <code>when</code> macro:
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define-syntax when
  (syntax-rules ()
    ((when condition body ...)
     (if condition
         (begin body ...)))))
</pre>
</div>
<ul class="org-ul">
<li>The <code>when</code> macro executes a sequence of expressions only if the condition is true.</li>
<li>The use of <code>begin</code> groups the multiple expressions together.</li>
</ul>
<p>
Another example would be a looping macro (<code>while</code>):
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define-syntax while
  (syntax-rules ()
    ((while test body ...)
     (let loop ()
       (when test
         body ...
         (loop))))))
</pre>
</div>
<ul class="org-ul">
<li>The <code>while</code> macro repeatedly evaluates the <code>body</code> as long as the <code>test</code> condition holds true.</li>
<li>The internal <code>loop</code> function is recursively called; the macro is hygienic, so the identifier <code>loop</code> does not conflict with any other variable.</li>
</ul>
<p>
Hygienic macros prevent accidental variable capture, making code more maintainable and predictable.
</p>
</div>
</div>
<div id="outline-container-org8899700" class="outline-4">
<h4 id="org8899700"><span class="section-number-4">1.8.8.</span> Primitives vs. Library</h4>
<div class="outline-text-4" id="text-1-8-8">
<p>
<b>Primitives</b> are core language constructs built into Scheme that cannot be defined in terms of simpler language constructs. For example, <code>if</code> and <code>lambda</code>. The "bare bones" of the language.
</p>

<p>
On the other hand, <b>library functions/macros</b> can be defined in terms of primitives.
</p>
<ul class="org-ul">
<li>The Scheme standard clearly categorizes what is considered a primitive and what belongs to the library.</li>
<li>vs. languages like C++ or Java, in which the distinction between core features and libraries is often blurred.</li>
</ul>
<p>
Scheme maintains a very small set of primitives.
</p>
<ul class="org-ul">
<li><b>Required features</b>: Every Scheme implementation must support a set of core features (e.g., integer arithmetic, basic I/O).</li>
<li><b>Optional features</b>: Features like multiple-precision floating-point arithmetic (i.e., numbers wider than 64 bits) are optional.</li>
<li><b>Extensions</b>: Implementation-specific features that are not standardized.</li>
<li><b>Implication for portability</b>:
<ul class="org-ul">
<li>Programs that stick to required features are highly portable.</li>
<li>Relying on optional features or extensions can reduce portability to only a few implementations (e.g. only running on Racket)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org66bbf59" class="outline-4">
<h4 id="org66bbf59"><span class="section-number-4">1.8.9.</span> Categorization of errors in Scheme</h4>
<div class="outline-text-4" id="text-1-8-9">
<p>
Scheme standards categorize errors and "bugs" to help programmers write portable, robust code.
</p>
<ol class="org-ol">
<li>Implementation restrictions
<ul class="org-ul">
<li>Definition: These occur when a valid Scheme program fails to run on a particular implementation due to resource limits (e.g., available memory).</li>
<li>Example: Creating extremely long lists might exhaust virtual memory on some implementations.</li>
</ul></li>
<li>Unspecified behavior
<ul class="org-ul">
<li>Definition: The Scheme standard does not dictate how certain operations behave, leaving room for variation between implementations.</li>
<li>Example: equality predicates - Scheme offers several predicates for equalty, each with its own purpose
<ol class="org-ol">
<li><code>eq?</code>: Typically checks whether two expressions reference the same object (pointer or identity comparison). \(O(1)\) cost but might no compare contents.</li>
<li><code>eqv?</code>: Like <code>eq?</code>, but also compares numbers and characters by value.</li>
<li><code>equal?</code>: Does a deep, recursive comparison of data structures (lists, trees). <b>Pitfall</b>: with cyclic structures, can lead to infinite loops.</li>
<li>Numeric comparison: checks whether numbers are numerically equal. Some implementations may choose to re-use objects for small integers, while others create new objects.</li>
</ol></li>
<li>Implications:
<ul class="org-ul">
<li>Programs should be robust regardless of the specific behavior of these operations.</li>
<li>Relying on a particular behavior (e.g., expecting eq? to always return true for numbers) can harm portability.</li>
</ul></li>
</ul></li>
<li>Undefined behavior
<ul class="org-ul">
<li>Definition: when a program violates the Schem standard, the behavior is undefined; the implementation can do anything</li>
<li>Example: taking <code>car</code> of something that is not a pair.</li>
<li>Comparison: similar to undefined behavior in C/C++ (e.g. array out-of-bounds, nullptr dereferencing)</li>
</ul></li>
<li>Error signaling
<ul class="org-ul">
<li>Some operations (opening a file) must signal an error if they fail</li>
<li>Can display error message, throw exception, exit</li>
<li>Implementation differences: some Scheme systems may signal erros for even "cheap" operations (e.g. taking <code>car</code> of non-pair)</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-org1647732" class="outline-4">
<h4 id="org1647732"><span class="section-number-4">1.8.10.</span> Continuations</h4>
<div class="outline-text-4" id="text-1-8-10">
<p>
Continuations capture the “rest of the computation” and are a distinguishing feature of Scheme. At a high level, a continuation is a representation of “what to do next” in the program. It can be thought of as the program’s “bucket list” of pending operations.
</p>

<p>
At the <b>hardware</b>, we can think of it as the following:
</p>
<ol class="org-ol">
<li>We have two essential pointers:
<ul class="org-ul">
<li>Instruction pointer (IP):
<ul class="org-ul">
<li>Points to the next machine instruction or bytecode to execute.</li>
<li>On hardware (e.g., x86-64), this is represented by the RIP register.</li>
</ul></li>
<li>Environment pointer (EP):
<ul class="org-ul">
<li>Points to the current activation record (stack frame) containing local variables, temporaries, and the return address.</li>
<li>Often implemented using the base pointer register (e.g., RBP) or sometimes the stack pointer (RSP).</li>
</ul></li>
</ul></li>
<li>Stack frame diagram is the following &#x2013; image a call stack in a C program:</li>
</ol>
<pre class="example">
┌─────────────────────────────┐
│ Main Function Frame         │  &lt;-- Base (no caller)
│ (global variables, etc.)    │
└─────────────────────────────┘
        ↑ Return Address stored here
┌─────────────────────────────┐
│ Caller’s Frame              │  &lt;-- Contains saved RBP, local variables
│ (Return address, EP, etc.)  │
└─────────────────────────────┘
        ↑ Return Address stored here
┌─────────────────────────────┐
│ Current Function’s Frame    │  &lt;-- Contains local variables, etc.
│ (Local variables, IP, EP)   │
└─────────────────────────────┘
</pre>
<ul class="org-ul">
<li>IP points into machine code within the current frame.</li>
<li>EP points to caller's frame, providing context for when the current function returns.</li>
</ul>
<p>
<b>Continuation as data</b>:
</p>
<ul class="org-ul">
<li>Continuation representation:
<ul class="org-ul">
<li>Abstractly, a continuation can be seen as a pair <code>(IP, EP)</code></li>
<li>This pair represents the plan for what to execute next.</li>
</ul></li>
<li>Scheme's uniqueness:
<ul class="org-ul">
<li>Unlike many languages, Scheme allows you to capture this continuation (using <code>call/cc</code>) and manipulate it as a first-class object.</li>
</ul></li>
</ul>
<p>
As a diagram:
</p>
<pre class="example">
Current Continuation:
┌─────────────────────────┐
│ (IP, EP) of Current     │
│ Function’s Frame        │
├─────────────────────────┤
│ (IP, EP) of Caller’s    │
│ Frame                   │
├─────────────────────────┤
│ (IP, EP) of Main Frame  │
└─────────────────────────┘
</pre>
<ul class="org-ul">
<li>When a function returns, its frame is popped off, and the continuation resumes from the saved <code>(IP, EP)</code> of the caller's frame.</li>
</ul>
<p>
<b>Importance of continuations</b>
</p>
<ul class="org-ul">
<li>Control flow manipulations: Continuations allow the implementation of non-standard control structures such as early exits, coroutines, and backtracking.</li>
<li>Few languages expose continuations as directly as Scheme. This is often cited as the “essence of Scheme” (and sometimes as its “biggest mistake” because of the complexity it introduces).</li>
</ul>
<p>
Example: using <code>call/cc</code>
</p>
<div class="org-src-container">
<pre class="src src-scheme">(call/cc
  (lambda (cont)
    (display "Before continuation")
    (cont "Jumped out!")  ; Immediately returns this value and abandons the rest.
    (display "This will never be printed")))
</pre>
</div>
<ul class="org-ul">
<li><code>call/cc</code> (call with current continuation) passes the current continuation (the rest of the computation) as an argument (<code>cont</code>) to the lambda</li>
<li>Calling <code>cont</code> immediately exits the lambda and returns the given value, skipping subsequent code.</li>
</ul>
<p>
<b>Intuition and examples</b>
A continuation represents "the rest of the computation"—that is, everything your program still has to do at a given point in time. Imagine you’re reading a to-do list; your current continuation is the list of steps you plan to complete.
</p>

<p>
In programming, when a function is executing, the continuation is the “future work” (like what to do after the function returns). In many languages, this is hidden (it’s managed by the call stack), but Scheme makes it possible to capture that continuation as a first-class object.
</p>

<p>
When a function is called, the computer saves the place to return to (its return address) along with the local environment (variables, etc.). Conceptually, a continuation is like capturing this entire “stack” or the “rest of the program” so that you can later resume execution from that point.
</p>

<p>
<code>call/cc</code>: call/cc stands for “call with current continuation.” It is a Scheme primitive that captures the current continuation and passes it as an argument to a function (usually a lambda). This allows you to control the flow of your program in ways that are impossible in most languages.
</p>
<ul class="org-ul">
<li>Syntax: <code>(call/cc (lambda (k) ... ))</code>. Here, <code>k</code> is the current continuation &#x2013; a function representing "waht to do next."</li>
<li>Behavior:
<ol class="org-ol">
<li><code>call/cc</code> calls the lambda, passing the current continuation as <code>k</code>.</li>
<li>Within the lambda, you can
<ul class="org-ul">
<li>Call <code>k</code> with a value. If you do the entire <code>call/cc</code> expression immediately returns that value, skipping any code after the call.</li>
<li>Not call <code>k</code> at all, in which case the lambda’s normal result is returned.</li>
</ul></li>
</ol></li>
</ul>
<p>
<b>Examples</b>:
What if you want to exit-early from a computation? With <code>call/cc</code> you can "bail out" from a deeply nested function call.
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (find-first lst predicate)
  (call/cc
    (lambda (exit)
      (for-each
        (lambda (x)
          (when (predicate x)
            (exit x)))  ; call exit with x when predicate is true
        lst)
      #f)))  ; if no element satisfies the predicate, return false

;; Example usage:
(find-first '(1 3 5 8 9) even?)  ; Returns 8 because it's the first even number.
</pre>
</div>
<ul class="org-ul">
<li>When an element satisfies <code>predicate</code>, we bail out (call <code>(exit x)</code>, which immediately ends the function and returns <code>x</code>.)</li>
</ul>
<p>
What about non-local exits? (escape), i.e. nested computation?
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (compute-sum lst)
  (call/cc
    (lambda (exit)
      (let loop ((lst lst) (acc 0))
        (if (null? lst)
            acc
            (if (&gt; acc 100)
                (exit acc)  ; if accumulator &gt; 100, exit early
                (loop (cdr lst) (+ acc (car lst)))))))))

;; Example usage:
(compute-sum '(10 20 30 40 50))  ; May exit early if the sum exceeds 100.
</pre>
</div>
<p>
We can also simulate backtracking: <code>try-all</code> goes through a list trying to find a "successful" element.
</p>
<pre class="example">
(define (try-all lst)
  (call/cc
    (lambda (fail)
      (let loop ((lst lst))
        (if (null? lst)
            (fail 'no-success)  ; if list is exhausted, invoke fail continuation
            (if (successful? (car lst))
                (car lst)         ; return the first successful element
                (loop (cdr lst))))))))

;; Assume 'successful?' is a predicate that checks if an element is “successful.”
</pre>
<p>
<b>Mechanics of call/cc</b>
Recall that call with current continuation, definitionally:
</p>
<ul class="org-ul">
<li>A primitive built into scheme that lets you capture the current continuation &#x2013; i.e., "what to do next"</li>
<li>It takes one argument: a procdure (function) <code>P</code>.</li>
<li>It creates a continuation (we call <code>K</code>) representing the rest of the computation from the point of invocation.</li>
<li>Then, <code>call/cc</code> calls the procdure <code>P</code>, passing <code>K</code> as its argument.</li>
<li>Finally, it returns whatever <code>P</code> returns (unless the continuation <code>K</code> is later invoked)</li>
</ul>
<p>
<i>Mechanically</i>, how do we create a continuation?
</p>
<ul class="org-ul">
<li>Internall, the continuiation is represented by a small data structure containing two pointers:
<ol class="org-ol">
<li>Instruction pointer (IP): points to the next instruction</li>
<li>Environment pointer (EP): points to current activation record (or stack frame), containing local variables and the returna ddress</li>
</ol></li>
</ul>
<p>
The continuation <code>K</code> is essentially a "snapshot" of these two pointers. It is very cheap to create (comparable to a cons cell in cost), as it only stores two words.
</p>

<p>
Even more <i>mechanically</i>, when we use a continuation, we have
</p>
<ul class="org-ul">
<li><code>K</code> is a first class object (a function) that you can call with a single argument <code>V</code></li>
<li>When you call <code>K</code> with value <code>V</code>, the interpreter "jumps" back to the point where <code>call/cc</code> was invoked, and the entire <code>call/cc</code> expression then returns <code>V</code>.</li>
<li>In low-level terms (x86-64 analagy)
<ol class="org-ol">
<li>The return value register <code>rax</code> is set to <code>V</code></li>
<li>The environment pointer <code>rbp</code> is restored from <code>K</code>'s stored EP</li>
<li>The instruction pointer <code>rip</code> is set to <code>K</code>'s stored IP, effectively performing a jump.</li>
</ol></li>
</ul>
<p>
<b>More examples</b>
Imagine you have a long list of integers that you wish to multiply together. However, if any integer is 0, you want to immediately return 0 without performing all the multiplications. Continuations let you do this <b>non-local exit</b> efficiently.
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (prod lst)
  ;; Use call/cc to capture the continuation for an early exit.
  (call/cc
    (lambda (break)
      ;; Define a helper using named let for recursion.
      (let loop ((lst lst) (acc 1))
        (cond ((null? lst) acc)       ; If list is empty, return accumulator.
              ((zero? (car lst))
               (break 0))             ; If head is 0, immediately exit with 0.
              (else (loop (cdr lst) (* acc (car lst)))))))))

;; Example usage:
(prod '(2 3 4))          ; Returns 24.
(prod '(2 3 0 4 5))      ; Returns 0 immediately, without multiplying 4 and 5.
</pre>
</div>
<p>
Another function &#x2013; Imagine a function that searches through a tree. If a certain condition is met, you want to immediately exit and return a result.
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (find-in-tree tree predicate)
  (call/cc
    (lambda (exit)
      (define (search node)
        (if (predicate node)
            (exit node)  ; immediately exit if predicate is satisfied
            (if (pair? node)
                (or (search (car node)) (search (cdr node)))
                #f)))
      (search tree))))

;; Example usage:
(find-in-tree '((1 2) (3 (4 5))) even?) ; Returns first even number encountered.
</pre>
</div>
<p>
We can also simulate a simple try-catch
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (safe-divide a b)
  (call/cc
    (lambda (handle-error)
      (if (zero? b)
          (handle-error 'division-by-zero)
          (/ a b)))))

;; Example usage:
(safe-divide 10 2)         ; Returns 5.
(safe-divide 10 0)         ; Returns 'division-by-zero.
</pre>
</div>
<p>
<b>Example: Green threads</b>
We have mainly discussing using continuations to perform non-local exits (try-catch). Now we extend this idea: continuation in Scheme can do more than
just "jump out" of a function &#x2013; they can also let you jump <i>back into</i> a function. 
</p>

<p>
We will use continuations to implement <b>green threads</b> &#x2013; lightweight, user-level thjreads that cooperate on a single CPU.
</p>
<ul class="org-ul">
<li>They provide concurrency without the complications of true multi-threading (no parallel execution, simpler synchronization)</li>
<li>Since only one thread runs at a time, many race conditions are avoided</li>
<li>No need for OS-level threads!</li>
</ul>
<p>
We can build green threads in Scheme in the following way
</p>
<div class="org-src-container">
<pre class="src src-scheme">;; Global variable: a list to hold all green threads (thunks) waiting to run.
(define green-thread-list '())
;; Thread Constructor: gt-cons
;; Purpose:
;;   - Create a new green thread from a thunk (a no-argument function)
;;   - Append the new thread to the global green-thread-list.
(define (gt-cons thunk)
  ;; Append the new thread (as a singleton list) to the end of green-thread-list.
  (set! green-thread-list (append green-thread-list (list thunk))))
;; Scheduler Function: start
;; Purpose:
;;   - Remove the first thread from green-thread-list and execute it.
;;   - This function acts as the scheduler to transfer control to the next thread.
(define (start)
  (if (null? green-thread-list)
      (error "No green threads to run!")
      (let ((next-thread (car green-thread-list)))
        ;; Remove the thread from the head of the list.
        (set! green-thread-list (cdr green-thread-list))
        ;; Execute the next thread (thunk).
        (next-thread))))
;; Yield Function: yield
;; Purpose:
;;   - Allow a running thread to suspend its execution,
;;     capture its current continuation, and add itself to the end of
;;     the green-thread-list.
;;   - Then, immediately transfer control to the next thread.
;;
;; How it works:
;;   - call/cc is used to capture the current continuation as a function k.
;;   - k represents the remainder of the current thread's computation.
;;   - We append k to green-thread-list and then call start to schedule the next thread.
(define (yield)
  (call/cc
    (lambda (k)
      ;; Append the captured continuation k (as a thunk) to the end of green-thread-list.
      (set! green-thread-list (append green-thread-list (list k)))
      ;; Transfer control to the next thread.
      (start))))

(define (print-h)
  (let loop ()
    (display "H")
    ;; Yield control after printing H.
    (yield)
    ;; Resume here after yield and loop forever.
    (loop)))
(define (print-i)
  (let loop ()
    (display "I")
    (newline)
    ;; Yield control after printing I and a newline.
    (yield)
    ;; Resume here after yield and loop forever.
    (loop)))


;; Main: Create Threads and Start the Scheduler
;; Clear the global thread list
(set! green-thread-list '())

;; Add our green threads to the thread list.
(gt-cons print-h)   ;; Add the thread that prints "H".
(gt-cons print-i)   ;; Add the thread that prints "I" followed by a newline.

;; Start the scheduler to begin green thread execution.
(start)
</pre>
</div>
<p>
The issue with this code is that we will have the threads run forever - the <code>yield</code> function acts like a true <code>yield</code> function for a true multithreaded application.
</p>

<p>
Can we do things without using <code>call/cc</code>? Yes:
</p>

<p>
<b>Continuation Passing Style (CPS)</b>
Programming style in which every function takes an extra argument (a continuation) that represents "what to do next" after that function finishes. Instead of returning a value directly, the function passes its result to the continuatiion function.
</p>
<ul class="org-ul">
<li>Makes control flow explicit</li>
<li>Allows you to simulate non-local exits, backtracking, etc without relying on built-in primitives like <code>call/cc</code></li>
<li>Can be implemented in any language with first-class functions (e.g. Scheme, Python, OCaml)</li>
</ul>
<p>
The key idea is that every function is rewritten so that it never "returns" in the usual sense. Instead, when a result is computed, it is pased to the continuation (a function) that was provided as an extra argument.
</p>

<p>
Imagine you’re following a recipe. Instead of finishing a step and then “returning” to continue, someone (a friend) stands by with a note saying, “When you finish this step, give me the result so I can tell you what to do next.” In CPS, that note is the continuation. For example, the normal function would be
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (add x y)
  (+ x y))
</pre>
</div>
<p>
When we call <code>(add 2 3)</code> it returns <code>5</code>. Now the CPS version looks like
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (add-cps x y cont)
  (cont (+ x y)))
</pre>
</div>
<p>
Instead of returning a value, it passes 5 to the continuation cont. For example, if cont is a function that prints its argument, (add-cps 2 3 display) would print 5.
</p>

<p>
We can no re-write the <code>prod</code> function using CPS. The traditional function looks like
</p>
<div class="org-src-container">
<pre class="src src-scheme">(define (prod lst)
  (if (null? lst)
      1
      (if (zero? (car lst))
          0
          (* (car lst) (prod (cdr lst))))))
</pre>
</div>
<p>
In the CPS version We add an extra parameter k (the continuation). Instead of returning a value, the function passes its result to k. We have the following changes:
</p>
<ol class="org-ol">
<li>Base case: If the list is empty, then the product is 1. Instead of returning 1, we call the continuation k with 1.</li>
<li>Early exit case: If the head of the list is 0, we immediately pass 0 to the continuation.</li>
<li>Recursive case: Otherwise, we recursively compute the product of the tail of the list. However, we must modify the continuation to multiply the result by the head.</li>
</ol>
<div class="org-src-container">
<pre class="src src-scheme">(define (prod-cps lst k)
  (if (null? lst)
      ;; Base case: if list is empty, product is 1. Pass 1 to continuation.
      (k 1)
      (if (zero? (car lst))
          ;; Early exit: if head is 0, product is 0. Immediately pass 0 to k.
          (k 0)
          ;; Recursive case:
          ;; Compute the product of the tail, but update the continuation so that
          ;; once the tail's product is computed, multiply it by (car lst).
          (prod-cps (cdr lst)
                    (lambda (tail-prod)
                      ;; Multiply the head by the product of the tail, and pass the result to k.
                      (k (* (car lst) tail-prod)))))))

;; A helper to call our CPS function with the identity continuation.
(define (prod lst)
  (prod-cps lst (lambda (x) x)))

;; Example usage:
;; (prod '(2 3 4)) returns 24.
;; (prod '(2 3 0 4 5)) returns 0 immediately when 0 is encountered.
(display (prod '(2 3 4)))   ; Should display 24.
(newline)
(display (prod '(2 3 0 4 5))) ; Should display 0.
(newline)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf1da629" class="outline-4">
<h4 id="orgf1da629"><span class="section-number-4">1.8.11.</span> Storage Management (from Albert's Notes)</h4>
<div class="outline-text-4" id="text-1-8-11">
<p>
The storage hierarchy starts with small/fast and ends slow/big: registers -&gt; L1/2/3 cache -&gt; RAM -&gt;
flash -&gt; disk. On a higher level, the contents of variables get put into the registers or into RAM: the
compiler (or us, potentially) decide which one; the system decides whether to cache something or
not. On the other hand, we typically interface with persistent storage through I/O commands, i.e.
explicit actions.
</p>

<p>
We also need to store machine code somewhere (for our program, for library code, and for the code
running an interpreter if we’re using an interpreted language). We need a stack, to hold the
instruction pointer and return addresses, as well as separate stacks for separate threads. Finally, we
need somewhere to store an I/O buffer as well as memory to contain temporary values.
</p>

<p>
On a higher level, we need first static storage. The simplest such approach is to allocate everything
statically in RAM (a la 1950s FORTRAN). This is simple and lets you optimize well but is less secure
and doesn’t let you use object oriented/recursive patterns.
</p>

<p>
We also need a stack to better support calling/returning from functions. The stack is simple, is fast to
allocate/free, but has obvious limitations in terms of what order we can free storage.
</p>

<p>
An extension to the stack is the idea of an “activation record”, i.e. the storage allocated when you
enter a function/freed when you return from it. In C, for instance, the size of this activation record is
known per function and thus can be statically allocated. In C99, on the other hand, we can
dynamically have different activation record sizes for the same function, making stack allocations
more complex. This is more flexible but more complex, leads to security issues, and can lead to stack
overflows (which typically are not checked for performance reasons in lower-level languages).
</p>

<p>
A parallel idea is the idea of nested functions where internal functions can refer to the variables of
the function they were declared in. This is difficult to implement. For instance, let’s say a function’s
activation record contains one slot per local variable, one slot per temporary, and one slot for the
return address. It then becomes difficult to figure out how to access the memory of the nonlocal
variable using static analysis techniques: instead of just maintaining a dynamic chain of pointers
referring to callers, we also need to maintain a chain of pointers referring to the definers of each
function, called the static chain. We then need to tell a function, when it’s called, where its definer’s
stack frame is.
</p>

<p>
This idea of having some machine code be linked to the context in which it is called is known as a
closure. This is very important in high level programming languages.
</p>

<p>
Finally, we need a heap. This removes the restriction of LIFO allocation but is generally more
expensive/complicated to manage than the stack. For instance, we need to manage a free list to keep
track of whether we still have extra heap space available or not. However, we need to deal with
issues like making sure we have big blocks of memory available (fragmentation), coalescing free
chunks, and even potentially mechanisms like quick lists for commonly-requested sizes.
</p>

<p>
Note that if we forget to free/free memory too early, this can lead to significant performance/
reliability issues. The standard fix for these issues is to use a garbage collector, which frees memory
that is no longer accessible by the program. A standard garbage collection algorithm is the mark and
sweep algorithm: all reachable objects are marked recursively; all unmarked objects are freed. The
compiler thus needs to tell the garbage collector where the program variables are, which it can do
because it allocated storage for them.
</p>
</div>
</div>
</div>
<div id="outline-container-org4ab1754" class="outline-3">
<h3 id="org4ab1754"><span class="section-number-3">1.9.</span> Storage Management</h3>
<div class="outline-text-3" id="text-1-9">
<p>
<b>A. Why Memory Management Matters</b>
</p>
<ul class="org-ul">
<li><b><b>Purpose:</b></b>  
Memory management deals with storing all the data a program uses:
<ul class="org-ul">
<li>Variables: Local, global, and temporary.</li>
<li>Return addresses and environment pointers: Essential for tracking function calls (the call stack).</li>
<li>Program instructions and constants: Stored in memory (the text segment).</li>
</ul></li>

<li><b><b>Challenge:</b></b>  
Modern programs have many more variables than available CPU registers. Large objects (like arrays) cannot reside solely in registers and must be stored in RAM.</li>
</ul>

<p>
<b>B. Memory Layout Overview</b>
</p>

<p>
A typical program’s memory is partitioned into several regions:
</p>

<ul class="org-ul">
<li><b><b>Text Segment (Code):</b></b>  
Contains machine instructions and read‑only constants.  
<b>Advantages:</b> Can be marked read‑only to prevent accidental writes.</li>

<li><b><b>Initialized Data Segment (Data):</b></b>  
Contains global variables that are pre‑initialized.  
Loaded from the executable; its initial values are fixed.</li>

<li><b><b>BSS Segment (Block Started by Symbol):</b></b>  
Contains global and static variables that are zero‑initalized.  
Not stored in the executable; simply zero‑filled at runtime.</li>

<li><b><b>Heap:</b></b>  
Used for dynamic memory allocation (via functions like malloc in C).  
Can grow and shrink during program execution.  
<b>Flexible:</b> You can allocate objects whose size isn’t known at compile time.  
<b>Downsides:</b> Slower to allocate/free and may require a memory manager.</li>

<li><b><b>Stack:</b></b>  
Used for function call management (activation records).  
Stores local variables, return addresses, and saved environment pointers.  
<b>Fast allocation/deallocation:</b> (by adjusting the stack pointer).  
<b>LIFO order:</b> Limits freeing out‑of‑order objects.</li>
</ul>

<pre class="example" id="orge2e03a4">
+----------------------+
|      Text Segment    |  ← Program instructions, read‑only data
+----------------------+
|  Initialized Data    |  ← Global variables (pre‑set)
+----------------------+
|         BSS          |  ← Zero‑initialized globals/statics
+----------------------+
|        Heap          |  ← Dynamic memory (grows/shrinks)
+----------------------+
|        Stack         |  ← Activation records (function calls)
+----------------------+
</pre>

<p>
<b>C. Static vs. Dynamic Memory Allocation</b>
</p>

<ol class="org-ol">
<li><b><b>Static Memory Allocation (e.g., Fortran 1958)</b></b>

<ul class="org-ul">
<li><b><b>Description:</b></b>  
All memory is allocated at compile time.</li>

<li><b><b>Advantages:</b></b>
<ul class="org-ul">
<li><b>Simplicity:</b> No runtime allocation or deallocation.</li>
<li><b>No memory exhaustion:</b> The program has a fixed amount of memory; you won’t run into “out‑of‑memory” errors.</li>
<li><b>Predictability:</b> The size and location of every variable are known at compile time.</li>
</ul></li>

<li><b><b>Disadvantages:</b></b>
<ul class="org-ul">
<li><b>Inflexibility:</b> If you underestimate the needed size (e.g., array size), you must recompile.</li>
<li><b>Limited recursion:</b> Fixed‑size activation records make recursion difficult or impossible.</li>
<li><b>Wasteful:</b> May reserve more memory than necessary.</li>
</ul></li>
</ul></li>

<li><b><b>Dynamic Memory Allocation (e.g., C from 1975 Onward)</b></b>

<ul class="org-ul">
<li><b><b>Description:</b></b>  
Memory is allocated at runtime.</li>

<li><b><b>Components:</b></b>
<ul class="org-ul">
<li><b><b>Stack Allocation:</b></b>
<ul class="org-ul">
<li>Used for function calls (activation records).</li>
<li>Fast allocation/deallocation via a stack pointer.</li>
<li><b>Limitation:</b> LIFO structure; you cannot deallocate out‑of‑order.</li>
</ul></li>
<li><b><b>Heap Allocation:</b></b>
<ul class="org-ul">
<li>Used for objects created during runtime (via malloc/free in C).</li>
<li>More flexible than the stack.</li>
<li><b>Slower:</b> Memory management involves more complex operations.</li>
</ul></li>
</ul></li>

<li><b><b>Advantages:</b></b>
<ul class="org-ul">
<li><b>Flexibility:</b> You can allocate variable‑sized objects at runtime (e.g., dynamic arrays).</li>
<li><b>Recursion is enabled:</b> Stack frames are allocated as functions are called.</li>
</ul></li>

<li><b><b>Disadvantages:</b></b>
<ul class="org-ul">
<li><b>Overhead:</b> Managing the heap is more complex and slower than stack allocation.</li>
<li><b>Fragmentation:</b> Dynamic allocation can lead to memory fragmentation.</li>
<li><b>Potential runtime errors:</b> Memory exhaustion, if too much is allocated.</li>
</ul></li>
</ul></li>

<li><b><b>Historical Example: Algol 60 vs. Traditional C</b></b>

<ul class="org-ul">
<li><b><b>Algol 60:</b></b>
<ul class="org-ul">
<li>Allowed local arrays to have sizes determined at runtime.</li>
<li>The activation record (stack frame) size could vary.</li>
</ul></li>
<li><b><b>Traditional C:</b></b>
<ul class="org-ul">
<li>Local arrays must have sizes determined by compile‑time constant expressions.</li>
<li>Activation records are fixed‑size, allowing the compiler to generate efficient code.</li>
<li><b>(Note: Modern C (C99) introduced variable‑length arrays, but C++ still requires fixed‑size local arrays.)</b></li>
</ul></li>
</ul></li>
</ol>
</div>
<div id="outline-container-org4f22a87" class="outline-4">
<h4 id="org4f22a87"><span class="section-number-4">1.9.1.</span> Memory management</h4>
<div class="outline-text-4" id="text-1-9-1">
<p>
Beyond activation reocrds (or frames) for the currently executing procedure, a language must also manage other objects such as I/O buffers and dynamically allocated objects (the heap).
</p>
<ul class="org-ul">
<li>Activation records hold all the necessary function for a function's execution, including local variables, return addresses and saved environment pointers.</li>
<li>Typically, these records are stored on the stack (LIFO), which is efficient for allocation and deallocation.</li>
<li>However, when a function returns but its inner function (e.g. a curried function) is still needed, the activation record must survive even after the original function has returned. In such case, the activation record is <i>promoted</i> to the heap.</li>
</ul>
<p>
In Scheme, recall that functions are not just pointers to code, they are "fat" function objects that consist of two components:
</p>
<ol class="org-ol">
<li>An instruction pointer (IP), which indicates where the machine code for the function resides.</li>
<li>An environment pointer (EP), which points to the activaetion record of the function that defined the current function.</li>
<li>When you curry a function (i.e. partially apply arguments), the resulting function object carries with it the necessary environment so that it can later refer to variables from the defining context.</li>
<li>This means that a curried function is effectively a pair of pointers (IP and EP). The EP must remain accessible even after the outer function has completed, so it is allocated on the heap rather than on the stack.</li>
</ol>
</div>
</div>
<div id="outline-container-org47688fc" class="outline-4">
<h4 id="org47688fc"><span class="section-number-4">1.9.2.</span> Dynamic vs. Static Chain</h4>
<div class="outline-text-4" id="text-1-9-2">
<ul class="org-ul">
<li>The <b>dynamic</b> chain records the sequence of calls that led to the current point of execution (i.e. the call stack). It is useful for generating backtraces and understanding runtime context.
<ul class="org-ul">
<li>"Who called us"; the only thing we can do with the dynamic chain is return and let the caller figure out what to do.</li>
</ul></li>
<li>The <b>static</b> chain reflects the lexical structure of the program. When a function needs to access a non-local variable, it follows the static chain to locate the variable's binding in the environment of the function that defined it.
<ul class="org-ul">
<li>"Who defined us"; can figure out the static chain at compile-time. Maximum nesting level in the original source code for Scheme is the max length of the static chain.</li>
<li>When a function follows its static chain, it goes from a function to its definer. It knows statically, at compile time, what the relevant activation record looks like, knows where the local variables are and can go look them up.</li>
</ul></li>
</ul>
<p>
Sometimes the two will be the same, i.e. if the function calls us and the function defines us. With currying and higher-order functions, the static chain is often shorter because it represents only the nesting structure of definitons, not the entire runtime call history.
</p>
<ul class="org-ul">
<li>If some function <code>F</code> returns, but its activation record is still around (i.e. due to the EP / dynamic chain) need not be stacked. So in Scheme, in general, your activation records are not on the stack (not LIFO). We have to "keep <code>F</code> around" so long as anybody has access to the function <code>F</code> returns. If we're not returning the function, the compiler can optimize it into a stack.</li>
<li>This makes it possible to design languages without <code>new</code> or <code>malloc()</code>. Instead, the way you allocate an object is similar to above: you call <code>F</code>, <code>F</code> returns a function, that function ties down the activation record, and that ties down your object.</li>
</ul>
</div>
</div>

<div id="outline-container-orgc2ffea4" class="outline-4">
<h4 id="orgc2ffea4"><span class="section-number-4">1.9.3.</span> Heap Management</h4>
<div class="outline-text-4" id="text-1-9-3">
<ul class="org-ul">
<li>The heap is used for dynamic memory allocation. Unlike the stack, objects on the heap can be freed in any order.</li>
<li>In a managed language like Scheme, the heap manager (or garbage collector) must track which objects are still reachable (the “roots”) and reclaim memory for those that are not.</li>
<li>The heap manager's gaol is to keep track of all the stuff that's in the heap, and make sure that if you can access an object in the heap, we keep it around, and if you cannot access that object, then we can reuse that piece of storage.</li>
</ul>
<p>
<b>Undefined vs. unspecified behavior</b>:
</p>
<ul class="org-ul">
<li><b>Unspecified Behavior</b>: The language standard does not specify the exact outcome (e.g., the exact result of pointer comparisons for small integers).</li>
<li><b>Undefined Behavior</b>: When the program violates the language rules (such as using a dangling pointer), the behavior is completely unpredictable; the program may crash, produce incorrect results, or seem to work on one run and not another.</li>
</ul>
<p>
Trade-offs:
</p>
<ul class="org-ul">
<li>Static allocation (as in early Fortran) is simple and efficient but inflexible.</li>
<li>Dynamic allocation (as in C) offers flexibility and supports recursion and variable‑sized objects but introduces overhead, potential fragmentation, and runtime errors.</li>
<li>Modern languages like Scheme use garbage collection to automatically manage heap memory, reducing the risk of errors like dangling pointers but at the cost of some performance overhead.</li>
</ul>
<p>
<b>Roots</b>
</p>
<ul class="org-ul">
<li>Roots are entry points from which the program can access objects in the heap (for example, variables in registers, on the stack, or global variables).</li>
<li>The garbage collector uses these roots to trace which objects are accessible; any object not reachable from any root is eligible for reclamation.</li>
</ul>
<p>
How can we manage roots?
</p>
<ol class="org-ol">
<li>We can explicitly inform thje heap manager
<ul class="org-ul">
<li>The progrma might call specific routines to push or pop roots when objects are created or destroyed.</li>
<li>However, this is error prone and unpopular.</li>
</ul></li>
<li>Compiler-recorded roots
<ul class="org-ul">
<li>When compling, the compiler can record the locations of global variables and local variables in activation reocrds as roots, sotring this information in read-only tables that the heap manager consults.</li>
</ul></li>
<li>Manual memory management
<ul class="org-ul">
<li>In languages like <code>C</code>, you have explicit allocation (<code>mallow</code>, <code>new</code>) and explicit deallocation (via <code>free</code> and <code>delete</code>)</li>
<li>The heap manager does not automatically know which objects are still needed and instead relies on the programmar.</li>
<li>This can lead to issues i.e. dangling pointers if an object is freed while still in use.</li>
</ul></li>
</ol>
<ul class="org-ul">
<li>All these options assume we have a <b>managed heap</b>: i.e. you need a garbage collector, which deletes the objects. It deduces, from the roots and object layout, which objects are unreachable from the program (and thus, <code>free</code> them)</li>
</ul>
<p>
How can we keep track of roots?
</p>

<p>
How can we keep track of free space? (while being fast)
</p>
<ul class="org-ul">
<li>Keep a linked list, where each link points to another, and the payload is a pointer to the free area and the size of the free area</li>
</ul>
<pre class="example">
+-----------+      +-----------+      +-----------+
| Free Node | ---&gt; | Free Node | ---&gt; | Free Node | ---&gt; NULL
+-----------+      +-----------+      +-----------+
    |                  |                  |
    V                  V                  V
[ Pointer, Size ]  [ Pointer, Size ]  [ Pointer, Size ]
</pre>
<ul class="org-ul">
<li>Each node represents a free memor block (e.g. 100 bytes, 50 bytes, 300 bytes)</li>
</ul>
<p>
Problem: keeping the free list in a separate memory area (a "meta" heap) &#x2013; but this leads to a recursive problem, as you'd then need a free list for the meta heap and so on.
</p>

<p>
Instead, stored the free list information wuthin the free blocks themselves. Reserve the first few words (e.g. 3 words) of each free block to hold the linked-list pointers and size information. You avoid extra memory management overhead since the metadata uses the free space itself.
</p>
<pre class="example">
Heap Block (Free):
+-------------------+
| Metadata: [ptr, sz] |  &lt;-- 3 words reserved for free list info
+-------------------+
|  Unused (free)    |
+-------------------+
</pre>
<p>
however, we still have an efficiency problem. We want \(O(1)\) <code>malloc()</code>. In the ideal case, if we request an allocation (30 bytes) and the first free block is large enough (100 bytes), you just "peel off" the requested bytes:
</p>
<ol class="org-ol">
<li>Subtract the allocated size from the free block</li>
<li>Adjust the pointer to mark the new start of the free block.</li>
</ol>
<p>
However, if you keep allocating and deallocating, free blocks can become very small. Eventually, even if the total free memory is large, you might not find a single contiguous block big enough for a new allocation request. This is known as <b>external fragmentation</b>. Instead, we could try:
</p>
<ul class="org-ul">
<li>Best fit (search for free block that most closely matches requested size): \(O(n)\)</li>
<li>Roving pointer (use a circular free list that remembers the last block used): may keep hitting a large block repeatedly, leaving smaller blocks unused.</li>
</ul>

<p>
What about <code>free</code>? Freeing should also be fast (ideally \(O(1)\)), but when an object is freed, you must:
</p>
<ol class="org-ol">
<li>Determine if the freed block is adjacent to an existing free block.</li>
<li>Coalesce (merge) adjacent free blocks to prevent fragmentation.</li>
</ol>
<p>
With a naive approach, checking adjacency might require scanning the free list, which has \(O(n)\) cost.
</p>

<p>
To speed up the freeing process, use extra words: wen allocating, reserve extra space before and/or after the object for metadata (e.g., size and pointers to neighbors). This extra information speeds up the coalescing process because you can directly check if neighboring blocks are free.
</p>
<pre class="example">
Allocated Block Layout:
+-------------------+-------------------+------------------+
| Metadata (header) |   User Data       | Metadata (footer)|
+-------------------+-------------------+------------------+
</pre>
<p>
with this structure, when freeing, you can: 
</p>
<ul class="org-ul">
<li>check the foot of the previous block</li>
<li>check the header of the next block</li>
</ul>
<p>
if either is free, merge the blocks quickly.
</p>

<p>
<b>Fragmentation issues</b>:
</p>
<ol class="org-ol">
<li>External fragmentation: Occurs when free space is scattered in many small blocks, so a large allocation request might fail even if the total free memory is sufficient. Adjust algorithms to avoid this.</li>
<li>Internal fragmentation: Happens when the allocated block is larger than requested (due to alignment or metadata overhead), leaving some unusable space. i.e. you request 49 bytes but allocator gives you 50 bytes (or even extra for metadata), the extra byte(s) are wasted. Less of a problem than external fragmentation.</li>
</ol>
</div>
</div>
<div id="outline-container-org3be1509" class="outline-4">
<h4 id="org3be1509"><span class="section-number-4">1.9.4.</span> Garbage collection</h4>
<div class="outline-text-4" id="text-1-9-4">
<p>
The basic GC algorithm, used in language like Java which don't have <code>free</code> is <b>mark and sweep</b>.It consists of
</p>
<ol class="org-ol">
<li><b>Mark phase</b>:
<ul class="org-ul">
<li>Start with roots: the garbage collector knows where the “roots” are (global variables, stack variables, etc.) because the compiler or runtime provides this information.</li>
<li>Recursively traverse: starting from the roots, recursively mark every object that is reachable.</li>
</ul></li>
<li><b>Sweep phase</b>:
<ul class="org-ul">
<li>Scan heap: go through every object in the heap</li>
<li>Free unmarked objects: if an object is not marked, add it back to the free list</li>
<li>Clear the mark bits for the next garbage collection cycle (for next cycle of mark-and-sweep).</li>
</ul></li>
</ol>
<p>
Problems:
</p>
<ul class="org-ul">
<li>The mark phase is proportional to the number of reachable objects. In large applications (with lots of objects), this can be very time-consuming</li>
<li>Long pauses during GC can freeze applications (e.g. a robot running Java might "freeze" during mark-and-sweep &#x2013; other computation can't be happening)</li>
</ul>
<p>
We can solve the second issue with <b>real-time garbage collection</b>: to mitigate long pauses, real-time garbage collectors perform a small, bounded amount of work (e.g., a fixed number of instructions) on each allocation. This spreads out the cost of garbage collection. This approach makes allocation a little slower overall compared to traditional mark and sweep, but it avoids the disruptive pauses.
</p>

<p>
Outside of mark-and-sweep, what about languages that have explicit garbage collection with <code>free</code> or <code>delete</code>? Manual memory management can lead to
</p>
<ul class="org-ul">
<li><b>Dangling pointers</b> (using freed memory), or</li>
<li><b>Memory leaks</b> (forgetting to free memory); GCs will get taken care of the next time it marks-and-sweeps</li>
</ul>
<p>
<b>Conservative garbage collection</b>
</p>
<ul class="org-ul">
<li>No explicit free: redefine <code>free</code> or simply ignore it, so that memory is never explicitly freed by the programmer</li>
<li>Automatic GC: the compiler doesn't know where roots are, we and we don't know the layout of the objects in memory. Instead, a garbage collector periodically scans the stack, registers, and static/global data to find anything that looks like a pointer into the heap.
<ul class="org-ul">
<li>If a bit pattern on the stack or in a register resembles a heap address, the conservative GC assumes it is a valid pointer and marks that object as in use.</li>
</ul></li>
</ul>
<p>
Pros:
</p>
<ul class="org-ul">
<li>Prevents dangling pointers, since nothing is explicitly freed</li>
<li>Simplifies memory management in a language not design for GC</li>
</ul>
<p>
Cons:
</p>
<ul class="org-ul">
<li>May falsely mark non-pointer values as pointers (leading to memory leaks) - but this is rare</li>
<li>Relies on the assumption that valid pointers have a recognizable format (e.g. they lie within the heap's address range).</li>
</ul>
<p>
<b>Garbage collection in</b> <code>Python</code>
Historically, Python wasn't as performance critical in terms of garbage collections because the interpreter itself was relatively slow. However, Python uses a very different
strategy than a full mark-and-sweep collector - we have a <b>reference count field</b>: every object in the heap carries an extra word (or field) that holds a reference count -  the number of pointers currently referencing that object.
</p>

<p>
For referencing counting, on assignment (when you assign one variable to another) two things happen:
</p>
<ol class="org-ol">
<li>The reference count for the old object (if overwritten) is decremented.</li>
<li>The reference count for the new object is incremeented.</li>
</ol>
<p>
This means that every assignment carries additional overhead because of these counter updates. However, when an object's reference count drops to zero, it can be freed immediately. No need for a separate mark-and-sweep phase for most objects.
</p>

<p>
<b>Pros &amp; cons</b>:
</p>

<p>
Advantages:
</p>
<ul class="org-ul">
<li>Immediate reclamation: objects are freed as soon as they are no longer needed</li>
<li>Simplicity: conceptually straightforward - each object "knows" how many activate references it has</li>
</ul>
<p>
Disadvantages:
</p>
<ul class="org-ul">
<li>Assignment overhead: every assignment now involves updating two reference counts</li>
<li>Cyclic references: cycles (e.g. A references B and B references A) will never drop to zero, leading to memory leaks
<ul class="org-ul">
<li>To fix this, Python supplements coutning with a periodic mark-and-sweep to clean up cyclic garbage</li>
</ul></li>
</ul>
<p>
Copy behavior:
</p>
<ul class="org-ul">
<li>A shallow copy creates a new object (with its own reference count starting at one) without altering the original object's reference count</li>
<li>A deep copy recursively copies objects, again starting fresh with a new reference count.</li>
</ul>
<p>
<b>Garbage collection</b> <code>Java</code>
In Java, we want to allocate new objects as quickly as creating a new stack frame in <code>C</code> (i.e., a simple pointer arithmetic operation).
Traditional free-list-based allocation can be slow because of fragment and seraching issues. Instead, we adopt the <b>nursery model</b>.
</p>

<p>
In the <b>nursery molde</b>, the heap is partitioned into several <i>generations</i>. The youngest objects are allocated in the <b>nursery</b> and allocations are performed by simple pointer bumping:
</p>
<ul class="org-ul">
<li>Heap pointer (HP): points to next free location</li>
<li>Limit pointer: marks the end of the nursey</li>
</ul>
<p>
The key idea is that in a generation-based collector, when you want to allocate new storage, you always allocate it from the nursery.
</p>

<p>
When the nursery fills, we can either promote it to "adolescents" and allocate a new nursery, or we can do <b>garbage collection</b> once it fills. This is well-suited for the case where objects always point to older objects, and this is true in general. This is especially true for functional programming.
Then, we can garbage collection just the nursery. Because most objects are short-lived, the garbage collector only needs to scan and reclaim the nursery. It uses the fact that most pointers from young objects point to older generations, so scanning can be restricted.
</p>
<ul class="org-ul">
<li>Instead of marking and sweeping in place, live objects in the nursery are copied into a new space.</li>
<li>The free space is consolidated, which improves cache locality and reduces fragmentation.</li>
</ul>
<p>
Java-based garbage collectors also use copying collectors. Suppose we are cleaning the nursery. A copying collector will not garbage collect the objects in place. instead, as we find an object that's in use, we make a
copy of the object down to a new nursery, and update the root to point to the copied object. Once we're done, we have a slab of storage that we can just "free" and give back to the operating system.
A copying collector updates all pointers (roots and internal pointers) as objects are relocated. This complicates the use of finalizers (cleanup methods) because the traditional mark-and-sweep isn’t scanning free objects, so special care (or even separate GC strategies) is needed for objects with finalizers.
</p>

<p>
<b>Advantage 1: Cache</b> - Since a copying collector copies objects into a contiguous regions (with free spaces left at the end), organizing the date in this way so that the related data lives within the same cache line improves the memory access.
</p>

<p>
<b>Advantage 2: Cost</b> - The cost of copying is proportional only to the number of objects in your system; we never look at the free areas. In mark-and-sweep, the cost is proportional to the cost of marking + the cost of sweeping all of the free areas into the free list. Cost is proportional to number of objects in use, not to the number of objects. This is good since in Java a lot of objects are created amd freed quickly.
</p>

<p>
<b>Disadvantage</b>: Java has a <code>finalized</code> method, which, when marked, executes just before the object is reclaimed / cleaned up. However, with this collector, we can't easily support this since we never look at all the object. In order to support this, the Java developers supported two garbage collectors: one regular mark-and-sweep for objects with <code>finalized</code> method, and the generational GC for all others. Eventually, <code>finalize</code> will go away since it is deprecated.
</p>

<p>
To deal with <b>multithreading</b> issues, each thread can have its own nursery to allow rapid, lock-free allocation. This avoids contention on the heap pointer shared among threads.
</p>

<p>
<b>Private free lists in traditional mark-and-sweep</b>
</p>
<ul class="org-ul">
<li>In some systems (e.g. Lisp <code>cons</code> cells), a dedicated free list is maintained for common, small objects.
<ul class="org-ul">
<li>Allocation: check the private free list, if non-empty, pop a cell off it.</li>
<li>Freeing: instead of returning to the general heap free list, the object is pushed back onto the private free list.</li>
</ul></li>
</ul>
<p>
This is a good in C/C++ since the private free list avoids the overhead of scanning a heterogeneous free list. In Java, with a copying collector, the garbage collector might copy even the unused objects from the private free list&#x2013;adding overhead and negating the speed advantage.
</p>
</div>
</div>

<div id="outline-container-orge414761" class="outline-4">
<h4 id="orge414761"><span class="section-number-4">1.9.5.</span> Names, Identifiers, and Substitution</h4>
<div class="outline-text-4" id="text-1-9-5">
<p>
In both natural language and programming, there’s a notion that a name should be interchangeable with what it stands for. For example, if you say: “Sir Walter Scott is the author of Waverley.” then substituting “the author of Waverley” for “Sir Walter Scott” should not change the truth of the statement. However, due to multiple meanings (or “senses”) attached to a name, the substitution sometimes fails.
This is <i>the substitution principle</i>. We want this to hold in programming. Consider the following 
</p>
<div class="org-src-container">
<pre class="src src-C">int I = 27;
return I + 3;
</pre>
</div>
<p>
here, the identifier <code>I</code> is bound to the value 27. By the substitution principle, one might expect that replacing <code>I</code> with 27 everywhere would yield the same meaning. In this simple case, it does. If <code>I</code> is declared with additional properties (like type), then a simple substitution might change meaning. For example:
</p>
<div class="org-src-container">
<pre class="src src-C">long int I = 27;
return sizeof(I);
</pre>
</div>
<p>
On an x86-64 machine, <code>sizeof(long int)</code> might be 8, not 4 (as would be for a 32-bit int).
The identifier <code>I</code> here isn’t just the number 27—it also carries its type information, which influences other operations (like sizeof).
</p>

<p>
An identifier in languages like <code>C</code> or <code>C++</code> can be associated with several pieces of information:
</p>
<ul class="org-ul">
<li>Value: actual runtime value (e.g., <code>27</code>)</li>
<li>Type: determines operations (e.g., <code>int</code>, <code>long int</code>)</li>
<li>Address: where it is stored in memory</li>
<li>Alignment: constraints on its memory address address. (<code>alignof</code>)</li>
</ul>
<p>
The implication is when reasoning about a program, you cannot simply replace the identifier with its value. You must consider its type (affecting, for instance, arithmetic and sizeof operations), its address (if the program takes its address), and possibly its alignment.
</p>

<p>
What we really need is a <b>binding</b>: the association between a name (identifier) and some attribute(s) or "value"—though "value" here is abstract. It might be the runtime value, type, address, alignment, or other properties. For example, the statement <code>int I = 19;</code> establishes a binding for <code>I</code> with
</p>
<ul class="org-ul">
<li>a value (<code>19</code>),</li>
<li>a type (<code>int</code>),</li>
<li>alignment,</li>
<li>and later (during execuction), an address in memory.</li>
</ul>
<p>
A set of bindings is like a python dictionary or namespace, it just amps a set of names to values.
These are either determined explicitly (i.e. <code>int i = 3</code>) or determined implicitly, (i.e. <code>fun i -&gt; i + 1</code>, inferred to be <code>int</code>).
</p>

<p>
<b>Binding time</b> - when does the binding occur?
</p>
<ul class="org-ul">
<li>Runtime bindings: the <b>value</b> of a local variable may change during execution. Its binding time (the moment when the variable is assigned or modified) occurs at runtime.</li>
<li>Compile typing bindings:
<ul class="org-ul">
<li>Type binding: the type of a variable is determined at compile time. E.g., <code>I</code> is known to be an <code>int</code> or (<code>long int</code>) when the program is compiled</li>
<li>Constants: some identifiers (like <code>INT_MAX</code>) are bound to a value as part of the language or platform definition long before any program execution begins. Not quite compile time, more like <i>platform-definition-time</i>. It was decided by the machine (e.g. x86-64 has 32/64 etc). Always the same, once you decide on the platform.</li>
</ul></li>
<li>Link-Time vs. Load-Time bindings:
<ul class="org-ul">
<li>Static variables: a variable declared as <code>static</code> has a single copy persists for the program's lifetime. its address is typically decided at <b>link time</b>. However, with techniques like Address Space Layout Randomization (ASLR), the actual address might not be fixed until the executable is loaded into memory.</li>
<li>Separate compilation is involved when we have too large a program to compile at once, so we compile different modules separately and link them. When that occurs, the compiler does not know all the details of the other modules, so it has to write reasonably generic code, regardless what these bindings happen to be.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org61b3b9e" class="outline-4">
<h4 id="org61b3b9e"><span class="section-number-4">1.9.6.</span> Names and terminology</h4>
<div class="outline-text-4" id="text-1-9-6">
<ol class="org-ol">
<li><b>Declaration</b> (Lightweight)
<ul class="org-ul">
<li>A declaration tells the compiler about the name, type, and (sometimes) certain properties of an entity (such as a function or variable) without giving its full implementation or storage.</li>
<li>Declarations are used in header files or at the start of a module to inform other modules about the interface.</li>
</ul></li>
<li><b>Definition</b> (Heavier weight)
<ul class="org-ul">
<li>A definition not only declares the name and type but also provides the actual implementation or storage.</li>
<li>Definitions are found in source files where the full code is provided.</li>
</ul></li>
</ol>
<p>
When it comes to namespaces, binding times, etc, the other modules will see the declarations, but won't see the definitions. The compiler uses declarations to check that any definitions later in the module (or in separate modules) are consistent with what was declared. For example, if a function is declared to return a double, its definition must return a double. 
</p>

<p>
<b>Filtering</b>
</p>

<p>
In many programming languages, a definition might include extra details that you do not wish to expose outside a module. The declaration is a <b>“filtered”</b> version containing only the interface—enough information for callers to use the function correctly.
</p>
<ul class="org-ul">
<li><b>Abstraction</b>: In software engineering, abstraction means exposing only what is necessary for using a component (e.g., a function or variable) without revealing the internal details.</li>
<li><b>Why filter?</b>
<ul class="org-ul">
<li>Encapsulation: Hides internal workings so that the user of the function doesn’t depend on its internal implementation.</li>
<li>Optimization: Certain implementation details (like whether a function is unsequenced) may help the compiler optimize the code, but they are not essential for a caller to know.</li>
</ul></li>
<li>Filtering in practice: The declaration might leave out attributes that appear only in the definition. For example, the keyword unsequenced (indicating that a function has no side effects and can be optimized aggressively) might be part of the definition but omitted in the declaration for clarity.
<ul class="org-ul">
<li>e.g. declaration without extra attributes would look like <code>double diff_time(time_t start, time_t end);</code></li>
<li>e.g. definition with additional attributes: <code>double diff_time(time_t start, time_t end) __attribute__((unsequenced)) { return difftime(end, start); }</code></li>
<li>When compiling, the compiler will merge the two—using the extra information from the definition to optimize calls to <code>diff_time</code> (for example, allowing common subexpression elimination when it sees multiple calls with the same arguments).</li>
</ul></li>
<li>Practical implications:
<ul class="org-ul">
<li>Interface stability; Other modules only see the declaration and are thus isolated from changes in the definition, which is useful for modular programming.</li>
<li>Compiler optimization; If the compiler knows (through a declaration or merged information) that a function is unsequenced (i.e., it has no side effects), it can simplify repeated calls: <code>if (diff_time(a, b) =</code> diff<sub>time</sub>(a, b))= (the compiler may optimize this to 'true' because it knows the function is pure).</li>
<li>Design consideration; Deciding what details to expose in a declaration is an important design decision. Too much information can leak implementation details, while too little might limit the compiler’s ability to optimize.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org340b6f6" class="outline-4">
<h4 id="org340b6f6"><span class="section-number-4">1.9.7.</span> Namespaces</h4>
<div class="outline-text-4" id="text-1-9-7">
<p>
A <b>namespace</b>  is a collection (or mapping) of names to their bindings. Think of it like a dictionary where each key is an identifier and each value is a bundle of information (value, type, address, etc.). For example for a given block of code
</p>
<pre class="example">
int x = 10;
double y = 3.14;
</pre>
<p>
the namespace (or binding set) might look like:
</p>
<pre class="example">
{ x -&gt; {type: int, value: 10, address: ...},
  y -&gt; {type: double, value: 3.14, address: ...} }
</pre>
<p>
<b>Block-structured namespaces and shadowing</b>
</p>

<p>
Programming languages often use <b>block structure</b> where inner blocks have their own namespaces that "shadow" (override) outer namespaces. For example,
</p>
<div class="org-src-container">
<pre class="src src-C">int x = 5;  // Global or outer scope
void foo() {
    int y = 10;  // Local to foo()
    {
        int x = 20;  // Shadows the outer x
        printf("%d\n", x);  // Refers to the inner x (20)
    }
    printf("%d\n", x);  // Refers to the outer x (5)
}
</pre>
</div>
<p>
the nested scope would look like
</p>
<pre class="example">
Outer Namespace:
  x -&gt; 5
  foo() -&gt; function

Inside foo():
  y -&gt; 10
  Inner Block:
     x -&gt; 20  (shadows outer x)
</pre>
<p>
As an aside, let's define what we mean by <b>scope</b>. The scope of a variable or of a name is the set of "locations" in the program where the name is visible. A name is <b>visible</b> if you write the name in the program, that's what that name will mean. If you know the scope of every name, you know the visibility of every name and vice versa.
</p>

<p>
<b>Primitive namespaces</b> are the built‐in, irreducible collections of names that a programming language provides by default. They are “primitive” in the sense that they are not constructed by the programmer; instead, they are inherent to the language design. Different kinds of identifiers—such as ordinary variables, function names, struct/union/enum tags, preprocessor macros, and labels—reside in different namespaces. This separation prevents collisions even if the same identifier (e.g., “F”) is used in multiple roles.
</p>

<p>
In C/C++, certain kinds of names belong to different namespaces:
</p>
<ul class="org-ul">
<li>Ordinary identifiers include variables, functions, etc</li>
<li>Struct/union/enum tags are names that follow keywords like <code>struct</code> or <code>enum</code>, which exist in a separate namespace</li>
<li>Preprocessor macros are managed by the preprocessor and not subject to block scope</li>
<li>And labels, which are used in <code>goto</code> statements</li>
</ul>
<p>
For example,
</p>
<div class="org-src-container">
<pre class="src src-C">int f() // Here we have an f that maps to this function
{
  f; // Inside this scope we have another variable so we have a local variable inside the function
  // What about something like below?
  struct f {
    float f; // This is also allowed because its inside the scope of the struct
  } f; // Structs have a different namespace because they always come with
  // an identifier struct. This creates another namespace for f the struct.
  // float f is allowed because the only time you can use f is when you do
  // ____.f or ____ -&gt; f which stands for the value of struct or the pointer of
  // f respectively, so the compiler will
  // understand the context of the f and will know when to use which name
  enum f{
    // can't use g or f here because it could get confused
    // if you do something like return &amp;f;, since both f inside and the
    // struct with name f are ordinary identifiers, it wouldn't know what to do,
    // same with why you cant do int i, i;
  } g; // this cant be named f either
  // but notice that enum f is ok because its in a different namespace than struct.
  #include &lt;f&gt; // this is also ok because its just a library include
  #define f g // this is a preprocessor macro name, so it also works
  f: goto f; // this really means g: goto g;, which is a infinite loop,
  // and since this g is preceeding a : or after goto, this would not collide
  // with the enum g
</pre>
</div>
<p>
the compiler uses context (e.g., after the <code>struct</code> keyword, or after a dot operator) to determine which namespace an identifier belongs to.
</p>

<p>
Python has a simpler model. All variables names within a function are in one namespace (for lcoals) and global names in another. There isn't the added complexity of separate namespaces for types versus variables because Python's model is unified.
</p>
</div>
</div>
</div>

<div id="outline-container-org6312f1b" class="outline-3">
<h3 id="org6312f1b"><span class="section-number-3">1.10.</span> Information hiding (for modularity)</h3>
<div class="outline-text-3" id="text-1-10">
<p>
<b>Information hiding</b> is a design principle that helps you build modular software. Each module (or class, or file) should expose only the functionality other parts of the program need and hide the rest of its implementation details. This approach:
</p>
<ul class="org-ul">
<li>Makes code easier to maintain, because internal details can change without breaking other modules.</li>
<li>Reduces naming conflicts by limiting visibility of internal identifiers.</li>
<li>Promotes cleaner, more understandable interfaces (APIs).</li>
</ul>
<p>
<i>For each identifier in your code, decide whether it should be externally visible or remain private.</i>
</p>
</div>

<div id="outline-container-org1bbe519" class="outline-4">
<h4 id="org1bbe519"><span class="section-number-4">1.10.1.</span> <code>C</code>-style information hiding</h4>
<div class="outline-text-4" id="text-1-10-1">
<p>
In <code>C</code>, the primary way to control visibiluty across different compilation units (i.e. <code>.c</code> files) is
</p>
<ul class="org-ul">
<li><code>static</code> (at file scope): Declaring a function or global variable as static makes it private to that .c file. Other .c files cannot see or call it.</li>
<li>No <code>static</code> (implicitly <code>extern</code>): If you omit static and define a function or global variable at file scope, it becomes externally visible to the rest of the program (assuming you provide a matching declaration in a header file).</li>
<li><code>extern</code> (in a header file): Typically, you place extern declarations in header files so other modules can reference a variable or function:</li>
</ul>
<p>
By default, <code>C</code> does not enforces strong encapsulation; however, using static for private symbols and extern for public symbols is an established convention to emulate a modular design.
Each .c file compiles separately, and the linker resolves references to non-static (public) symbols across object files.
</p>
</div>
</div>

<div id="outline-container-orgdef348d" class="outline-4">
<h4 id="orgdef348d"><span class="section-number-4">1.10.2.</span> <code>Java</code>-style acess modifiers</h4>
<div class="outline-text-4" id="text-1-10-2">
<p>
Java provides more granular access control for classes, methods, and fields:
</p>
<ol class="org-ol">
<li><code>public</code>: visible to all classes everywhere.</li>
<li><code>protected</code>: visible to the same class, subclasses, and other classes in the same package.</li>
<li><i>default</i> (no modifier): visible to classes in the same package only.</li>
<li><code>private</code>: visibly only within the same class.</li>
</ol>
<p>
Take the following visibility table
</p>
<pre class="example">
#+begin_src org
| Modifier   | Same Class | Subclass (diff pkg) | Same Pkg | Other Pkgs |
|------------+-----------+---------------------+---------+-----------|
| public     | ✓         | ✓                   | ✓       | ✓         |
| protected  | ✓         | ✓                   | ✓       | ✗         |
| default    | ✓         | ✗                   | ✓       | ✗         |
| private    | ✓         | ✗                   | ✗       | ✗         |
</pre>

<p>
#+END<sub>SRC</sub>
</p>
<ul class="org-ul">
<li>In Java, each class can be considered a “module” that decides how much of its internal state and methods to expose.</li>
<li>Classes in the same package can share “package-private” (default) access, a middle ground between fully public and fully private.</li>
</ul>
</div>
</div>

<div id="outline-container-org9b6bb27" class="outline-4">
<h4 id="org9b6bb27"><span class="section-number-4">1.10.3.</span> <code>OCaml</code> signatures</h4>
<div class="outline-text-4" id="text-1-10-3">
<p>
In OCaml, a <b>signature</b> is like an interface or “type” of a module, describing what the module exposes (types, values, submodules, etc.) without revealing full implementation details.
</p>
<ul class="org-ul">
<li><code>module</code> defines an implementation (like a "namespace" or "package")</li>
<li><code>module type</code> defines a signature (the interface of a module).</li>
<li><code>sig ... end</code> is the syntax for writing the contents of a signature</li>
<li><code>struct ... end</code> the syntax for writing the contents of a module (implementation)</li>
</ul>
<p>
For example, we have
</p>
<div class="org-src-container">
<pre class="src src-ocaml">module Q = struct
  (* Define a queue type with parameterized elements *)
  type 'a queue =
    | Empty
    | Node of int * 'a * 'a queue

  (* Function to enqueue an element (actual implementation should go here) *)
  let enqueue = (* actual implementation *)
end

(* Use signatures to control what gets exported *)
(* This is statically checked at compile time *)
(* More flexible than Java *)

module type QI = sig
  type 'a queue
  val enqueue : 'a * 'a queue -&gt; 'a queue
end
(* This is the only thing that will be exposed to outside code *)
</pre>
</div>
<p>
The crucial link to <b>information hiding</b> is that a signature can declare both
</p>
<ul class="org-ul">
<li>Type abstraction; you expose <code>type t</code> but hide its actual structure in the <code>struct</code>,</li>
<li>Value declarations; you expose function names and types, but not the internal detials of how they work.</li>
</ul>
<p>
This ensures that code using the module only relies on the published signature. Internals can change without breaking clients, which is a core advantage of the OCaml module system.
</p>
</div>
</div>
<div id="outline-container-org052c4f6" class="outline-4">
<h4 id="org052c4f6"><span class="section-number-4">1.10.4.</span> Other langauges (<code>Python</code>, etc)</h4>
<div class="outline-text-4" id="text-1-10-4">
<p>
<b>Python</b>:
</p>
<ul class="org-ul">
<li>Python does not have built-in private/public keywords for classes. Instead, it relies on naming conventions (<sub>underscore</sub> for “internal” objects, _<sub>double</sub><sub>underscore</sub> for name mangling) and a culture of “we’re all consenting adults here.”</li>
<li>Internally, Python stores attributes of a class in a dictionary. For example, MyClass._<sub>dict</sub>__ holds the class members. This is very dynamic—names can be added or removed at runtime.</li>
<li>Python’s “information hiding” is often done by only exposing certain names in <code>__all__</code> or by prefixing “private” objects with an underscore.</li>
</ul>
<p>
Linking back this to information hiding, why do we do this?
</p>
<ol class="org-ol">
<li>Modularity: Each part of the code is developed and tested independently.</li>
<li>Maintainability: Internals can change without affecting other parts of the program that only see the “public” interface.</li>
<li>Clarity: A smaller, well-defined interface is easier to understand than exposing every function/variable.</li>
</ol>
<p>
Some real world examples:
</p>
<ul class="org-ul">
<li>C Libraries: .h headers define the API; .c files contain private helpers marked static.</li>
<li>Java Libraries: Classes often expose only a small set of public methods. Internals remain private.</li>
<li>Python Packages: By convention, “public” APIs are documented, while internal modules or functions (with _ prefix) are for internal use.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org143b766" class="outline-3">
<h3 id="org143b766"><span class="section-number-3">1.11.</span> Errors, faults, and failures</h3>
<div class="outline-text-3" id="text-1-11">
</div>
<div id="outline-container-org6f75607" class="outline-4">
<h4 id="org6f75607"><span class="section-number-4">1.11.1.</span> Error checking</h4>
<div class="outline-text-4" id="text-1-11-1">
<p>
<b>Compile-Time Checking (Static Checking)</b>
</p>
<ul class="org-ul">
<li>Definition: Many errors are detected before running the program—during compilation.
<ul class="org-ul">
<li>Type Checking: Ensures that functions are called with the correct argument types, variables are used consistently, etc.</li>
<li>Name Resolution: Identifiers must be declared before use, preventing undefined symbol errors.</li>
</ul></li>
<li>Examples of Static Checking:
<ul class="org-ul">
<li>C/C++: The compiler checks function prototypes, variable types, etc.</li>
<li>Rust: Enforces strict rules about borrowing, lifetimes, and type constraints at compile time.</li>
<li>Java: Verifies types, class members, method signatures, etc., during compilation (though it also has dynamic checks for certain things).</li>
</ul></li>
<li>Advantages of Static Checking:
<ul class="org-ul">
<li>Catches many mistakes early, reducing run-time failures.</li>
<li>Can enable optimizations because the compiler knows more about the code structure and types.</li>
</ul></li>
<li>Limitations:
<ul class="org-ul">
<li>Cannot catch every possible error (e.g., array out-of-bounds might still happen at run time unless the language enforces bounds checks).</li>
<li>Requires more up-front specification (e.g., type annotations)</li>
</ul></li>
</ul>
<p>
<b>Run-Time Checking (Dynamic Checking)</b>
</p>
<ul class="org-ul">
<li>Definition: Errors are detected during execution. The language or runtime system checks conditions on-the-fly (e.g., type correctness, array bounds).</li>
<li>Examples of Dynamic Checking:
<ul class="org-ul">
<li>Python: Checks types at run time. If a function is called with the wrong argument type, an exception is raised only at the point of use.</li>
<li>Java/C#: Even though they do static checks, some checks are deferred to run time (e.g., NullPointerException, array bounds checks).</li>
</ul></li>
<li>Advantages:
<ul class="org-ul">
<li>Flexibility: you can write code without specifying strict types in advance (in dynamically typed languages).</li>
<li>More powerful runtime reflection or meta-programming.</li>
</ul></li>
<li>Limitations:
<ul class="org-ul">
<li>Errors can go undetected until a particular code path is executed, possibly in production.</li>
<li>Performance overhead due to continuous checks at run time.</li>
</ul></li>
</ul>
<p>
<b>Binding times and language differences</b>
</p>

<p>
There are two approaches to binding:
</p>
<ol class="org-ol">
<li>Earliest binding, where the language/runtime decides an identifier’s type or meaning at compile time (e.g., C variable types).</li>
<li>Latest binding, where the language defers deciding an identifier’s type/meaning until run time (e.g., Python variables can reference any type object at any moment).</li>
</ol>
</div>
</div>

<div id="outline-container-org0350678" class="outline-4">
<h4 id="org0350678"><span class="section-number-4">1.11.2.</span> Error-handling</h4>
<div class="outline-text-4" id="text-1-11-2">
<p>
In addition to compile-time vs. run-time checks, languages provide various error-handling mechanisms to deal with faults or failures when they do occur.
</p>

<p>
One approach are to use <b>preconditions</b>:
</p>
<ul class="org-ul">
<li>Preconditions are conditions that must be true before a function (or method) runs.</li>
<li>Eiffel: Known for “Design by Contract,” which allows you to specify preconditions (requires) and postconditions (ensures). If a precondition fails, the system raises an exception or error at run time.</li>
<li>Rust: Encourages writing safe functions that check invalid inputs (e.g., using <code>Result&lt;T, E&gt;</code>), or using <code>assert!()</code> macros for invariants. Rust’s type system also prevents certain classes of errors at compile time (like data races).</li>
</ul>

<p>
Another approach is "total definitions": make sure the function will do something reasonable no matter what the arguments are, but this
is not always feasible or reasonable in call cases. i.e. <code>sqrt(-1.0) = NaN</code>. This is done in Rust.
</p>

<p>
Fatal errors are errors that cause the program to stop immediately (e.g., <code>abort()</code>, an uncaught exception, or a <code>panic!</code> in Rust). For user error handling, the programmer explicitly checks for error conditions and decides what to do (return a special value, throw an exception, etc.).
</p>
</div>
</div>

<div id="outline-container-org564a3b6" class="outline-4">
<h4 id="org564a3b6"><span class="section-number-4">1.11.3.</span> Exceptions</h4>
<div class="outline-text-4" id="text-1-11-3">
<p>
Exceptions allow you to separate normal flow from error handling by “throwing” an exception when something unexpected happens, and “catching” it at an appropriate handler. Typically, we would have some <b>throw mechanism</b> <code>throw SomeException("message");</code>, where the runtime "unwinds" the stack until it finds a matching <code>catch</code>.
</p>

<p>
There are checked and unchecked exceptions:
</p>
<ul class="org-ul">
<li>For <b>checked</b> exceptions, they must be declared in a method's signature (Java). The compiler enforces that callers handle or declare these exceptions, i.e. every method specifies what it can throw. Java's exception class hierarchy has <code>java.lang.Object</code> as the root of all classes, <code>java.lang.Throwable</code> is the superclass for all "throwable" objects, <code>java.lang.Exception</code> and <code>java.lang.Error</code> are direct subclasses of <code>Throwable</code>, and <code>IOException</code> is one of many specific exception types that extend <code>Exception</code>.</li>
<li>On the other hand, for <b>unchecked</b> exceptions (Java <code>RuntimeException</code>, C++ exceptions, Python exceptions) the compiler does not force explicit handle (best practice to, typically)</li>
</ul>
<p>
In Java code, we could do something like
</p>
<div class="org-src-container">
<pre class="src src-java">try {
    doSomeStuff();
} catch (IOException e) {
    fixup(e);
} catch (NumberFormatException e) {
    fixupMore(e);
} finally { 
    // This block always executes, even if an exception occurs
    System.out.println("done");
}

</pre>
</div>
<p>
The runtime looks up the call stack for a matching handler. If none is found, the program may terminate (e.g., uncaught exception). Take the following diagram, which illustrates how an exception is thrown in function <code>h()</code>, then propagates up through callers (<code>f()</code>, then <code>main()</code>), searching for a matching <code>catch</code>.
</p>
<pre class="example">
+-------------------+
|    main()         |
|   (caller)        |
+---------+---------+
          |
          | calls f()
          v
+-------------------+
|    f()            |
|   (caller)        |
+---------+---------+
          |
          | calls h()
          v
+-------------------+
|    h()            |
+---------+---------+
          |
          | throw new SomeException("error");
          |
          +--------------------------------------+
          |  Exception thrown in h().           |
          |  No catch block in h(), so it        |
          |  propagates up to the caller (f()).  |
          +--------------------------------------+
          |
          v
+-------------------+
|    f()            |
+---------+---------+
          |
          | Does f() have a catch block for SomeException?
          |  - If YES, handle it here and continue normal flow.
          |  - If NO, rethrow to the next caller (main()).
          |
          v
+-------------------+
|    main()         |
+---------+---------+
          |
          | Does main() have a matching catch block?
          |  - If YES, handle it and continue.
          |  - If NO, program terminates with an unhandled exception.
          |
          v
(End of call stack)
</pre>
<p>
We can read this diagram as follows:
</p>
<ol class="org-ol">
<li><code>main()</code> calls <code>f()</code>, which in turn calls <code>h()</code></li>
<li><code>h()</code> throws an exception (<code>SomeException</code>)</li>
<li>If <code>h()</code> does not handle it, the runtime looks in <code>f()</code> for a matching <code>catch</code>.</li>
<li>If <code>f()</code> does not handle it, it propagates up to <code>main()</code></li>
<li>If <code>main()</code> does not handle it, the program typically terminates with an unhandled exception error.</li>
</ol>

<p>
We can also <b>checked exceptions</b> for statically typed languages (i.e. in Java here).
</p>

<p>
In Java, the compiler enforces that methods either handle or declare exceptions that might be thrown.
</p>
<ul class="org-ul">
<li>If a method can throw a checked exception, it must declare this in its signature using the <code>throws</code> keyword</li>
<li>e.g. <code>public void readFile(String filePath) throws IOException</code></li>
</ul>
<p>
then, the caller can also declare what exceptions it will throw, and must be declared or handled.
There are some exceptions, i.e. <code>NullPointerException</code>.
</p>
</div>
</div>
</div>


<div id="outline-container-orgda256dd" class="outline-3">
<h3 id="orgda256dd"><span class="section-number-3">1.12.</span> Parameter Passing, Object-Oriented Programming, Cost Models</h3>
<div class="outline-text-3" id="text-1-12">
</div>
<div id="outline-container-org8afde0b" class="outline-4">
<h4 id="org8afde0b"><span class="section-number-4">1.12.1.</span> Parameter Passing</h4>
<div class="outline-text-4" id="text-1-12-1">
<p>
There are two main issues to be concerned about when discussing parameter passing:
</p>
<ol class="org-ol">
<li><b>Semantic issues</b>, or how arguments in a function call correspond to the function's parameters. This includes matching based on position, keywords, or handling a variable number of arguments.</li>
<li><b>Efficiency issues</b>, or how the parameter passing mechanism affects performace, particularly in programs with lots of function calls / expensive function calls, etc.</li>
</ol>
<p>
There are a few different calling conventions discussed.
</p>
<ol class="org-ol">
<li><b>Positional correspondence</b> is the most traditional way, where each argument is paired with its corresponding parameter in order (left-to-right)
<ul class="org-ul">
<li>For example, a function <code>def F(x, y): return x + y</code> when called with <code>result = F(5, 10)</code> will match <code>x = 5, y = 10</code></li>
</ul></li>
<li><b>Variadic or "Packaged" arguments</b> allows you to pass multiple arguments that get bundle into a tuple (or similar data structure), not all languages support this.
<ul class="org-ul">
<li>For example, a function <code>def F(x, *y): return x, y</code> when called with <code>result =F(5, 10, 15, 20)</code> will match <code>x = 5</code> and <code>y = (10, 15, 20)</code></li>
</ul></li>
<li><b>Keyword correspondence</b> is when arguments are specified with names, allowing them to be passed in any order
<ul class="org-ul">
<li>For example, a function <code>def F(x, y): return x * y</code> when called with <code>result = F(y = 10, x = 5)</code> will correctly match the parameters by their name, even though the positions are swapped.</li>
</ul></li>
</ol>
<p>
The first calling convention we discuss is <b>call by value</b>. In call by value, the caller evaluates the argument (even if it's an expensive expression) and passes a copy of the resulting value to the function (callee).
</p>
<ul class="org-ul">
<li>It's <b>simple</b>, since each parameter in the function is a distinct copy. Changes in the calee do not affect the original value in the caller.</li>
<li>It's <b>efficient for small arguments</b> such as integers, floats, object pointers, since they are fast and easy to copy.</li>
</ul>
<p>
We also have <b>call by reference</b>, where instead of copying the value, the caller passes a reference (or address) to the variable where the data is stored. The callee then uses this reference to access or modify the original value.
</p>
<ul class="org-ul">
<li>It's a lot more <b>efficient for large data</b>, like arrays or tensors, since passing a reference is much faster than copying the entire object.</li>
<li>It enables <b>in-place modification</b>, since the callee can modify the object directly, which is useful when you want the changes to be visible to the caller.</li>
</ul>
<div class="org-src-container">
<pre class="src src-C">// Function that accepts two parameters by reference
void F(int *x, int *y) {
    // Dereference x and y to access their values
    *x = *x + 5;    // Modifies the value of the original variable pointed to by x
    *y = *y;        // Here, simply using y (could perform operations similarly)
}

int main() {
    int a = 1000;
    int b = 7;
    // Passing the address of 'a' and the address of a temporary (for example, b+7)
    F(&amp;a, &amp;b);
    // The original variable 'a' is modified by F
    return 0;
}
</pre>
</div>
<p>
If the argument is a complex expression (like <code>B + 7</code>), the compiler may create a temporary storage location, pass its address to the callee, and then work with that location. This ensures the original variable isn't accidently modified.
</p>

<p>
There are some downsides to call-by-reference. Namely,
</p>
<ul class="org-ul">
<li>It introduces <b>overhead for small data</b>, since passing an address might be slower when the data is small (e.g., a 4-byte integer might require passing an 8-byte address)</li>
<li>We may run into <b>aliasing problems</b>: when the same memory location is passed through different parameter names (aliasing), it can lead to confusing code and potential undefined behavior.</li>
</ul>
<p>
For example, in C++, we could have potential aliasing issues with the following code:
</p>
<div class="org-src-container">
<pre class="src src-cpp">// Function that demonstrates potential aliasing issues
void F(int &amp;x, int &amp;y) {
    printf("Initial x: %d\n", x);  // Should print the original value of x
    y = x++;  // Increment x via one alias and assign to y
    printf("Modified x: %d\n", x); // The outcome may be unexpected due to aliasing
}

int main() {
    int i = 12;
    // Passing the same variable for both parameters, causing aliasing
    F(i, i);
    // The behavior is undefined: i might not simply become 13.
    return 0;
}
</pre>
</div>
<p>
When the same variable is passed to both parameters, both <code>x</code> and <code>y</code> refer to the same memoery. This can lead to undefined behavior because the operations interfere with each other. Additionally, aliasing forces the compiler
to be conservative in optimizations. For example, it cannot cache a value in a register if it might be changed indirectly through an alias, potentially slowing down execution.
</p>

<p>
The <b>efficiency vs. semantics</b> trade-offs is a balancing act. One one hand,
</p>
<ul class="org-ul">
<li><b>Call by value</b> offers simple semantics and potential for aggressive optimization (e.g., caching in registers) but may be inefficient for large objects.</li>
<li>whereas <b>call by reference</b> is efficient for large data by avoiding copies but can lead to aliasing issues that complicate both human understanding and compiler optimizations.</li>
</ul>
<p>
Designers of programming languages often try to provide mechanisms that balance clear semantics with execution efficiency. In many modern languages, object handling is abstracted so that you might see call-by-value in source code but under the hood, the language may actually pass references (especially for objects) to maintain efficiency.
</p>
</div>
</div>

<div id="outline-container-orga0e3d20" class="outline-4">
<h4 id="orga0e3d20"><span class="section-number-4">1.12.2.</span> Advanced Calling Conventions</h4>
<div class="outline-text-4" id="text-1-12-2">
<p>
Now, we can discuss <b>call-by-result</b>. In call-by-result, the callee starts with an uninitialized parameter, uses it during execution, and then &#x2013; just before returning &#x2013; copies its final value back to a location provided by the caller. Step-by-step, we have
</p>
<ul class="org-ul">
<li>The caller designates a location where the result should be stored</li>
<li>The callee writes the computed result into its local (but uninitialized) parameter</li>
<li>Upon return, that result is copied back into the caller's designated storage</li>
</ul>
<p>
In pseudocode, that could look something like
</p>
<pre class="example">
procedure F(result: out int) {
    // 'result' is uninitialized at the start
    result := computeSomething();  // The function computes and assigns a value
}

// Caller
int res;
F(res);
// Now, res contains the result computed by F.
</pre>
<p>
This approach avoids the cost of copying large data on function entry, since the data is only copied back out at the end of the function. It is especially beneficial when returning large objects without incurring extra copying overhead. Another example could be <code>read</code>. In the Linux API,
the function might be declared like
</p>
<div class="org-src-container">
<pre class="src src-C">ssize_t read(int file_descriptor, void *buffer, size_t buff_size);
</pre>
</div>
<p>
From the caller's perspective,
</p>
<ul class="org-ul">
<li>The caller provides a file descriptor, a buffer (an array), and the size of the buffer</li>
<li>The caller does not need to initialize the buffer's contents because its initial state doesn't matter</li>
</ul>
<p>
From the callee's perspective (inside <code>read</code>),
</p>
<ul class="org-ul">
<li>The function treats the provided buffer as uninitialized</li>
<li>It reads data from the file descriptor into the buffer</li>
<li>It then returns the number of bytes successfully read</li>
</ul>
<p>
It can be thought of in Ada-like pseudocode as follows:
</p>
<pre class="example">
procedure ReadFile(Result_Buffer : out Buffer_Type; Num_Bytes_Read : out Integer) is
begin
    -- The buffer is assumed to be uninitialized.
    -- Read data from the file and fill in Result_Buffer.
    -- Set Num_Bytes_Read to the number of bytes read.
end ReadFile;
</pre>
<p>
This is more efficient, because instead of copying a potentially large buffer from the caller to the callee (as in call by value), the callee writes directly into the buffer provided by the caller.
</p>

<p>
Additionally, this mechanism, allows the function to "return" both the updated buffer and the count of the bytes read (multiple return values), specifically, for our <code>read</code> example, what happens is that
</p>
<ul class="org-ul">
<li>The <b>return value</b> would be the number of bytes read. Our function returns this as normal.</li>
<li>The <b>call-by-result</b> parameter, i.e. our buffer parameter was passed in uninitialized. Inside the function, the read operation fills this buffer with data. Then, just before the function returns, the final contents of that buffer are copied back to the caller's buffer.</li>
</ul>
<p>
So, although the function explicitly returns the number of bytes read, the buffer itself is updated by the callee using call-by-result semantics. This lets the function effectively "return" multiple values (both the buffer and the count) without having to package them into a single tuple or structure.
</p>

<p>
Another similar convention is <b>call-by-value-result</b>. It is similar to call by result but involves an extra copying step:
</p>
<ol class="org-ol">
<li>The caller passes a copy of the argument (like call by value)</li>
<li>The callee works on that copy</li>
<li>Upon return, the callee copies the (possibly modified) value back to the caller</li>
</ol>
<p>
This scheme avoids aliasing issues because the callee is operating on a local copy during execution. However, it t requires two copies (one when entering the function and one when returning), which might be inefficient for large data.
</p>


<p>
Another "convention" are <b>macro calls</b>. They are quite different from parameter passing methods discussed earlier, and exist in languages like <code>C, C++, Scheme</code>. They work as follows
</p>
<ul class="org-ul">
<li>They are <b>compile-time evaluated</b>: the compiler expands the macro call by copying and substituting code text. This occurs during compilation, not at runtime.</li>
<li>Essentially they do <b>test substitution</b>. Rather than evaluating arguments and passing values or addresses, the macro mechanism directly inserts the code provided in the macro definition.</li>
</ul>
<p>
Suppose we have a simple macro in <code>C</code>, <code>#define SQUARE(x) ((x) * (x))</code>. When you write <code>SQUARE(5)</code> in your code, the preprocessor replaces it with <code>((5) * (5))</code> during compilation. This is purely a compile-time text substitution. There's no runtime cost of function calling or parameter passing.
</p>

<p>
Finally, we discuss <b>call-by-name</b>. In call by name, the caller does not evaluate the argument expression immediately. Instead, it passes a “recipe” for computing the value—often implemented as a parameterless function (sometimes called a <i>thunk</i>). As a mechanism,
</p>
<ul class="org-ul">
<li>The callee receives a function (or a reference to an expression) instead of a direct value</li>
<li>Each time the callee needs the argument's value, it calls this function, thus reevaluating the expression in the caller's context.</li>
</ul>
<p>
Imagine <code>F</code> which takes two call-by-name parameters
</p>
<pre class="example">
// Define F which expects thunks (parameterless functions) for its arguments
function F(getX, getY) {
    // When F needs x or y, it calls the functions to evaluate the expressions.
    valueX = getX()  // Evaluates the expression for x
    valueY = getY()  // Evaluates the expression for y
    // Use valueX and valueY in computations
    return valueX + valueY;
}
</pre>
<p>
In order to call <code>F</code> with call by name, semantics, it would look something like
</p>
<pre class="example">
// Caller defines the expressions but does not evaluate them immediately.
N = number_of_items;
sum = compute_sum_of_array();

// Pass the expressions as parameterless functions (thunks)
result = F(
    function() { return sum; },
    function() { return sum / N; }
);
</pre>
<p>
In terms of <b>advantages</b>, we have
</p>
<ul class="org-ul">
<li><b>Reliability</b> (in certain cases). For example if an expression might cause an error (such as division by zero) when evaluated, call by name will only compute it if it's actually needed. More specifically, if the function checks for <code>N =</code> 0= before calling <code>getY()</code> then the dangerous expression (<code>sum / N</code>) is never evaluated when it would cause a crash. Similarily, we would also avoid infinite loops.
<ul class="org-ul">
<li>Specifically, consider a function where one of the arguments might cause an error (e.g., division by zero or entering an infinite loop). With call by name, if the function’s body never uses that argument, the dangerous expression is never evaluated.</li>
</ul></li>
<li>We also can <b>avoid unnecessary computation</b>. If the callee never uses a particular argument, its potentially expensive computation is entirely skipped.</li>
</ul>
<p>
However, it also has a lot of <b>drawbacks</b>. Namely,
</p>
<ul class="org-ul">
<li>It comes with a huge <b>performance overhead</b>. Every time the callee needs the value, the thunk is called. This repeated function call can be less efficient than computing the value once.</li>
<li>There is also a bit of <b>implementation complexity</b>. The underlying machine code must support this deferred evaluation, making the process more complicated than simple value copying.</li>
</ul>
<p>
We go through an example that Eggert discussed in lecture. Imagine a function that prints the average of items in an array. With call by value, the expression for the average might be computed before the function is called—leading to a potential crash when dividing by zero. With call by name, the function can check the condition before evaluating the dangerous expression.
In call-by-value, we would do something like
</p>
<pre class="example">
// Assume sum and N are computed before calling print_average.
print_average(sum / N, N);  // Division by zero occurs if N is 0, even if not used in the function.
</pre>
<p>
whereas in call-by-name, we can check:
</p>
<pre class="example">
function print_average(getAverage, getCount) {
    if (getCount() == 0) {
        print("No items to average.");
    } else {
        print("Average is " + getAverage());
    }
}

// Caller passes thunks to delay evaluation.
print_average(
    function() { return sum / N; },
    function() { return N; }
);
</pre>
<p>
In Scheme, call-by-name might look something like
</p>
<div class="org-src-container">
<pre class="src src-Scheme">(define (F getX getY)
  ;; When F needs the values, it calls the functions:
  (let ((x (getX))
        (y (getY)))
    (+ x y)))  ; For example, here we add the two results.

;; Suppose we have:
(define a 10)
(define b 20)

;; Calling F with thunks:
(F (lambda () a) (lambda () (+ b 7)))

;; Or, the example from earlier
;; Define a function that prints an average only if the count is non-zero.
(define (print-average getAverage getCount)
  (if (= (getCount) 0)
      (display "No items to average.")
      (display (string-append "Average is " (number-&gt;string (getAverage))))))

;; Caller wraps the expressions in lambdas (thunks) so they are not evaluated immediately.
(define sum 100)
(define N 0)

(print-average
  (lambda () (/ sum N))  ; Dangerous expression (division) that won’t be evaluated if N==0.
  (lambda () N))

</pre>
</div>
<p>
Oftentimes, you, as the programmer, don’t actually write these thunks explicitly. Instead, the compiler automatically transforms the argument expressions into thunks (or similar constructs), so that the function call effectively delays the evaluation of these expressions.
</p>

<p>
The term <b>lazy evaluation</b>, a programmy strategy where evaluation of an expression is deferred until its value is actually needed, is sometime synonymous with call-by-name. On the other hand, <b>eager</b> evaluation is the opposite of lazy evaluation; arguments are evaluated before the function call, and is generally thought of as all the other methods (call-by-value, by-reference, by-result, by-value-result).
</p>

<p>
This brings us to <b>call-by-need</b>, which is an improvement over call-by-name (the "eager" evaluation version). Instead of evaluating the argument every time it’s used, the result is cached (or “memoized”) after the first evaluation. Subsequent accesses use the cached value, thereby avoiding repeated computation.
</p>

<p>
One example of the problem with pure call by name is that if you are inside a loop and an argument (thunk) is used repeatedly, call by name forces re-evaluation each time. This can be inefficient if the computation is expensive.
On the other hand, in this scenario, under call-by-need, when the thunk is called the first time, its result is stored. Any subsequent use of that argument returns the cached value rather than recalculating it. Take this example in <code>Haskell</code> that demonstrates lazy evaluation -
</p>
<div class="org-src-container">
<pre class="src src-Haskell">-- A lazy list of all prime numbers (conceptual; actual implementation may vary)
primes :: [Integer]
primes = sieve [2..]  -- Imagine sieve is a function that lazily computes primes

-- Accessing the 7th prime in the list.
-- Only as many primes as needed will be computed.
seventhPrime :: Integer
seventhPrime = primes !! 6  -- List indexing starts at 0
</pre>
</div>
<p>
There are a few interesting things about Haskell to note here:
</p>
<ol class="org-ol">
<li><b>Infinite data structure</b> - Haskell can represent infinite lists because it computes only as much of the list as is needed. In the above example, when we ask for the 7th prime, Haskell computes just enough of the prime sequence.</li>
<li><b>Goal-oriented computation</b> - The program is “read backwards”: you first see what is needed (the 7th element), then determine which computations are required to supply that element.</li>
<li><b>Call-by-need efficiency</b> - Once a value is computed (like an element of the prime list), it’s cached. This avoids recalculating the same value repeatedly, which is especially useful in loops or repeated function calls.</li>
</ol>

<p>
Finally, we will do what <code>prolog</code> uses, which is <b>call-by-unification</b>, which was discussed earlier but will be discussed again here. Call-by-unification is a method where arguments are not passed as already computed values but rather as expressions or terms that include variables. These variables can be <i>uninstantiated</i> (or partially instantiated) when passed to a predicate. The callee (or predicate) then “unifies” these terms—matching and binding variables to values during the computation.
</p>

<p>
<b>Unification</b> is the process of making two logical terms equal by finding a substitution for variables that makes them identical, and is central to Prolog's execution model. Importantly, it enables
</p>
<ol class="org-ol">
<li><b>Partial information</b>: unlike call-by-value or call-by-result, unification allows variables to remain free (uninstantiated) until the predicate provides more information.
<ul class="org-ul">
<li>For example, a predicate may accept an argument that is not fully known until further constraints are applied.</li>
</ul></li>
<li><b>Bidirectional flow of information</b>: the caller and callee can both contribute to determining the value of a variable.
<ul class="org-ul">
<li>The callee might bind a variable to a value.</li>
<li>Later, the caller might further refine that value based on additional context.</li>
</ul></li>
</ol>
<p>
Call-by-unification is “maybe the closest is call by value-result,” in that both allow the callee to modify data that is then “returned” to the caller. However, call-by-unification is more flexible because variables can remain uninstantiated and be filled in as the predicate executes.
</p>
<div class="org-src-container">
<pre class="src src-prolog">% Define a predicate that adds two numbers and unifies the result with a third.
add(X, Y, Z) :-
    Z is X + Y.

% Query examples:
% 1. Fully instantiated arguments:
?- add(3, 4, Result).
% This will unify Result with 7.

% 2. Partially instantiated query:
?- add(3, Y, 7).
% Unification will determine Y = 4, since 3 + Y must equal 7.

% 3. Uninstantiated output variable:
?- add(X, Y, 10).
% This predicate might leave X and Y underconstrained unless additional conditions are provided.
</pre>
</div>
<p>
In the above examples, the predicate add/3 is called with various degrees of known information:
</p>
<ul class="org-ul">
<li>When all arguments are given except the result, the predicate computes the sum and unifies the result.</li>
<li>When one of the input values is missing, Prolog uses unification to solve for the missing value that satisfies the equation.</li>
</ul>
<p>
This mechanism lets the program “fill in” unknowns during execution. The same predicate can be used to compute sums or to infer missing numbers based on the known result.
</p>

<p>
Beyond various argument-passing strategies, many calling conventions rely on extra, <b>hidden parameters</b> (or "transformed parameters"), not explicitly written in the source code but are cruicial for
</p>
<ul class="org-ul">
<li>OOP: <code>this</code> or <code>self</code> is an implicit pointer passed to methods that refers to the current object
<ul class="org-ul">
<li>Also, extra slots for objects (some overhead, i.e. a <code>type</code> slot, or <code>prototype</code>, or for the garbage collector (size, etc))</li>
</ul></li>
<li>Static and dynamic chains:
<ul class="org-ul">
<li>Static chain; Used for nested functions, it’s a pointer to the defining environment (the definer’s frame) so that non-local variables can be resolved.</li>
<li>Dynamic chain; A pointer to the caller’s frame (for example, <code>rbp</code> on x86-64) that helps maintain context during function calls.</li>
</ul></li>
<li>The return address; i.e., the address to which control returns after a function call, often stored as the top word on the stack or in a dedicated register.</li>
<li>Thread-local-storage (TLS); A pointer to data that is local to the current thread, allowing efficient access to thread-specific variables without locking.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org55402cf" class="outline-3">
<h3 id="org55402cf"><span class="section-number-3">1.13.</span> Cost Models</h3>
<div class="outline-text-3" id="text-1-13">
<p>
First, there is the idea of high-level vs. low-level software. Typically, we trade off ease-of-use vs. speed. In ML, there are
</p>
<ul class="org-ul">
<li>High level frameworks (PyTorch), which are easy to use and lead to rapdi development, but may run mich slower because they incur overheads that hide low-level optimizations</li>
<li>and low-level systems (CUDA) which are optimized for perofrmance on specific hardware, but are difficult to program in and the code is made to be highly specialized.</li>
</ul>
<p>
We digress and discuss GPU cost models and the memory hierarchy. The latest NVIDIA chips are the Blackwell series, which (like other GPUs), have the following memory hierarchy:
</p>
<ul class="org-ul">
<li>Registers
<ul class="org-ul">
<li>Located in each “SM” (streaming multiprocessor).</li>
<li>Very fast, but extremely limited in size (e.g., registers might total only a few hundred bytes per SM).</li>
</ul></li>
<li>L1 cache
<ul class="org-ul">
<li>Faster than main memory; closer to the computational unit.</li>
<li>Capacity might be around tens of megabytes (e.g., 24 MB on the B100 module).</li>
</ul></li>
<li>L2 cache
<ul class="org-ul">
<li>Larger but slower than L1; shared among multiple SMs (e.g., 128 MB).</li>
</ul></li>
<li>HBM3e (high-bandwidth memory)
<ul class="org-ul">
<li>Acts like very fast RAM; capacities can be very high (e.g., 196 GB).</li>
<li>Much faster than DDR memory in laptops.</li>
</ul></li>
</ul>
<p>
The implications are clear:
</p>
<ul class="org-ul">
<li>Computations that fit into registers run extremely fast.</li>
<li>If your data spills into slower memory (like L1 or HBM), performance suffers.</li>
<li>Cost models for GPU programming are very different from traditional CPU models, where memory was assumed to be random access with uniform speed.</li>
</ul>
<p>
There are these evolving cost models
</p>
<ul class="org-ul">
<li>Code optimized for the previous generation (like the A100) may run slower on a Blackwell chip because the sizes and speeds of different memory levels have shifted.</li>
<li>Reoptimizing low‑level C++/CUDA code may take years as programmers learn the new cost model of the hardware.</li>
</ul>

<p>
This brings us to: <b>what is a cost model?</b>
</p>

<p>
It's a mental model of computiong resources that you need to understand in order to operate your software at the greatest efficiency needed or efficient execution.
</p>

<p>
We also need to be wary of both big-O type cost models, but also absolute cost-models. If you know your software is going to run on some bounded sizes of the problem, it may not always be true that some asymptotically better runtime will be better than some other algorithm.
Here, we will worry about the constant factor / absolute cost. In terms of costs, we can consider <i>time</i>, <i>memory</i>. There are trade-offs (i.e. trade time for space).
</p>

<p>
Quick digression: CPU time is the actual time where the CPU does computation, whereas real time also includes the time when your program is sitting around and waiting for input (i.e. includes I/O overhead). On multi-threaded applications, real-time can be faster than CPU time.
</p>

<p>
Returning, we also consider <i>power</i> and <i>energy</i>. i.e. you may need to run low-power. There's also <i>I/O</i> access time and space, <i>reliability cost</i> (we assume machines are perfect, though), and <i>network access time</i>. Minimizing accesses on and off the netowrk is important here. We will focus on speed cost models.
</p>
</div>
<div id="outline-container-org581aea1" class="outline-4">
<h4 id="org581aea1"><span class="section-number-4">1.13.1.</span> Cost models in programming languages</h4>
<div class="outline-text-4" id="text-1-13-1">
<p>
Let's look at the cost model for lists. The basic model is that we have some list of <code>N</code> itees, represented in RAM in e.g. the following way
</p>
<pre class="example">

[ 0 ] -&gt; [ 1 ] - [ 2 ] -&gt; [ 3 ] -&gt; ...
Addr 0  Addr 300  Addr 1391 ...
</pre>
<p>
although the boxes are in nice numeric order, the addresses are somewhat random, but we don't really care since we have RAM.
Under this cost model, what's the cost of <code>append</code>?
</p>

<p>
First, let's consider <code>Lisp</code>. In <code>Lisp</code>, a list is a chain of "cons" cells (each cell contains a value and a pointer to the next cell). Under an <code>append</code> operation,
we create a new list that copies the cells from the first list and then links to the second list. Under this, the cost is proportional to the length of the first list.
</p>
<pre class="example">
List A: [A1] -&gt; [A2] -&gt; [A3] -&gt; NIL
List B: [B1] -&gt; [B2] -&gt; NIL

append(A, B) produces:
[A1] -&gt; [A2] -&gt; [A3] -&gt; [B1] -&gt; [B2] -&gt; NIL
</pre>
<p>
Now, we turn to lists in <code>Python</code>. In <code>Python</code>, they are implemented as dynamic arrays. II.e., they contain a header with a pointer to the array of items, and the current length and allocated capacity. When we do
<code>append</code>, in the common case, adding one element is \(O(1)\) because the array has extra capacity. Although occasionally a new, larger array must be allocated and copied, the average (amortized) cost per append is still \(O(1)\).
</p>

<p>
Briefly, Prolog's unification cost model is as follows. When comparing two logical terms, the cost of unification is roughly proportional to the size of the smaller term. This must be built into the cost model for logic programs, similar to how list operations are analyzed in imperative languages.
</p>

<p>
What about <b>low-level array acess and optimization</b> in our cost model?
</p>

<p>
First, array access calls. Assume that we have an array of 64-bit (8-byte) doubles. Then the address of element <code>A[i]</code> can be computed by <code>base_address + (i * 8)</code>. How can we calculate this as quickly as possible?
</p>

<p>
First, we place the address of the array and the index <code>i</code> in register. Then, notice that multiplication by 8 can be replaced with a left shift by 3 bits:
</p>
<div class="org-src-container">
<pre class="src src-C">// Given: double *A is the base address, and i is the index.
// Instead of: address = A + (i * 8);
// Do:
address = A + (i &lt;&lt; 3); // Left shift i by 3 (i.e., multiply by 8)
</pre>
</div>
<p>
this part of the reason primitive types are powers of two. However, there are some complications with just doing this all the time. 
Some languages (e.g., Java, JavaScript) require subscript checking to ensure indices are in range. The naive implementation would be to do
two comparisons (i &lt; 0 and i &gt;= length) with two branches. We can slightly optimize by using an unsigned comparison to combine these checks into a single conditional branch
</p>

<p>
We also care about <b>cache effects</b> on array access. Cache lines (e.g., 64 bytes) determine how data is loaded from memory.
Using power-of-2 sizes can lead to cache conflicts. Use row that are not exact powers of 2 (or even prime numbers) so that different array elements map to different cache lines.
</p>

<p>
For example, when summing a column in a 2D array, if all rows are aligned with a power‑of‑2 size, many elements may hash to the same cache line, causing frequent cache evictions.
By choosing a row size that is not a power of 2, the accesses will spread out over multiple cache lines, leading to better performance
</p>
</div>
</div>
</div>

<div id="outline-container-org83b77fd" class="outline-3">
<h3 id="org83b77fd"><span class="section-number-3">1.14.</span> Rust</h3>
<div class="outline-text-3" id="text-1-14">
<p>
<b>At a glance</b>
</p>

<p>
The initial focus of <b>Rust</b> is memory safety. Importantly, compared to other languages like Python, Scheme and OCaml
which have memory safety done at runtime (dynamically), Rust does the memory safety statically - at compile time. However,
Rust's memory safety does not extend to all memory accesses. In this way, the dynamic checks also not 100% sure all the time.
Also, OCaml does do some memory safety at compile-time.
</p>
</div>

<div id="outline-container-org52327a1" class="outline-4">
<h4 id="org52327a1"><span class="section-number-4">1.14.1.</span> Digression: Static checking of encryption-based program</h4>
<div class="outline-text-4" id="text-1-14-1">
<p>
Suppose you have a program which wants to compute something, but you want to do it in a way that you can run it on some
other machine, like say at Amazon, but we don’t want Amazon to know what you are doing. Mathematically, we want the program
\[ y = f(x) \]
and we tell Amazon
\[
E(y) = F_E(E(x))
\]
What we tell Amazon is something else. We encrypt \(x\), we ship it off to Amazon, and Amazon gives you the encrypted version
of \(y\), which it computes by some more complicated program \(F_E\). \(F_E\) is an encrypted program that can read its input,
compute, compute the encrypted output, but <i>such that if you inspect \(F_E\) while it's running, you can't debug it (effectively)</i>.
I.e. it is slower, and "impossible" to debug. We are interested in the question: <b>how can we debug this?</b>. So, dynamic analysis (debugger, etc) will not work.
This leaves only <b>static checking</b> of the original program and the compiler (from \(F\) to \(F_E\)).
</p>
</div>
</div>

<div id="outline-container-org2ce4903" class="outline-4">
<h4 id="org2ce4903"><span class="section-number-4">1.14.2.</span> Rust: Performance, Ownership and Borrowing</h4>
<div class="outline-text-4" id="text-1-14-2">
<p>
The mantra is 0 cost abstraction: we want to operating at a higher-level than machine code, but we want the same performance of machine code.
</p>

<p>
<b>Ownership</b>
</p>

<p>
Every value in Rust has a unique owner. When the owner goes out of scope, the value is automatically cleaned up (this is often referred to as RAII in C++ terms). 
The compiler statically enforces that there is exactly one owner at any time, preventing issues like double frees or memory leaks.
</p>

<p>
<b>Borrowing</b>
</p>

<p>
Instead of transferring ownership, you can borrow references to a value. Rust distinguishes between mutable and immutable references.
</p>
<ul class="org-ul">
<li>Immutable references allow multiple parts of the program to read a value concurrently.</li>
<li>Only one mutable reference is allowed at a time, ensuring that no two parts of the program can modify the same data simultaneously (thus avoiding data races).</li>
</ul>
<p>
The Rust compiler <i>uses these rules to check your code at compile time</i>. If you violate ownership or borrowing rules (for example, by trying to have a mutable reference while immutable ones exist), the compiler will produce an error.
</p>

<p>
Because data races are prevented by the ownership/borrowing system, writing safe multi-threaded code in Rust is much easier. You can use multi-core systems effectively without the traditional pitfalls of locking in C or C++.
</p>

<p>
We also tackle <i>garbage collection</i> by preventing a class of errors (i.e. dangling pointer, memory leaks) from occurring.
When a reference variable drops out of scope, if it owns the objects, its <code>drop</code> method is invoked, and that must clean up. The <code>drop method</code> will also automatically go
through all of the slots in the object and drop them as well, i.e., pointers, etc. The <i>transitive closure</i> is automatic. The memory management parts are automatic.
</p>

<p>
Note that immutable in Rust is a bit tricky. First, we compare to <code>C</code>'s <code>const. We have can have a pointer declared as
pointing to some constant value (=const int *p</code>). Although the pointer <code>p</code> is declared to point to a constant integer (you cannot
modify the value through <code>p</code>), it does not guarantee that the object itself will never change. For instance,
</p>
<div class="org-src-container">
<pre class="src src-C">int *q = ...; // q points to the same integer as p, and q can modify it.
int x = *p;
// ... some operations ...
int y = *p;
</pre>
</div>
<p>
even though <code>p</code> is a pointer to const, the underlying integer might change via another pointer like <code>q</code>, so we cannot guarantee <code>x</code> and <code>y</code> are equal.
</p>

<p>
We have the same notion in rust. "immutable" does not guarantee that an object will <i>never</i> change. Instead,
immutability in Rust means that, <b>in the current state or context, the value is not allowed to be modified</b> ("shared access"). This allows us
to make some guarantees on data races. In <code>C</code>, a data race occurs at the intersection of <i>shared data</i> and <i>mutable state</i>.
Functional programming makes guarantees on this by making everything immutable. Rust's approach is different. Rust kills the combination of shared data and mutable state, but not the two individually.
If an object is accessed via an immutable reference, it signals that the object should not be changed while that reference is active. However, the object itself might later become mutable once those immutable references are no longer in scope.
</p>

<p>
In order to have true memory safety, Rust also has
</p>
<ul class="org-ul">
<li><b>Runtime bounds checking</b>: We keep track of an array's bounds, and if it's out of bounds, we <code>panic</code> (error handling). The practical implication is that an array rust is no longer just an array with
a pointer to a base address, it's a "fat pointer" that also includes an upper bound, and potentially a lower bound. At runtime, whenever we do a subscript check, which costs us at least one comparison and potential branch.
You can do a trick with for loops here, so you don't have to check everything every single time (i.e. just check loop bounds). However, it is still slower.</li>
<li><b>No pointer arithmetic</b>: i.e. can only point to start of the array</li>
<li><b>No free operation</b>: <code>drop</code> is called for you automatically</li>
<li><b>No null pointer</b>: if you want your "nullpointer" in Rust, you wrap things in <code>Option</code> like how OCaml does it.</li>
</ul>
<p>
All of these points attack the following problems
</p>
<ul class="org-ul">
<li>Use after free,</li>
<li>Double free,</li>
<li>Buffer overrun,</li>
<li>Data races,</li>
<li>and null pointer dereferencing</li>
</ul>
<p>
common errors in <code>C</code>, but not in <code>Rust</code>. However, this cuts down performance. As an option, <code>Rust</code> has the keyword <code>unsafe</code> which brings
us back to the <code>C/C++</code> world and get the performance. The goal is to not use <code>unsafe</code> as much as possible, and it's typical
that this happens for the code that runs most of time (for the performance benefits). However, this is OK, since it will have to be
tested and used a lot. Can be thought of as the Linux Kernel, which is used all the time so gets rigorously tested in this way.
</p>
</div>
</div>

<div id="outline-container-org7cb08b0" class="outline-4">
<h4 id="org7cb08b0"><span class="section-number-4">1.14.3.</span> Rust failure handling</h4>
<div class="outline-text-4" id="text-1-14-3">
<p>
Form 1: Rust has a "panic" primitive (<code>panic("ouch");</code>), which kills the program with a backtrace to help you with debugging, and unwinds the stack, and finally exits. 
You can configure the panic primitive if you want, to not exit as quickly as possible.
</p>

<p>
Form 2: <code>Result</code>, declared as
</p>
<div class="org-src-container">
<pre class="src src-rust">enum Result&lt;T, E&gt; {
Ok(T),
Err(E)
}
</pre>
</div>
<p>
which can be thought of as a discriminant union, similar to the <code>option</code> in OCaml. Can be thought of
as simply being a pointer to an object, and the object either says "I'm OK" and have a value of type <code>T</code>,
or "I'm not OK" and have a value of type <code>E</code>. It's similar to an exception, but there is no try-catch,
which means you can use it in a function and pass the information (the type) back up to your callers. In some ways,
you can write exception-handling "by hand". This is better for <i>low-level</i> code, where you need to be worried about
what errors can happen all the time, and want <i>low-level</i> control over it.
</p>
</div>
</div>

<div id="outline-container-orgff17c77" class="outline-4">
<h4 id="orgff17c77"><span class="section-number-4">1.14.4.</span> Cargo build system</h4>
<div class="outline-text-4" id="text-1-14-4">
<p>
Does things like dependency management, builds (<code>make</code>), and testing, benchmarks, and documentation. Lint checkers, formatter,
all the things you normally expect in an ecosystem.
</p>

<p>
Eggert sees the problem of subscript checking, and would rather have it at <i>compile-time</i> than <i>run-time</i>.
Subscript checking still isn't there, and we should expect progress here.
</p>
</div>
</div>
</div>

<div id="outline-container-org411430a" class="outline-3">
<h3 id="org411430a"><span class="section-number-3">1.15.</span> Programming Language Semantics</h3>
<div class="outline-text-3" id="text-1-15">
<p>
Semantics are "what does a program mean?" Software developers should care about this (and not just philosophers) because
we want tools that run programs and analyze programs, and importantly, we want our tools to be <i>correct</i>.
</p>

<p>
We have the classic distinction between <i>syntax</i> and <i>semantics</i>. The former is easy, the latter is easy. Semantics is split into two subcategories:
</p>
<ul class="org-ul">
<li><b>Dynamic semantics</b> asks the question, "what does the program mean, as you run it?"</li>
<li><b>Static semantics</b> asks the question, "how much of the program's meaning can you determine, before you start running the program?"</li>
</ul>
<p>
In general, static semantics are easier for <i>traditional</i> software programs, whereas dynamic semantics are in general harder.
</p>
</div>

<div id="outline-container-orgb0809cb" class="outline-4">
<h4 id="orgb0809cb"><span class="section-number-4">1.15.1.</span> Static semantics</h4>
<div class="outline-text-4" id="text-1-15-1">
<p>
One way to determine static semantics is through <b>attribute</b> grammars. The ideas is that you take your parse tree, but
you "annotate" it with attributes on each node (nonterminals), i.e. types and scope. For example, if we have a plus node, and we have a leaf that is
an identifier (<code>float</code>) and some other indentifier (<code>int</code>). Then for the plus, we can compute the attribute and find that e.g. it becomes an <code>int</code>.
</p>

<p>
The way we formalize is this is through grammars. For example, we have some grapper rule <code>Expr → Expr + Expr</code>. Now, we also have an extra rule
with "subscripts", and write a rule like <code>Expr.type = unify(Expr₁.type, Expr₂.type)</code>. We can do this for every grammar rule that we have.
</p>

<p>
This approach suffice for attributes called <b>synthesized attributes</b>, which is an attribute that has the information "flow up" from the leaves up to the root.
Unfortunatelly, not all attributes are that simple. Consider, for example, attributes that deal with <b>scope</b>. For scope, we need a symbol table that maps
identifier names to types. It is an <b>inherited attribute</b> that is passed down from a parent node to its children. Each scope has its own symbol table,
and when you enter a new block/scope, the block's scope is represented by a new symbol table that <i>inherits</i> all entries from the parent, and potentially add new entries. You could
also flow upwards, but it's easier to think about it flowing down.
</p>

<p>
Static semantics is very disciplined.
</p>

<p>
<b>Dynamic semantics</b>
</p>

<p>
This is the hard part. We discuss 3 forms: operational, axiomatic, and denotational semantics. 
</p>
</div>
</div>
<div id="outline-container-org0295312" class="outline-4">
<h4 id="org0295312"><span class="section-number-4">1.15.2.</span> Operational semantics</h4>
<div class="outline-text-4" id="text-1-15-2">
<p>
The basic idea is that you want a programming language L's semantics. If you don't know how the language works,
what you can do is "ask someone else to explain L to you", by writing a program that will interpret correctly, i.e.,
read the source code for an interpreter/compiler for L. Typically, this interpreter is written in some simpler language. Once you learn
one language, you can learn what another language means, by "seeing what its programs do", and then you observe the program B, written in L, under some interpreter I, written in some simpler language K,
You can either see the behavior, or even run it through a debugger.
</p>

<p>
Usually, what you'll do is read the source code, "mentally understand it", and then you don't have to run the program &#x2013; you will understand the language.
</p>

<p>
Consider the following example, where we write the semantics for ML, in prolog. We want to effectively describe OCaml in Prolog, where
</p>
<ul class="org-ul">
<li><code>m</code> is a prediction in Prolog which determines the meaning of an Ocaml expression</li>
<li><code>E</code> is an ML expression</li>
<li><code>C</code> is a context (tell me all the values of the variables I can see, map variable names to values)</li>
<li><code>R</code> is the result of evaluating <code>E</code> in <code>C</code></li>
</ul>
<p>
constants represent themselves, we want function calls (<code>call/2</code>), and atoms represent variables. We write the following
</p>
<div class="org-src-container">
<pre class="src src-prolog">call/2 = call(F, E) % represents: F E
fun(I, E) % represents: fun I -&gt; E
let(I, E, F) % represents: let I=E in F
m(E, C, R).
m(E, _, E)
   :-integer(E). % constants evaluate to themselves
m(E, C, R) % Variables (atoms) are looked up in the context
   :-atom(E), 
   member(E=R, C). % Lookup variable from context, a list.
   !, % We add this because if the same variable appears more than once in
   % the context (shadowing), it may return outermost binding instead of innermost.
   % to prevent prolog from backtracking, we add a cut !
m(let(I, E, F), C, R)
   :- m(E, C, V), % Evaluate E in the current context to get value V
   m(F, [I=V|C], R). % Evaluate F where I bound to V, growing the context

% When you define fun(I, E), you produce a function value represented
% as lambda(I, C, E)
m(fun(I, E), C, lambda(I, C, E)).
% We do lambda(*) because it has to be a function, so we indicate this to prolog
m(call(lambda(I, C, E1), E), C, R) % Call a function on an argument
   :- m(let(I,E,E1), C, R)
</pre>
</div>
<p>
If we wanted to implement Prolog in Ocaml, we would probably want to define variables, atoms and compount terms
</p>
<div class="org-src-container">
<pre class="src src-ocaml">type term =
  | Var of string              (* A variable, e.g. "X" *)
  | Atom of string             (* An atom, e.g. "foo" *)
  | Compound of string * term list  (* A compound term, e.g. f(a, X) *)
</pre>
</div>
<p>
also clauses, which are either a fact or a rule
</p>
<div class="org-src-container">
<pre class="src src-ocaml">type clause =
  | Fact of term             (* e.g., parent(john, mary). *)
  | Rule of term * term list (* e.g., ancestor(X, Y) :- parent(X, Z), ancestor(Z, Y). *)
</pre>
</div>
<p>
and then the most important is unify, which would need to write a unification function
in Ocaml:
</p>
<div class="org-src-container">
<pre class="src src-ocaml">(* A substitution maps variable names to terms *)
type substitution = (string * term) list

(* Attempt to unify two terms, returning a substitution if successful *)
val unify : term -&gt; term -&gt; substitution option
</pre>
</div>
<p>
we could write a unification in OCaml like so
</p>
<div class="org-src-container">
<pre class="src src-ocaml">(* A substitution maps variable names to terms *)
type substitution = (string * term) list

(* Apply a substitution to a term *)
let rec apply_subst (subst : substitution) (t : term) : term =
  match t with
  | Var x -&gt;
      (match List.assoc_opt x subst with
       | Some t' -&gt; apply_subst subst t'  (* recursively apply, in case t' is also a variable *)
       | None -&gt; Var x)
  | Atom _ -&gt; t
  | Compound (f, args) -&gt;
      Compound (f, List.map (apply_subst subst) args)

(* Extend the substitution by binding x to t *)
let extend (subst : substitution) (x : string) (t : term) : substitution =
  (x, t) :: subst

(* A simple unification function that attempts to unify two terms with an initial substitution *)
let rec unify (t1 : term) (t2 : term) (subst : substitution) : substitution option =
  let t1 = apply_subst subst t1 in
  let t2 = apply_subst subst t2 in
  match (t1, t2) with
  | (Atom a1, Atom a2) -&gt;
      if a1 = a2 then Some subst else None
  | (Var x, t) | (t, Var x) -&gt;
      (* If they are the same variable, no change is needed *)
      if t = Var x then Some subst
      else
        (* For simplicity, we do not do an occurs-check here *)
        Some (extend subst x t)
  | (Compound (f1, args1), Compound (f2, args2)) -&gt;
      if f1 = f2 &amp;&amp; List.length args1 = List.length args2 then
        unify_lists args1 args2 subst
      else
        None
  | _ -&gt; None

(* Helper function to unify two lists of terms *)
and unify_lists (l1 : term list) (l2 : term list) (subst : substitution) : substitution option =
  match (l1, l2) with
  | ([], []) -&gt; Some subst
  | (t1 :: rest1, t2 :: rest2) -&gt;
      (match unify t1 t2 subst with
       | None -&gt; None
       | Some subst' -&gt; unify_lists rest1 rest2 subst')
  | _ -&gt; None

(* A wrapper function that starts with the empty substitution *)
let unify_terms t1 t2 = unify t1 t2 []
</pre>
</div>
<p>
we have <b>circular dependencies</b> in operational semantics, e.g. in Lisp.
Although it refers to itself in its operational rules, the semantics are defined inductively (or recursively) so that evaluation eventually bottoms out on base cases (like evaluating constants or primitives). This self-reference is handled by the language’s ability to resolve recursive definitions, ensuring that well-formed expressions eventually evaluate in a consistent manner.
</p>
</div>
</div>

<div id="outline-container-org75a6b10" class="outline-4">
<h4 id="org75a6b10"><span class="section-number-4">1.15.3.</span> Axiomatic semantics</h4>
<div class="outline-text-4" id="text-1-15-3">
<p>
We want to write the logic for how to reason about a program
\[{P}S{Q}\]
the above "S" is a statement, where "P" is a precondition (a boolean statement which is true or false), and ”Q” is a
postcondition (a boolean statement that should be true after ”S” is executed. The meaning of this notation is "If P is true and S runs then finishes, then Q is true afterwards." For example,
we could write <code>{i &lt; 0}i : = i + 1; {i ≤ 0}</code>, literally \({P}S{Q}\). We want to write a set of axioms for our program that will allow us
to prove whatever we need. In Pascal, we have somet thing like
</p>
<pre class="example">
Prove:
      {P}If B then T else E{Q}
Assuming no side effects in B
We need to prove 2 things:
      {P^B}T{Q} and {P^notB}E{Q}
Prove:
      {P}while W do S{Q}

{P^W}S{P} {P^notW}S{Q}
Catch: this is valid but doesn’t check that the loop terminates.
Will also need to prove that a loop terminates.
</pre>
<p>
a huge advantage of axiomatic semantics over operational semantics is that we can prove things
about the program before running it.
</p>
</div>
</div>
<div id="outline-container-orga489a61" class="outline-4">
<h4 id="orga489a61"><span class="section-number-4">1.15.4.</span> Denotational semantics</h4>
<div class="outline-text-4" id="text-1-15-4">
<p>
When you want to compute the denotational semantics of a program, it will always l be a function which takes byte sequences to byte sequences and an exit status.
\[
\text{semantics}(P) = \text{function from inp to out}
\]
the semantics of i.e. a C program is a function of byte sequence to (byte sequence, exit status). To figure out
the semantics of a program, you write down this function.
</p>


<p>
We can map these semantics to types of semantics to our three different types of programming languages: imperative,
functional, and logic. Imperative corresponds to operational, logic to axiomatic, functional to denotational.
</p>
</div>
</div>
</div>

<div id="outline-container-org6989e5a" class="outline-3">
<h3 id="org6989e5a"><span class="section-number-3">1.16.</span> Notes on Python <code>asyncio</code> and <code>Future</code></h3>
<div class="outline-text-3" id="text-1-16">
<p>
A <b>coroutine</b> is a special function defined with <code>async def</code> taht can pause its execution using the <code>await</code>
keyword. When you call an async function, it returns a coroutine object rather than running immediately.
This coroutine must be scheduled on an event loop (managed by <code>asyncio</code>) to be executed. Some basic syntax
</p>
<ul class="org-ul">
<li><code>async def my_coroutine():</code> defines a coroutine</li>
<li><code>await some_async_operation()</code> pauses the coroutine until the awaited operation (which must be an awaitable) completes</li>
</ul>
<p>
A Future is an object is an object that represents a result that isn't available yet. In <code>asyncio</code>, many operations return a Future
that will eventually be resolved with a result.
</p>
<ul class="org-ul">
<li>You can think of a Future as a promise to provide a result later.</li>
<li>Coroutines often await Futures to pause execution until the result is ready.</li>
</ul>
<div class="org-src-container">
<pre class="src src-python">import asyncio

async def say_hello():
    print("Hello")
    # Simulate an asynchronous operation by sleeping
    await asyncio.sleep(1)
    print("World")
    return "Done"

# Running the coroutine using asyncio's event loop.
async def main():
    # The coroutine returns a Future-like object.
    result = await say_hello()
    print("Result:", result)

# This will run the main coroutine.
asyncio.run(main())
</pre>
</div>
<ul class="org-ul">
<li><code>say_hello()</code> is a coroutine that prints a message, waits asynchronously for 1 second, then prints another message</li>
<li>the <code>await asyncio.sleep(1)</code> statement pauses <code>say_hello</code> <i>without blocking the whole program</i></li>
<li><code>asyncio.run(main())</code> starts the event loop, schedules <code>main()</code> and waits for its completion.</li>
</ul>
<p>
<b>Threads</b> run concurrently but each thread has its own system-level overhead (e.g., context switching, memory consumption). Threads can run in parallel on multiple CPUs, but Python’s Global Interpreter Lock (GIL) often prevents true parallel execution for CPU-bound tasks.
</p>

<p>
<b>I/O-bound tasks</b> (such as reading from disk or waiting for a network response) frequently block. When using threads, one thread might block waiting for I/O while others can run, but managing many threads can be inefficient and error-prone (e.g., dealing with race conditions, locks, and thread-safety).
</p>

<p>
In terms of advantages, coroutines are much lighter than threads. They allow thousands of concurrent tasks to run in the same thread without the overhead of OS-level threads.
It allows it to pause while waiting for I/O without blocking the entire process. Additionally, thanks to it being in a 
single-threaded event loop, you typically avoid many of the synchronization issues (deadlocks, race conditions) common in threaded programming.
</p>
</div>
</div>
</div>

<div id="outline-container-org38c5a13" class="outline-2">
<h2 id="org38c5a13"><span class="section-number-2">2.</span> TODO:</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>Language "at a glance" for each one</li>
<li>Practice / get intuitive feeling for writing code in scheme, prolog, know basics in Java, re-view ocaml</li>
<li>Review first half</li>
<li>Reread notes and make sure everything makes sense.</li>
<li>Organize notes + index (Wed)</li>
<li>Review first half (Wed/Thurs)</li>
</ul>


<p>
Wed:
</p>
<ul class="org-ul">
<li>Print out notes and understand things well, make sure everything is nice and organized
<ul class="org-ul">
<li><b>Prep notes for printing</b></li>
</ul></li>
<li><b>Do all the worksheets</b></li>
<li><b>Review first half material, especially grammars, ambiguity, HW2, OCaml, etc</b></li>
<li><b>Scheme <code>call/cc</code> video</b></li>
<li>Do the LA practice final</li>
<li>Get intuitive feeling for writing code in all the languages, what's important, etc</li>
<li>Print out homeworks that are annotated</li>
<li><b>Make sure to get all "language at a glance stuff" correct and spend time making your notes good</b></li>
<li><b>Summarize all the concepts you've learned and the "Eggert vocab" in a single page</b></li>
</ul>

<p>
Thurs:
</p>
<ul class="org-ul">
<li>Review weak points, especially writing code in the languages</li>
<li>Review hard concepts, make sure everything is understandable</li>
<li>Do practice exams, review notes</li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Raayan Dhar</p>
<p class="date">Created: 2025-03-20 Thu 15:08</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
